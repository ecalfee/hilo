## variant_sites/Snakefile: pipeline to call SNPs

workdir: path_hilo
# note: working directory is hilo/ and all file inputs/outputs below are relative to that working directory

# function to find mixture proportion estimates for a population based on NGSAdmix results (as prior for ancestry_hmm)
def Alphas4LocalAncestrybyPop(pop, alphas_file):
    with open(alphas_file) as f: # "global_ancestry/results/NGSAdmix/" + prefix_all + "/K2_alphas_by_symp_pop.txt"
        for line in f:
            row = line.split("\t")
            if "pop" + row[0] == pop:
                alphas_dict = {"alpha_maize": float(row[1]), "alpha_mex": float(row[2])}
        return(alphas_dict)

## allo_freqs: estimates the minor allele frequency at all SNPs for allopatric maize and mexicana
rule allo_freqs:
    input:
        bams = all_bams, # all input bams
        bais = [bam + ".bai" for bam in all_bams],
        sites = "variant_sites/results/" + prefix_all + "/{REGION}.var.sites",
        idx = "variant_sites/results/" + prefix_all + "/{REGION}.var.sites.idx",
        bin = "variant_sites/results/" + prefix_all + "/{REGION}.var.sites.bin",
        bam_list = "samples/ALL_byPop/{GROUP}_bams.list",
        ref = ref,
        fai = fai
    output:
        mafs = "local_ancestry/results/alloFreqs/" + prefix_all + "/{GROUP}/{REGION}.mafs.gz"
    params:
        p = "med2",
        region = lambda wildcards: regions_dict[wildcards.REGION],
        out_prefix = lambda wildcards: "local_ancestry/results/alloFreqs/" + prefix_all + "/" + wildcards.GROUP + "/" + wildcards.REGION
    shadow:
        "minimal"
    conda:
        "../envs/environment.yaml"
    threads:
        1
    resources:
        time_min = lambda wildcards, attempt: attempt * 12 * 60,
        mem = lambda wildcards, attempt: attempt * 12
    shell:
        "angsd -out {params.out_prefix} "
        "-r {params.region} "
        "-sites {input.sites} "
        "-ref {input.ref} "
        "-bam {input.bam_list} "
        "-remove_bads 1 "
        "-minMapQ 30 -minQ 20 "
        "-doMajorMinor 3 "
        "-P 1 "
        "-baq 2 "
        "-GL 1 -doMaf 1"


## thin_sites_4HMM: thins sites to a set of ancestry informative markers in low LD based on sufficient coverage within allopatric maize and mexicana and high freq difference between them
rule thin_sites_4HMM:
    input:
        # input minor allele freq files for allopatric maize and mexicana
        maize_maf = expand("local_ancestry/results/alloFreqs/" + prefix_all + "/allopatric_maize/{REGION}.mafs.gz",
        REGION=list(regions_dict.keys())),
        mex_maf = expand("local_ancestry/results/alloFreqs/" + prefix_all + "/allopatric_mexicana/{REGION}.mafs.gz",
        REGION=list(regions_dict.keys())),
        # variant sites too
        sites = expand("variant_sites/results/" + prefix_all + "/{REGION}.var.sites",
        REGION=list(regions_dict.keys())),
        # and recombination positions (in cM)
        rpos = expand("variant_sites/results/" + prefix_all + "/{REGION}.rpos",
        REGION=list(regions_dict.keys())),
        # master list of all regions
        regions = "data/refMaize/divide_5Mb/ALL_regions.list"
    output:
        # outputs 1 sites file for the whole genome and a corresponding recombination distance file
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        # difference in Morgans between positions (1st position on any chromosome = 0)
        rdiff = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.rdiff",
        # recombination position (in cM)
        rpos = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.rpos",
        # summary counts of the number of snps that passed filters
        counts = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/counts_thinned_aims.txt"
    params:
        p = "med2",
        prefix_all = prefix_all,
        min_maf_diff = 0.3, # minimum difference in minor allele frequency between maize and mexicana reference panels
        min_cM = 0.001, # minimum spacing
        min_n_maize = 44, # i.e. 90% of individuals have data (44/55). 95% of SNPs have this much data for maize.
        min_n_mex = 12 # i.e. 28% of individuals have data (12/43). 86% of SNPs have this much data for mexicana.
    resources:
        time_min = lambda wildcards, attempt: attempt * 30,
        mem = lambda wildcards, attempt: attempt * 2
    conda:
        "../envs/environment.yaml"
    script:
        "thin_sites_4HMM.R"

## get_tracts_from_sites: takes in var.sites positions and creates a bed file that defines the tracts around each ancestry call. Adjacent tracts are divided at the midpoint (in cM) between sites and first/last tract for a chromosome start/end at first/last site position.
rule get_tracts_from_sites:
    input:
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        rmap = "data/linkage_map/ogut_fifthcM_map_agpv4_EXTENDED.txt"
    output:
        bed = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.bed"
    params:
        p = "med2",
        prefix_all = prefix_all
    resources:
        time_min = lambda wildcards, attempt: attempt * 15,
        mem = lambda wildcards, attempt: attempt * 2
    conda:
        "../envs/environment.yaml"
    script:
        "tracts_from_sites.R"


## index_sites_4HMM: uses angsd to index thinned sites
rule index_sites_4HMM:
    input:
        "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites"
    output:
        idx = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.idx",
        bin = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.bin"
    params:
        p = "med2"
    resources:
        time_min = 15,
        mem = 2
    conda:
        "../envs/environment.yaml"
    shell:
        "angsd sites index {input}"


## count_ACGT: counts reads matching each nucleotide A, C, T or G at each position in sites file for 1 sample's bam
rule count_ACGT:
    input:
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        idx = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.idx",
        bin = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.bin",
        ref = ref,
        fai = fai,
        bam = "filtered_bams/merged_bams/{ID}.sort.dedup.bam", # note: to run you need to make symlink for allopatric maize from e.g. filtered_bams/MAIZE55/MAIZE1.sort.dedup.bam to filtered_bams/merged_bams/MAIZE1.sort.dedup.bam
        bai = "filtered_bams/merged_bams/{ID}.sort.dedup.bam.bai"
    output:
        acgt = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.counts.gz",
        pos = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.pos.gz",
        bam_list = "samples/HILO_MAIZE55_byInd/{ID}_bams.list"
    params:
        p = "med2",
        out_prefix = lambda wildcards: "local_ancestry/results/countsACGT/" + prefix_all + "/" + wildcards.ID
    resources:
        time_min = lambda wildcards, attempt: attempt * 16 * 60,
        mem = lambda wildcards, attempt: attempt * 8
    conda:
        "../envs/environment.yaml"
    shadow:
        "minimal"
    threads:
        2
    shell:
        """
        echo {input.bam} > {output.bam_list}
        angsd -out {params.out_prefix} \
        -ref {input.ref} \
        -bam {output.bam_list} \
        -minQ 20 -minMapQ 30 -baq 2\
        -remove_bads 1 \
        -doCounts 1 -dumpCounts 3 \
        -P {threads} \
        -sites {input.sites}
        """

## count_maj_min: turns ACGT read counts at each position into major and minor read counts based on sites file (ignores reads for other alleles)
rule count_maj_min:
    input:
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        acgt = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.counts.gz",
        pos = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.pos.gz"
    output:
        majmin = "local_ancestry/results/countsMajMin/" + prefix_all + "/{ID}.counts.txt"
    params:
        p = "med2"
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 4
    conda:
        "../envs/environment.yaml"
    script:
        "count_maj_min.R"

## make_allo_counts: for allopatric populations, sample 1 read (based on major/minor read counts) per individual to get an allele count. Also add SNP information to output file.
rule make_allo_counts:
    input:
        maize = "samples/ALL_byPop/allopatric_maize_ids.list",
        mex = "samples/ALL_byPop/allopatric_mexicana_ids.list",
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        rdiff = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.rdiff",
        counts = ["local_ancestry/results/countsMajMin/" + prefix_all + "/" + id + ".counts.txt" for id in all_ids]
    output:
        allo_counts = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.allo.counts"
    params:
        p = "high2", # only 1 job, high priority b/c other jobs wait on this one
        prefix_all = prefix_all
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 4
    conda:
        "../envs/environment.yaml"
    script:
        "make_allo_counts_ancestry_hmm.R"


## make_input_hmm: combines the snp information, allopatric reference pop allele freqs, and read counts for sympatric individuals
rule make_input_hmm:
    input:
        allo = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.allo.counts", # SNP info and counts from allopatric pops
        pop_ids = "samples/Over0.5x_byPop/{POP}_ids.list",
        counts = ["local_ancestry/results/countsMajMin/" + prefix_all + "/" + id + ".counts.txt" for id in all_ids]
    output:
        counts = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.counts",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy"
    params:
        p = "med2",
        prefix_all = prefix_all
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 4
    conda:
        "../envs/environment.yaml"
    script:
        "make_input_ancestry_hmm.R"

## run_ancestry_hmm: runs local ancestry hidden Markov model (ancestry_hmm)
rule run_ancestry_hmm:
    input:
        counts = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.counts",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy",
        alphas = "global_ancestry/results/NGSAdmix/" + prefix_all + "/K2_alphas_by_symp_pop.txt"
    output:
        # empty output file is created only if ancestry_hmm runs sucessfully
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_noBoot/{POP}.completed"
        # also creates posterior files for each ID in POP, but snakemake can't track dynamic output like this
        # posteriors = lambda wildcards: expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_noBoot/posterior/{ID}.posterior", ID = symp_dict[wildcards.POP])
        # note: can't use 'shadow' with these extra output files!
    params:
        p = "med2",
        alpha_maize = lambda wildcards, input: Alphas4LocalAncestrybyPop(wildcards.POP, input.alphas)["alpha_maize"],  # proportion maize ancestry from NGSAdmix
        alpha_mex = lambda wildcards, input: Alphas4LocalAncestrybyPop(wildcards.POP, input.alphas)["alpha_mex"], # proportion mexicana ancestry from NGSAdmix
        Ne = lambda wildcards: wildcards.Ne, # Ne = effective population size after admixture
        t_gen = -100, # prior for generations. negative means not a fixed parameter (estimated by the hmm instead)
        dir = lambda wildcards: "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne" + wildcards.Ne + "_noBoot/posterior"
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 8
    log: # log file keeps info about inferred timing of admixture
        "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_noBoot/{POP}.log"
    shell:
        """
        module load Ancestry_HMM
        mkdir -p {params.dir}
        cd {params.dir}
        ancestry_hmm -a 2 {params.alpha_maize} {params.alpha_mex} \
        -p 0 10000 {params.alpha_maize} -p 1 {params.t_gen} {params.alpha_mex} \
        --ne {params.Ne} --tmin 0 --tmax 10000 \
        -e 3e-3 \
        -i "../../input/{wildcards.POP}.counts" \
        -s "../../input/{wildcards.POP}.ploidy" > ../{wildcards.POP}.log && echo "all done!" > ../{wildcards.POP}.completed
        """

## boot_ancestry_hmm: runs ancestry_hmm with a bootstrap for admixture timing
rule boot_ancestry_hmm:
    input:
        counts = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.counts",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy",
        alphas = "global_ancestry/results/NGSAdmix/" + prefix_all + "/K2_alphas_by_symp_pop.txt"
    output:
        # empty output file is created only if ancestry_hmm runs sucessfully
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_yesBoot/{POP}.completed"
        # also creates posterior files for each ID in POP, but snakemake can't track dynamic output like this
        # posteriors = lambda wildcards: expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_noBoot/posterior/{ID}.posterior", ID = symp_dict[wildcards.POP])
        # note: can't use 'shadow' with these extra output files!
    params:
        p = "med2",
        alpha_maize = lambda wildcards, input: Alphas4LocalAncestrybyPop(wildcards.POP, input.alphas)["alpha_maize"],  # proportion maize ancestry from NGSAdmix
        alpha_mex = lambda wildcards, input: Alphas4LocalAncestrybyPop(wildcards.POP, input.alphas)["alpha_mex"], # proportion mexicana ancestry from NGSAdmix
        Ne = lambda wildcards: wildcards.Ne, # Ne = effective population size after admixture
        t_gen = -100, # prior for generations. negative means not a fixed parameter (estimated by the hmm instead)
        dir = lambda wildcards: "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne" + wildcards.Ne + "_yesBoot/posterior"
    resources:
        time_min = lambda wildcards, attempt: attempt * 3 * 60,
        mem = lambda wildcards, attempt: attempt * 8
    log: # log file keeps info about inferred timing of admixture
        "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_yesBoot/{POP}.log"
    shell:
        """
        module load Ancestry_HMM
        mkdir -p {params.dir}
        cd {params.dir}
        ancestry_hmm -a 2 {params.alpha_maize} {params.alpha_mex} \
        -p 0 10000 {params.alpha_maize} -p 1 {params.t_gen} {params.alpha_mex} \
        --ne {params.Ne} --tmin 0 --tmax 10000 \
        -e 3e-3 \
        -b 100 1000 \
        -i "../../input/{wildcards.POP}.counts" \
        -s "../../input/{wildcards.POP}.ploidy" > ../{wildcards.POP}.log && echo "all done!" > ../{wildcards.POP}.completed
        """
#-b 100 1000 does 100 bootstraps for time estimates with 1000 snps/block

## summarise_posterior: takes in posterior probabilities for a population from ancestry_hmm and summarises ancestry proportions for each individual genomewide and for each population for SNPs along the genome
rule summarise_posterior:
    input: # this file tracts that the posteriors are complete, but actually posterior files vary
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/{POP}.completed", # YESNO can be yes or no
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy" #"samples/Over0.5x_byPop/{POP}_ids.list"
    output:
        pop = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc/{POP}.anc.freq",
        ind = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc/{POP}.anc.ind",
        alpha = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc/{POP}.alpha.ind"
    params:
        p = "med2",
        dir_post = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/posterior"
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 2
    script:
        "post2anc.R"

## get_maximum_a_posterior: takes in posterior probabilities for a population from ancestry_hmm and gets maximum a posterior ancestry state and posterior probability for that state
rule get_maximum_a_posterior:
    input: # this file tracts that the posteriors are complete, but actually posterior files vary
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/{POP}.completed", # YESNO can be yes or no
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy" #"samples/Over0.5x_byPop/{POP}_ids.list"
    output:
        anc = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/MAP/{POP}.anc.ind",
        prob = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/MAP/{POP}.prob.ind"
    params:
        p = "med2",
        dir_post = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/posterior"
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 2
    script:
        "post2map.R"

## get_homozygous_ancestry_tracts_and_bams: finds homozygous ancestry tracts for every sample in a sympatric population
rule get_homozygous_ancestry_tracts:
    input: # this file tracts that the posteriors are complete, but actually posterior files vary
        bed = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.bed", # ancestry tracts around each site
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/{POP}.completed",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy"
    output: # doesn't track all output files, just one file that updates when the population is completed
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/tracts/{POP}.completed",
    params:
        p = "med2",
        dir_out = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/tracts",
        dir_post = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/posterior",
        ZEA = "{ZEA}"
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = lambda wildcards, attempt: attempt * 1 * 60,
        mem = lambda wildcards, attempt: attempt * 2
    shell:
        "./local_ancestry/get_homozygous_ancestry_tracts.sh {input.bed} {params.ZEA} {input.ploidy} {params.dir_post} {params.dir_out} && touch {output.complete}"

## get_homozygous_ancestry_bams: filters bams to only include reads within homozygous ancestry tracts for every sample in a sympatric population
rule get_homozygous_ancestry_bams:
    input: # this file tracts that the posteriors are complete, but actually posterior files vary
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/tracts/{POP}.completed",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy"
    output: # doesn't track all output files, just one file that updates when the population is completed
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/bams/{POP}.completed",
    params:
        p = "med2",
        dir_tracts = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/tracts",
        dir_in_bams = "filtered_bams/merged_bams",
        dir_out = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/bams"
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = 12 * 60,
        mem = lambda wildcards, attempt: attempt * 8
    shell:
        "./local_ancestry/get_homozygous_ancestry_bams.sh {input.ploidy} {params.dir_tracts} {params.dir_in_bams} {params.dir_out} && touch {output.complete}"

## index_and_list_homozygous_ancestry_bams: creates .bai file for bams with only reads from homozygous ancestry tracts and a list of bams per population
rule index_and_list_homozygous_ancestry_bams:
    input: # doesn't track all input files, just one file that updates when the homozygous ancestry bams are updated
        complete = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/bams/{POP}.completed",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy"
    output: # doesn't track all output files, just one file that updates when the population is completed
        list = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/bams/{POP}_bams.list"
    params:
        p = "med2",
        dir_bams = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/HOMOZYG/{ZEA}/bams"
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = 15,
        mem = 2
    shell:
        """
        (for SAMPLE in $(cat {input.ploidy} | cut -f1)
            do echo {params.dir_bams}/$SAMPLE.bam
            samtools index {params.dir_bams}/$SAMPLE.bam
            done) > {output.list}
        """

## rule combine_pop_ancestry_data: takes in population ancestry frequencies for all sympatric pops in maize or mexicana and saves a summary R dataframe SNPs X Pops, a combined sample allele frequency file, and pop-level metadata for all included inds
rule combine_pop_ancestry_data:
    input:
        "samples/" + prefix_all + "_meta.RData",
        "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.bed",
        expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy", POP = symp_pops),
        expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{{Ne}}_{{YESNO}}Boot/anc/{POP}.anc.freq", POP = symp_pops)
    output:
        pop_anc = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc/{ZEA}.pops.anc.RData",
        combined_anc = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc/{ZEA}.combined.anc.bed",
        meta_pop = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc/{ZEA}.pop.meta.RData" # meta_pops dataframe has n_local_ancestry, ELEVATION, and alpha_local_ancestry for each pop, sorted in same order as anc
    params:
        p = "med2",
        dir_anc = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/anc",
        dir_ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input",
        zea = lambda wildcards: wildcards.ZEA
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = lambda wildcards, attempt: attempt * 10,
        mem = lambda wildcards, attempt: attempt * 8
    script:
        "combined_pop_anc_data.R"

## get_admix_times: takes in log files from ancestry_hmm and extracts the inferred timing of admixture. first entry is optimum, then bootstrap results.
rule get_admix_times:
    input:
        log = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/{POP}.log"
    output:
        times = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_{YESNO}Boot/{POP}.times"
    params:
        p = "med2"
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = 10,
        mem = 2
    shell:
        "awk -v N=4 'NR % N == 0' {input.log} | cut -f3 > {output.times}"

## plot_admix_times:
rule plot_admix_times:
    input:
        expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{{Ne}}_{{YESNO}}Boot/anc/{ZEA}.pop.meta.RData", ZEA = zea),
        expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{{Ne}}_{{YESNO}}Boot/{POP}.log", POP = symp_pops),
        "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.allo.counts",
        "colors.R"
    output:
        png_times = "local_ancestry/plots/admix_times_Ne{Ne}_{YESNO}Boot.png"
    params:
        p = "med2",
        Ne = lambda wildcards: wildcards.Ne,
        YESNO = lambda wildcards: wildcards.YESNO,
        prefix_all = prefix_all,
        colors = "colors.R",
        alpha = 0.1
    conda:
        "../envs/environment.yaml"
    resources:
        time_min = 10,
        mem = 2
    script:
        "plot_admix_times.R"
