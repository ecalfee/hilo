## variant_sites/Snakefile: pipeline to call SNPs

workdir: path_hilo
# note: working directory is hilo/ and all file inputs/outputs below are relative to that working directory

# function to read in all the sample IDs for local ancestry inference in a sympatric population
#def IDs4LocalAncestrybyPop(wildcards):
#    with open("samples/Over0.5x_byPop/" + wildcards.POP + "_ids.list") as f:
#        pop_ids = f.read().splitlines()
#        return pop_ids

# function to find mixture proportion estimates for a population based on NGSAdmix results (as prior for ancestry_hmm)
def Alphas4LocalAncestrybyPop(pop, alphas_file):
    with open(alphas_file) as f: # "global_ancestry/results/NGSAdmix/" + prefix_all + "/K2_alphas_by_symp_pop.txt"
        for line in f:
            row = line.split("\t")
            if "pop" + row[0] == pop:
                alphas_dict = {"alpha_maize": float(row[1]), "alpha_mex": float(row[2])}
        return(alphas_dict)


## thin_sites_4HMM: thins sites to a set of ancestry informative markers in low LD based on sufficient coverage within allopatric maize and mexicana and high freq difference between them
rule thin_sites_4HMM:
    input:
        # input minor allele freq files for allopatric maize and mexicana
        maize_maf = expand("variant_sites/results/popFreq/allopatric_maize/{REGION}.mafs.gz",
        REGION=list(regions_dict.keys())),
        mex_maf = expand("variant_sites/results/popFreq/allopatric_mexicana/{REGION}.mafs.gz",
        REGION=list(regions_dict.keys())),
        # variant sites too
        sites = expand("variant_sites/results/" + prefix_all + "/{REGION}.var.sites",
        REGION=list(regions_dict.keys())),
        # and recombination positions (in cM)
        rpos = expand("variant_sites/results/" + prefix_all + "/{REGION}.rpos",
        REGION=list(regions_dict.keys())),
        # master list of all regions
        regions = "data/refMaize/divide_5Mb/ALL_regions.list"
    output:
        # outputs 1 sites file for the whole genome and a corresponding recombination distance file
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        # difference in Morgans between positions (1st position on any chromosome = 0)
        rdiff = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.rdiff",
        # recombination position (in cM)
        rpos = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.rpos",
        # summary counts of the number of snps that passed filters
        counts = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/counts_thinned_aims.txt"
    params:
        p = "med2",
        prefix_all = prefix_all,
        min_maf_diff = 0.3, # minimum difference in minor allele frequency between maize and mexicana reference panels
        min_cM = 0.001, # minimum spacing
        min_n_maize = 44, # i.e. 90% of individuals have data (44/55). 95% of SNPs have this much data for maize.
        min_n_mex = 12 # i.e. 28% of individuals have data (12/43). 86% of SNPs have this much data for mexicana.
    resources:
        time_min = lambda wildcards, attempt: attempt * 30,
        mem = lambda wildcards, attempt: attempt * 2
    conda:
        "../envs/environment.yaml"
    script:
        "thin_sites_4HMM.R"


## index_sites_4HMM: uses angsd to index thinned sites
rule index_sites_4HMM:
    input:
        "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites"
    output:
        idx = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.idx",
        bin = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.bin"
    params:
        p = "med2"
    resources:
        time_min = 15,
        mem = 2
    conda:
        "../envs/environment.yaml"
    shell:
        "angsd sites index {input}"


## count_ACGT: counts reads matching each nucleotide A, C, T or G at each position in sites file for 1 sample's bam
rule count_ACGT:
    input:
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        idx = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.idx",
        bin = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites.bin",
        ref = ref,
        fai = fai,
        bam = "filtered_bams/merged_bams/{ID}.sort.dedup.bam",
        bai = "filtered_bams/merged_bams/{ID}.sort.dedup.bam.bai"
    output:
        acgt = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.counts.gz",
        pos = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.pos.gz",
        bam_list = "samples/HILO_MAIZE55_byInd/{ID}_bams.list"
    params:
        p = "med2",
        out_prefix = lambda wildcards: "local_ancestry/results/countsACGT/" + prefix_all + "/" + wildcards.ID
    resources:
        time_min = lambda wildcards, attempt: attempt * 12 * 60,
        mem = lambda wildcards, attempt: attempt * 8
    conda:
        "../envs/environment.yaml"
    shadow:
        "minimal"
    threads:
        2
    shell:
        """
        echo {input.bam} > {output.bam_list}
        angsd -out {params.out_prefix} \
        -ref {input.ref} \
        -bam {output.bam_list} \
        -minQ 20 -minMapQ 30 -baq 2\
        -remove_bads 1 \
        -doCounts 1 -dumpCounts 3 \
        -P {threads} \
        -sites {input.sites}
        """

## count_maj_min: turns ACGT read counts at each position into major and minor read counts based on sites file (ignores reads for other alleles)
rule count_maj_min:
    input:
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        acgt = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.counts.gz",
        pos = "local_ancestry/results/countsACGT/" + prefix_all + "/{ID}.pos.gz"
    output:
        majmin = "local_ancestry/results/countsMajMin/" + prefix_all + "/{ID}.counts.txt"
    params:
        p = "med2"
    resources:
        time_min = lambda wildcards, attempt: attempt * 15,
        mem = lambda wildcards, attempt: attempt * 4
    conda:
        "../envs/environment.yaml"
    script:
        "count_maj_min.R"

## make_allo_counts: for allopatric populations, sample 1 read (based on major/minor read counts) per individual to get an allele count. Also add SNP information to output file.
rule make_allo_counts:
    input:
        maize = "samples/ALL_byPop/allopatric_maize_ids.list",
        mex = "samples/ALL_byPop/allopatric_mexicana_ids.list",
        sites = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.var.sites",
        rdiff = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.rdiff",
        counts = ["local_ancestry/results/countsMajMin/" + prefix_all + "/" + id + ".counts.txt" for id in all_ids]
    output:
        allo_counts = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.allo.counts"
    params:
        p = "med2",
        prefix_all = prefix_all
    resources:
        time_min = lambda wildcards, attempt: attempt * 15,
        mem = lambda wildcards, attempt: attempt * 4
    conda:
        "../envs/environment.yaml"
    script:
        "make_allo_counts_ancestry_hmm.R"


## make_input_hmm: combines the snp information, allopatric reference pop allele freqs, and read counts for sympatric individuals
rule make_input_hmm:
    input:
        allo = "local_ancestry/results/thinnedSNPs/" + prefix_all + "/whole_genome.allo.counts", # SNP info and counts from allopatric pops
        pop_ids = "samples/Over0.5x_byPop/{POP}_ids.list",
        counts = ["local_ancestry/results/countsMajMin/" + prefix_all + "/" + id + ".counts.txt" for id in all_ids]
    output:
        counts = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.counts",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy"
    params:
        p = "med2",
        prefix_all = prefix_all
    resources:
        time_min = lambda wildcards, attempt: attempt * 15,
        mem = lambda wildcards, attempt: attempt * 4
    conda:
        "../envs/environment.yaml"
    script:
        "make_input_ancestry_hmm.R"

## run_ancestry_hmm: runs ancestry_hmm
rule run_ancestry_hmm:
    input:
        counts = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.counts",
        ploidy = "local_ancestry/results/ancestry_hmm/" + prefix_all + "/input/{POP}.ploidy",
        alphas = "global_ancestry/results/NGSAdmix/" + prefix_all + "/K2_alphas_by_symp_pop.txt"
    output:
        # empty output file is created only if ancestry_hmm runs sucessfully
        "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_noBoot/{POP}.completed"
        # also creates posterior files for each ID in POP, but snakemake can't track dynamic output like this
        # posteriors = lambda wildcards: expand("local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne{Ne}_noBoot/posterior/{ID}.posterior", ID = symp_dict[wildcards.POP])
        # note: can't use 'shadow' with these extra output files!
    params:
        p = "med2",
        alpha_maize = lambda wildcards, input: Alphas4LocalAncestrybyPop(wildcards.POP, input.alphas)["alpha_maize"],  # proportion maize ancestry from NGSAdmix
        alpha_mex = lambda wildcards, input: Alphas4LocalAncestrybyPop(wildcards.POP, input.alphas)["alpha_mex"], # proportion mexicana ancestry from NGSAdmix
        Ne = lambda wildcards: wildcards.Ne, # Ne = 10000 effective population size after admixture
        t_gen = -100, # prior for generations. negative means not a fixed parameter (estimated by the hmm instead)
        dir = lambda wildcards: "local_ancestry/results/ancestry_hmm/" + prefix_all + "/Ne" + wildcards.Ne + "_noBoot/posterior/"
    resources:
        time_min = lambda wildcards, attempt: attempt * 60,
        mem = lambda wildcards, attempt: attempt * 8
    log: # log file keeps info about inferred timing of admixture
        "local_ancestry/results/ancestry_hmm/" + prefix_all + "Ne{Ne}_noBoot/{POP}.log"
    shell:
        """
        cd {params.dir}
        ancestry_hmm -a 2 {params.alpha_maize} {params.alpha_mex} \
        -p 0 100000 {params.alpha_maize} -p 1 {params.t_gen} {params.alpha_mex} \
        --ne {params.Ne} --tmin 0 --tmax 10000 \
        -i "../../input/{wildcards.POP}.counts" \
        -s "../../input/{wildcards.POP}.ploidy" && touch ../{wildcards.POP}.completed
        """

## summarise_ancestry: takes in posterior probabilities for a population from ancestry_hmm and summarises ancestry proportions for each individual genomewide and for each population for SNPs along the genome
#rule summarise_ancestry:

## get_admix_time: takes in log file from ancestry_hmm and extracts the inferred timing of admixture

## ancestry_sites_to_tracts: takes in var.sites positions and creates a bed file that defines the tracts around each ancestry call.

# # NOTE: use lambda function in params to extract the correct alphas for your pop (read in as a dictionary)
