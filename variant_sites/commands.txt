After read mapping, we use the whole sample (reference and admixed individuals) to identify variant sites using ANGSD.
# PASS1 ANALYSIS LOG:
1) Identified sites with >5% MAF in global sample based on reads meeting mapping and base quality cutoffs.
Filters out sites with too high coverage in global sample and sites with data for too few individuals.
mafs.gz output files from angsd are divided into 5Mb regions across the genome, regions 0-425:
allVarAngsdGL1.sh
(note: high coverage cutoff was based on calculating total depth of coverage (from ANGSD) across some random regions:
R script plot_mapping_metrics.R was used to process the results of calcDepthCovRegions.sh)

2) Created and indexed 'sites' file for each mafs.gz file, listing the chromosome and bp position of each variant site:
mafsToSitesFile.sh

Note: Filtering of individual level data for low or high coverage happens within each analysis.
Different analyses require different filtering from this point for levels of acceptable LD between included SNPs, min. number of individuals with data, and possible enrichment for "ancestry-informativeness" using min. # individuals and allele frequency differencess in allopatric maize and mex reference panels

# UPDATED ANALYSIS:
Call new set of SNPs for combined sample with all hilo individuals plus new allopatric maize (landraces from Li) and old allopatric maize (15 of the 16 lowland mex maize individuals).
Goal is to create a PCA and check that the old and new allopatric maize look fine in PCA space.
hilo_alloMAIZE_MAIZE4LOW_bams.list
# first get mean coverage estimate:
variant_sites$ sbatch --export=PREFIX=hilo_alloMAIZE_MAIZE4LOW,REGIONS_LIST=N1000.L100.regions calcDepthCovRegions.sh
Submitted batch job 9585133 - NOT RUNNING (CHECK ON THIS!). Fixed angsd module in calculating depth per region and waiting for new HILO85 file to be completed:
variant_sites$ sbatch --dependency=afterok:9593488 --export=PREFIX=hilo_alloMAIZE_MAIZE4LOW,REGIONS_LIST=N1000.L100.regions calcDepthCovRegions.sh
Submitted batch job 9593538 - ERROR 3.6.19. Fixed typo in bam path:
Submitted batch job 9593626 - fixed additional bam path issue
Submitted batch job 9593643 - COMPLETED 3.6.19. Re-do including HILO268-276:
variant_sites$ sbatch --export=PREFIX=hilo_alloMAIZE_MAIZE4LOW_RIMMA0625_small,REGIONS_LIST=N1000.L100.regions calcDepthCovRegions.sh
Submitted batch job 9735142 - CANCELLED -- incomplete sample list.
Submitted batch job 9735212 - RUNNING 3.9.19

# call variant sites with cutoff based on < 2x mean
# interactively using plot_mapping_metrics.R I inferred the mean coverage for random regions is ~440x
# So I'll use 880 as maximum allowed depth in ANGSD calling variant sites
variant_sites$ sbatch --export=PREFIX=hilo_alloMAIZE_MAIZE4LOW,MAX_DEPTH=880 id_SNPs.sh
Submitted batch job 9612676 - DIDN'T WORK. FIXED TYPO & RERUNNING:
Submitted batch job 9613146 - RAN 3.6.19. Errors with 2 files (out-of-memory) - redoing:
slurm-log/idSNPs_9613146_39.out: "slurmstepd: error: Detected 1 oom-kill event(s) in step 9613191.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler." And same problem with _361.
variant_sites$ sbatch --array=39,361 --mem=24G --export=PREFIX=hilo_alloMAIZE_MAIZE4LOW,MAX_DEPTH=880 id_SNPs.sh
Submitted batch job 9716699 - COMPLETED (fast) 3.8.19
# estimated coverage in plot_PCA.R using flagstat total # mapped reads (>Q30) results
variant_sites$ sbatch --array=0-425 --mem=24G --export=PREFIX=hilo_alloMAIZE_MAIZE4LOW_RIMMA0625_small,MAX_DEPTH=1130 id_SNPs.sh
Submitted batch job 9735284 - COMPLETE 3.9.19

# make var.sites files to complement the mafs.gz files:
variant_sites$ sbatch --export="DIR=results/hilo_alloMAIZE_MAIZE4LOW,ALL" -p med2 mafsToSitesFile.sh
Submitted batch job 9722339 - COMPLETED 3.8.19
variant_sites$ sbatch --dependency=afterok:9735284 --export="DIR=results/hilo_alloMAIZE_MAIZE4LOW_RIMMA0625_small,ALL" -p med2 mafsToSitesFile.sh
Submitted batch job 9737769 - COMPLETED 3.9.19

# I also look at just hilo_alloMAIZE and mean coverage for random regions is ~260 based on coverage over random regions, so I could filter to ~ <550 depth for this subset, but currently RIMMA0625 looks low coverage when it shouldn't be (!)

# I remapped RIMMA0625 and it's now (correctly) high coverage.
# Now calling variant sites for pass2 + landraces:
# interactively in plot_PCA.R I find that total estimated coverage 
# for reads that map is 401x (of which ~121x is from hilo samples), 
# so I use 803x as an upper bound filter to exclude SNPs > 2x mean coverage
variant_sites$ sbatch --mem=24G --export=PREFIX=hilo_alloMAIZE,MAX_DEPTH=803 id_SNPs.sh
Submitted batch job 10451669 - COMPLETED 4.23.19
# oops! I ran hilo_alloMAIZE when I meant pass2_alloMAIZE. Try again:
variant_sites$ sbatch --mem=24G --export=PREFIX=pass2_alloMAIZE,MAX_DEPTH=803 id_SNPs.sh
Submitted batch job 10453141 - COMPLETED 4.23.19
# create sites file from SNPs:
variant_sites$ sbatch --export="DIR=results/pass2_alloMAIZE,ALL" --dependency=afterok:10453141 -p med2 mafsToSitesFile.sh
Submitted batch job 10453205 - COMPLETED 4.23.19 


# Get SNPs called including parviglumis so I can get accurate allele frequencies for parviglumis
# first make list of samples:
samples$ cat pass2_alloMAIZE_IDs.list PalmarChico_IDs.list > pass2_alloMAIZE_PalmarChico_IDs.list
samples$ cat pass2_alloMAIZE_bams.list PalmarChico_bams.list > pass2_alloMAIZE_PalmarChico_bams.list
# then call SNPs
# call variant sites with cutoff based on < 2x mean
# interactively using plot_mapping_metrics.R I inferred the mean coverage for random regions is ~1189x
# running flagstat to get that estimate right now
filtered_bams/metrics$ for i in {1..50}; do cat PARV"$i".flagstat | grep 'QC-passed' | cut -f1 -d" "; done > PalmarChico.nreads
filtered_bams/metrics$ cat PalmarChico.nreads | paste -s -d+ | bc --
11217984084 # total number reads
> 11217984084*150/ref_genome_size
[1] 788.1181 # or about 15x per parviglumis

# find SNPs
variant_sites$ sbatch --mem=48G --export=PREFIX=pass2_alloMAIZE_PalmarChico,MAX_DEPTH=1590 id_SNPs.sh
# create sites file from SNPs mafs.gz file
Submitted batch job 11278797 - RUNNING 6.10.19
variant_sites$ sbatch --export="DIR=results/pass2_alloMAIZE_PalmarChico,ALL" --dependency=afterok:11278797 -p med2 mafsToSitesFile.sh
Submitted batch job 11278884 - WAITING 4 DEPENDENCY
# note: if this takes far too long I could just run inv4m or limit number of parviglumis individuals included in SNP calling to a subset


