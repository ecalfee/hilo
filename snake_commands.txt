# HMMM.... BENCHMARKS:
# how can I log how a task is doing??
--cluster-status ./status.py
# see example here: https://snakemake.readthedocs.io/en/stable/tutorial/additional_features.html
a 2.4G file only took 7hrs (24304seconds) real time, and 101 hrs (364921sec) cpu time
that's using 16 threads.
Max memory was <14G

what do I expect for samtools??

# HOW DO I STOP AND RESTART SNAKEMAKE?
# Can I restart failed jobs with double time and double memory? yes, done.
# I want to log cluster-status. skip for now.
OUT_OF_MEMORY CANCELLED STOPPED TIMEOUT FAILED
PENDING RUNNING COMPLETED COMPLETING SUSPENDED
--parsable #within cluster call . skip for now.
--cluster-status ./status.py #in profile
# test with a task that should run out of memory (give way too little)
# test also with a task that should run out of time (too little time)
# and some that should run through, but take a min or two
--max-status-checks-per-second 0.2

# can memory take in file size?
snakemake -n --quiet # will just print final summary


# I CANCELLED MY SCREEN SNAKEMAKE JOB WITH THESE PROCESSES LEFT TO RUN:
21567559   bigmemm bwa_map.  ecalfee  R     8:53:03      1 16  64G    bigmem8
21567566   bigmemm bwa_map.  ecalfee  R     1:41:03      1 16  64G    bigmem3
21567565   bigmemm bwa_map.  ecalfee  R     1:52:03      1 16  64G    bigmem7
21567563   bigmemm bwa_map.  ecalfee  R     3:58:50      1 16  64G    bigmem6
21567562   bigmemm bwa_map.  ecalfee  R     6:41:59      1 16  64G    bigmem7
21567573   bigmemm bwa_map.  ecalfee  R       14:04      1 16  64G    bigmem8
21567568   bigmemm bwa_map.  ecalfee  R     1:00:06      1 16  64G    bigmem5
21567569   bigmemm bwa_map.  ecalfee  R     1:00:06      1 16  64G    bigmem4
21567570   bigmemm bwa_map.  ecalfee  R     1:00:06      1 16  64G    bigmem4
21567571   bigmemm bwa_map.  ecalfee  R     1:00:06      1 16  64G    bigmem4
21567572   bigmemm bwa_map.  ecalfee  R     1:00:06      1 16  64G    bigmem4
21567567   bigmemm bwa_map.  ecalfee  R     1:04:13      1 16  64G    bigmem8
21582116      med2 remove_d  ecalfee  R        6:37      1 3   8G     c6-89
21581372   bigmemm samtools  ecalfee  R        6:37      1 4   32G    bigmem8

# TEST WITH a & b again. TEST UPDATED SNAKECODE






# testing snakemake pipeline
# make a very small dataset with 2 fastqs:
ecalfee@c6-93:~/hilo/data/HILO_raw_reads$ zcat April2020_1/HILO312_1.fq.gz | head -n 800 | gzip > TEST/a_1.fq.gz
ecalfee@c6-93:~/hilo/data/HILO_raw_reads$ zcat April2020_1/HILO312_2.fq.gz | head -n 800 | gzip > TEST/a_2.fq.gz
ecalfee@c6-93:~/hilo/data/HILO_raw_reads$ zcat April2020_1/HILO355_1.fq.gz | head -n 800 | gzip > TEST/b_1.fq.gz
ecalfee@c6-93:~/hilo/data/HILO_raw_reads$ zcat April2020_1/HILO355_2.fq.gz | head -n 800 | gzip > TEST/b_2.fq.gz
all_files = ["TEST/a.sort.dedup.baq.bam.bai", "TEST/b.sort.dedup.baq.bam.bai"]

# useful commands for conda environments:
# to list available environments
conda info -e
# to create conda environment (only do with -p 2022)
#conda create --name hilo-env --file environment.yaml
# to update conda environment: (only do when ssh'd into -p 2022)
#conda env update --name hilo-env --file envs/environment.yaml --prune
# to list packages and versions for hilo-env
#source activate hilo-env #activate
#conda list # list packages
#conda deactivate hilo-env # de-activate



# slurm nodes vs. tasks per node vs. cpus
# cores vs. nodes vs. tasks: https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/resource-requests/
multi-process program = multiple tasks (--ntasks)
multi-threaded program = multiple cores, 1 task (--cpus-per-task)

e.g. running samtools sort with 8 threads:
https://hcc.unl.edu/docs/applications/app_specific/bioinformatics_tools/data_manipulation_tools/samtools/running_samtools_commands/
#SBATCH --nodes=1 -N
#SBATCH --ntasks-per-node=8  -n

# running bowtie2 with 16 threads.
#SBATCH --cpus-per-task=16 -c


# GREAT RESOURCE FOR SNAKEMAKE: http://garthkong.com/snakemake-resources/
# GREAT RESOURCE ON SNAKEMAKE FOR CLUSTER: https://www.sichong.site/2020/02/25/snakemake-and-slurm-how-to-manage-workflow-with-resource-constraint-on-hpc/
How to get an email for failed jobs: --mail-type {cluster.email_type} --mail-user {cluster.email}
# HOW TO USE DICTIONARIES TO MAP 1 SAMPLE TO MULTIPLE FILES: https://www.jakevc.com/posts/2019/07/snakemake-examples/
# HOW TO USE SUBWORKFLOWS: https://lachlandeer.github.io/snakemake-econ-r-tutorial/subworkflows-divide-and-conquer.html#subworkflow-basics
# DAVIS FARM USE OF SLURM AND PROFILES: http://bluegenes.github.io/posts/
# useful for later: merging files: (caution! this isn't docs for the most up-to-date snakemake)
"Here is an example where you want to merge N files together, but if N == 1 a symlink will do." https://snakemake.readthedocs.io/en/v3.9.1/project_info/faq.html


--shadow-prefix=/scratch/<username>
--use-conda
--number of submissions to slurm at a time etc.
shadow: "minimal"
# minimal only symlinks all the input files .. so make sure they are all the rule needs to access to run!

# TO-DO: get it to run, then make a .profile for basic slurm setup, check still runs
I made a profile for slurm: mkdir -p ~/.config/snakemake/slurm
nano ~/.config/snakemake/slurm/config.yaml
jobs: 100
cluster: sbatch
use-conda: true
# TO RUN WITH THIS PROFILE:
snakemake --profile slurm
# COOL -- ADD MORE MEMORY BASED ON ATTEMPT # https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html

hmm...maybe resources can only take specific values (but seems not to be the case with high_mem_mb)

# then add additional quality check rules.
fast_QC
fastq_screen
samtools file information

# mark temporary output files
# then get your list of files 2 run. show DAG. start running.



snakemake -n # dry--run. could be along list!
snakemake -n --quiet # gives just a summary if there are LOTS of files
snakemake --dag **blah blah**

# don't want to see all the file paths, just the rules?
snakemake --rulegraph | dot -Tsvg > ruleGraph.svg


module load bio3
source activate hilo-env
snakemake results/TEST/b.bam --jobs 2 --cluster-config submit.json --cluster "sbatch -p {cluster.p} --mem {cluster.mem} --cpus-per-task {cluster.cpus-per-task} --time {cluster.time} --job-name {cluster.name} -e {cluster.e} -o {cluster.o}"

# ok in bash not main node
hilo$ srun --mem 8G -p med2 -t 2:00:00 --pty bash
source activate hilo-env
snakemake results/TEST/b.sort.bam --jobs 2 --cluster-config submit.json --cluster "sbatch -p {cluster.p} --mem {cluster.mem} --cpus-per-task {cluster.cpus-per-task} --time {cluster.time} --job-name {cluster.name} -e {cluster.e} -o {cluster.o}"


# create a snakemake copy of the environment.
ssh -p 2022 ecalfee@farm.cse.ucdavis.edu
module load conda3
source activate hilo-env
hilo$ snakemake -n all --use-conda --create-envs-only
Building DAG of jobs...
Conda environment ../envs/environment.yaml will be created.
hilo$ snakemake all --use-conda --create-envs-only



# anytime you update the conda environment:
ssh -p 2022 ecalfee@farm.cse.ucdavis.edu
cd hilo
hilo$ module load conda3
hilo$ conda env update --name hilo-env --file envs/environment.yaml --prune
hilo$ source activate hilo-env
hilo$ snakemake -n all --profile slurm --conda-create-envs-only
hilo$ snakemake all --profile slurm --conda-create-envs-only
#(use -n above if you just want to see which environments will be created)
hilo$ conda deactivate
hilo$ exit  # now log out of farm and re-log in without -p 2022 to run snakemake

# to run pipeline:
 start screen.
 screen -S snake
 snakemake all --profile slurm
[detached from 21715.snake] # hmm. things didn't run b/c low priority. also accidentally put screen on an sru nso lost it after I cancelled ind stalled jobs

# check after a bit if it ran through .. any errors?
# look at multiqc reports and library in bam file. samtools View bam
# then change TEST to your prefix. Commit DAG and simplified DAG.
# update GIT.
# Look at DAG. Look at simplified DAG. Then run all.
snakemake --dag all | dot -Tsvg > dag.svg
snakemake all --profile slurm
[detached from 32676.snake_run] # it's running in screen now


# argh, can't use this to parse wildcards in my 'profile', oh well: print(*{wildcards}.values(), sep = ".")





### ok so my original memory/time estimates were really high, possibly preventing my jobs from being scheduled efficiently. I added more dynamic memory with 3 total attempts for failed jobs and less intense status checking to reduce impact to farm (once every 2 sec)
### I cancelled original running snakemake process, but only had to cancel 1 individual job (let others scheduled run until complete).
### now running pipeline again:
ecalfee@farm:~/hilo$ screen -r 32676.snake_run
[detached from 32676.snake_run] 8.20.20 11pm
# two jobs got hung up (suspended). So I cancelled them. but snakemake doesn't seem to be rescheduling them.
ecalfee@farm:~$ squeue -u ecalfee
         JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
      21613385   bigmemm bwa_map.  ecalfee  S     1:55:54      1 16  24G    bigmem3
      21612650   bigmemm bwa_map.  ecalfee  S     2:34:50      1 16  24G    bigmem3
ecalfee@farm:~$ screen -ls
There is a screen on:
	32676.snake_run	(05/19/2020 10:57:41 PM)	(Detached)
1 Socket in /run/screen/S-ecalfee.
ecalfee@farm:~$ scancel 21613385 21612650
ecalfee@farm:~$ screen -r 32676.snake_run
# HILO306 and HILO317 are the ones that got hung up and need to be rerun. I will also check what would need to be rerun if I included all the older files too.

# first make combined list with all samples except those affected by label errors:
(hilo-env) ecalfee@c6-97:~/hilo/data/HILO_raw_reads$ (cut -f1 merged_all.list; cat April2020_IDs.list) > Combined_IDs.list
(hilo-env) ecalfee@c6-97:~/hilo/data/HILO_raw_reads$ (cut -f3 merged_all.list; cat April2020_libraries.list) > Combined_libraries.list
~/hilo/data/HILO_raw_reads$ (cut -f2 merged_all.list; cat April2020_lanes.list) > Combined_lanes.list

# I had to rename the picard and fastQC reports to get snake to recognize samples were added: multiqc_report.html.backup
# I need to simlink the fastqs for March2018 and simlink (or just copy over) all of the mark duplicate metrics from previous runs to the correct directory.
snakemake -n --quiet all --profile slurm
# to keep directory naming consistent for snakemake to find files, I created symlinks for all March2018 fastqs into the March2018/ directory, not just March2018_all/
# only HILO71 was previously symlinked into this directory
(hilo-env) ecalfee@c6-97:~/hilo/data/HILO_raw_reads$ parallel 'ln -s $(ls /group/jrigrp6/DanAlignments/{1}/{1}_*_{2}.fq.gz) March2018/{1}_{2}.fq.gz' :::: March2018_all_IDs.list ::: 1 2
<<gnu parallel message>>
ln: failed to create symbolic link 'March2018/HILO71_1.fq.gz': File exists
ln: failed to create symbolic link 'March2018/HILO71_2.fq.gz': File exists
parallel 'ln -s $(ls /group/jrigrp6/DanAlignments/{1}/{1}_*_{2}.fq.gz) March2018/{1}_{2}.fq.gz' :::: March2018_all_IDs.list ::: 1 2

# updated time-stamp for symbolic links to show snakemake these files are current:
(hilo-env) hilo$ parallel 'ln -s $(ls /home/ecalfee/hilo/data/hilo_bam_mapped2v4_allContigs/metrics/hilo_{1}.metrics.txt) filtered_bams/metrics/picard/March2018/HILO{1}.metrics.txt' ::: {1..70} {72..79} {81..200}
hilo$ parallel 'unlink filtered_bams/results/March2018/HILO{1}.sort.dedup.baq.bam; ln -s /home/ecalfee/hilo/data/hilo_bam_mapped2v4_allContigs/results/hilo_{1}.sort.dedup.baq.bam filtered_bams/results/March2018/HILO{1}.sort.dedup.baq.bam; ' ::: {1..70} {72..79} {81..200}
hilo$ parallel 'unlink filtered_bams/results/March2018/HILO{1}.sort.dedup.baq.bam.bai; ln -s /home/ecalfee/hilo/data/hilo_bam_mapped2v4_allContigs/results/hilo_{1}.sort.dedup.baq.bam.bai filtered_bams/results/March2018/HILO{1}.sort.dedup.baq.bam.bai; ' ::: {1..70} {72..79} {81..200}

# good! now snakemake doesn't want to remake these files anymore :)
(hilo-env) ecalfee@c6-97:~/hilo$ snakemake -n --quiet all --profile slurm
Job counts:
	count	jobs
	1	all
	2	bwa_map
	1004	fastQC
	1	multiQC_fastQC
	1	multiQC_flagstat
	1	multiQC_picard
	2	remove_duplicates
	315	samtools_flagstat
	2	samtools_index
	2	samtools_sort
	1331
# now I will stop and restart snakemake on my screen instance:
snakemake all --profile slurm
# running: [detached from 32676.snake_run]
# all ran fine except HILO1 b/c fastq_1 for that file appears to have been truncated!
hilo$ less filtered_bams/snake_logs/fastQC.ID\=HILO1\,LANE\=March2018\,READ\=1.21816*err
...
Approx 95% complete for HILO1_1.fq.gz
Failed to process file HILO1_1.fq.gz
uk.ac.babraham.FastQC.Sequence.SequenceFormatException: Ran out of data in the middle of a fastq entry.  Your file is probably truncated
ecalfee@c6-97:~/hilo$ zcat data/HILO_raw_reads/March2018/HILO1_1.fq.gz | wc -l
gzip: data/HILO_raw_reads/March2018/HILO1_1.fq.gz: unexpected end of file
24590717
(hilo-env) ecalfee@c6-97:~/hilo$ zcat data/HILO_raw_reads/March2018/HILO1_2.fq.gz | wc -l
33132348
# When iplant is back online I can check if we have a backup. Looks like truncation likely happened after mapping/deduplication etc. so all the reads may be there somewhere.
# for now I will just do a 'touch' placeholder for that fastQC file so I can finish snakemake.
hilo$ touch filtered_bams/metrics/fastQC/March2018/HILO1_1_fastqc.html
hilo$ snakemake -n --quiet all --profile slurm
Job counts:
	count	jobs
	1	all
	1	multiQC_fastQC
	2
snakemake all --profile slurm
# ran out of time. Increased max time to 2hrs. Will rerun when I fix the dedup step also here:
# in my original MarkDuplicates I didn't actually remove duplicates. I add a rule rm_duplicates using samtools flag 0x0400
# then to use that rule I rename my prior output HILO*.sort.mrkdup.baq.bam.
# note this only applies to the April2020 data. previous data had duplicates removed during MarkDuplicates (can see in flagstat):
hilo/filtered_bams/results/April2020_1$ for i in {300..395}; do mv HILO$i.sort.dedup.baq.bam HILO$i.sort.mrkdup.baq.bam; done
hilo/filtered_bams/results/April2020_1$ for i in {300..395}; do touch HILO$i.sort.mrkdup.baq.bam; done
hilo/filtered_bams/results/April2020_2$ for i in {396..488}; do mv HILO$i.sort.dedup.baq.bam HILO$i.sort.mrkdup.baq.bam; done
hilo/filtered_bams/results/April2020_2$ for i in {396..488}; do touch HILO$i.sort.mrkdup.baq.bam; done
# now running snakemake all for 'Combined' samples again on screen:
snakemake all -n --quiet --profile slurm
Job counts:
	count	jobs
	1	all
	1	multiQC_fastQC
	1	multiQC_flagstat
	189	rm_duplicates
	189	samtools_flagstat
	189	samtools_index
	570

# following up on truncated HILO1 -- Dan replaced with original file from /group/jrigrp6/DanAlignments/HILO1/*_1.fq.gz. I should now re-do mapping for HILO1.
# checking checksums for all other files that the ones I used from DanAlignments are identical to the originals
ecalfee@bigmem3:~/hilo$ for i in {1..200}; do md5sum /group/jrigrp6/DanAlignments/HILO"$i"/*.fq.gz > data/HILO_raw_reads/MD5_checksums/March2018/HILO"$i"_MD5.txt; done
# ugh..I just put this into a short script to submit to sbatch instead:
hilo$ sbatch scripts/check_md5sums.sh
Submitted batch job 21891631
# original checksums are here: /group/jrigrp6/RILAB_data/HILO/raw_data/HILO1/MD5.txt
# now I can do a file difference check: (except I only want to check the 1st column)
hilo$ for i in {1..79} {81..200}; do diff <(cut -f1 /group/jrigrp6/RILAB_data/HILO/raw_data/HILO"$i"/MD5.txt -d" ") <(cut -f1 data/HILO_raw_reads/MD5_checksums/March2018/HILO"$i"_MD5.txt -d" "); done
# great! just HILO1 had a truncated file. I will remap HILO1 which snakemake detected automatically because the fastq_1 was updated by Dan:
# I need to update my iplant backup using irsync - COMPLETED.
I also fixed samtools_flagstat rule to read the bam properly with - in the pipe, so all 190 of those are also running too for April2020 sequences.
Job counts:
	count	jobs
	1	all
	1	bwa_map
	2	fastQC
	1	mark_duplicates
	1	multiQC_fastQC
	1	multiQC_flagstat
	1	multiQC_picard
	1	rm_duplicates
	190	samtools_flagstat
	1	samtools_index
	1	samtools_sort
	201
[detached from 32676.snake_run]  5.26.20 9pm
# annoyingly TIMEOUT (CANCELLED) wasn't caught by slurm as a job failure, so it didn't try again. I will up time manually and run again:
# e.g. output files:
'filtered_bams/snake_logs/samtools_flagstat.ID=HILO428,LANE=April2020_2.21898239.err'
'filtered_bams/snake_logs/samtools_flagstat.ID=HILO428,LANE=April2020_2.21898239.out'
# so I added threads and 2 hours (now 6 hrs total) for initial samtools flagstat run -- HILO301, HILO305, HILO428:
[detached from 32676.snake_run] #May 27, 9:30am. Finished super fast (not sure what caused TIMEOUT earlier)
# rerun multiqc ignoring test bams:
hilo$ snakemake multiQC_picard multiQC_flagstat multiQC_fastQC -f -n --quiet --profile slurm
Job counts:
	count	jobs
	1	multiQC_fastQC
	1	multiQC_flagstat
	1	multiQC_picard
	3
(hilo-env) ecalfee@c6-97:~/hilo$ snakemake multiQC_picard multiQC_flagstat multiQC_fastQC -f --profile slurm

# testing trimmomatic:
hilo$ snakemake -f /home/ecalfee/hilo/data/HILO_trimmed_reads/TEST/a_1.fq.gz --profile slurm
# added TRUE . now seeing if it successfully removes adapters from the small test sets of reads:
snakemake metrics/fastQC_trimmed/multiqc/multiqc_report.html --profile slurm
# looks good on fastQC report!

# I made an alternative 'some' rule to the 'all' rule to just make trimmed_reads multiQC report and do bwa -> sorting reads. ack! in testing it removes temporary output file. So I get rid of the temp for now.
# first tested with just TEST/a.fq and b.fq. Now running with Combined sample:
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        502     bwa_map
        1004    fastQC_trimmed
        1       multiQC_fastQC_trimmed
        502     samtools_sort
        1       some
        502     trimmomatic
        2512
snakemake some --profile slurm
[detached from 32676.snake_run] # May 28, 2020 11:39pm

# I will have to decide later whether I want to cap base quality scores with BAQ, so I don't dedup yet in this run:
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
	count	jobs
	1	all
	502	bwa_map
	1004	fastQC_trimmed
	1	multiQC_fastQC_trimmed
	502	samtools_sort
	502	trimmomatic
	2512

# snakemake seemed frozen from log on Jun 4, so I did CTRL+C:
Terminating processes on user request, this might take some time.
Will exit after finishing currently running jobs.
Complete log: /home/ecalfee/hilo/.snakemake/log/2020-05-28T233738.667066.snakemake.log
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        1       bwa_map
        4       fastQC_trimmed
        1       multiQC_fastQC_trimmed
        1       samtools_sort
        1       some
        1       trimmomatic
        9
# REMINDER, HOW TO USE SCREEN: https://linuxize.com/post/how-to-use-linux-screen/
# ok, restarting these failed jobs (can't determined why they failed from logs, so could be random)
hilo$ snakemake some --profile slurm # RUNNING 6.25.2020
CRTL+A D
[detached from 32676.snake_run]


TEST ANGSD MIXING BAMS FROM GENOMES WITH DIFF SORT ORDERS:
# take 10 maize from hilo
# and 10 maize from Palmar Chico maize pop

# are the number of SNPs reasonable? Are they all at 50% freq?
hilo/test$ angsd -out test_10JRI_10alloMAIZE -r 1:100000000-100010000 -ref ../data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa -bam maize_10JRI_10alloMAIZE.bams -remove_bads 1 -minMapQ 30 -minQ 20 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -P 1 -doMajorMinor 2 -checkBamHeaders 0
hilo/test$ samtools view -bs 42.001 /home/jri/projects/ibd/JRIAL1/data/JRIAL1-2_srt_dedup.bam > small_JRIAL1-2_srt_dedup.bam
# index this bam
# try to run test2.bams -- has small_JRIAL1-2_srt_dedup.bam and some hilo's
hilo/test$ angsd -out test2 -r 2:100000000-110000000 -ref ../data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa -bam test2.bams -remove_bads 1 -minMapQ 30 -minQ 20 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -P 1 -doMajorMinor 2 -checkBamHeaders 0
# again but without this 1 bam from palmar chico = test2b:
angsd -out test2b -r 2:100000000-110000000 -ref ../data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa -bam test2b.bams -remove_bads 1 -minMapQ 30 -minQ 20 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -P 1 -doMajorMinor 2 -checkBamHeaders 0

# then re-sort according to the my genome
hilo/test$ samtools sort -m 6G -@ 1 -T tmp -O bam small_JRIAL1-2_srt_dedup.bam > small_JRIAL1-2_srt_dedup.resorted.bam
# and try to run test3 -- has small_JRIAL1-2_srt_dedup.resorted.bam and some hilo's
# the resulting SNP files should be the same from these two approaches if it's working
hilo/test$ angsd -out test3 -r 2:100000000-110000000 -ref ../data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa -bam test3.bams -remove_bads 1 -minMapQ 30 -minQ 20 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -P 1 -doMajorMinor 2 -checkBamHeaders 1

# hmm...still headers don't match. Try picard ReorderSam:
module load java
module load picardtools/2.7.1
java -Xmx6g -jar $PICARD/picard.jar ReorderSam \
INPUT= small_JRIAL1-2_srt_dedup.bam OUTPUT=small_JRIAL1-2_srt_dedup.reordered.bam \
REFERENCE=../data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa
test4.bams has the reordered bam file:
samtools index small_JRIAL1-2_srt_dedup.reordered.bam
hilo/test$ angsd -out test4 -r 2:100000000-110000000 -ref ../data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa -bam test4.bams -remove_bads 1 -minMapQ 30 -minQ 20 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -P 1 -doMajorMinor 2
# runs just fine without checking bam headers ..
# maize palmar chico needs to run ReorderSam (above) + dedup marked dups + filter to mapQ>=1
hilo/test$ samtools view -bF 0x400 -q 1 small_JRIAL1-2_srt_dedup.bam | samtools flagstat -
# ok actually I don't want to copy these all over (> 2T). As long as I just use autosomes, the order is the same in bams, so angsd should work with -checkBamHeaders flag
# BUT I do need to make sure to use the -removeBads 1 and -mapQ 30 and -baq 2 in ANGSD because these files
# have no pre-computed baq and only actually mark (not remove) duplicate reads

# make symlinks for all maize 55 data to the filtered bams folders
hilo/filtered_bams/results/Maize55$ for i in {1..55}; do ln -s /home/jri/projects/ibd/JRIAL1/data/JRIAL1-"$i"_srt_dedup.bam MAIZ"$i".sort.dedup.bam; done

# rerun snakemake to dedup all hilo and then index everything, incl. maize 55
# note that maize55 I got pre-mapped (w/ bwa) and duplicates marked, but all reads included (even unmapped)
screen -r 32676.snake_run
(IN SCREEN:)
Job counts:
	count	jobs
	1	all
	502	mark_duplicates
	1	multiQC_flagstat
	557	samtools_flagstat
	557	samtools_index
	1618
hilo$ snakemake all --profile slurm
CTRL+A+D (detached screen. June 26, 2020 11:55pm)
# ran fine. Now I merge bams for the same individual. Syntax here in snakemake is a bit tricky. shell only (not run) is required to use conda
snakemake all -n --profile slurm
# running test case with a/b files 6.30.2020. Oops next time don't test with multiQC reports (will inclue a & b test files in there too)
screen -r 32676.snake_run
[detached from 32676.snake_run]
# oops. merge bams not just for the sample cases:
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        520     merge_bams
        521
(hilo-env) ecalfee@farm:~/hilo$ snakemake all --profile slurm # running 7.7.2020

# calculating depth for minQ > 20:
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n calc_depth --profile slurm
Building DAG of jobs...
Job counts:
        count   jobs
        1       calc_depth
        1

[Tue Jul  7 01:46:57 2020]
rule calc_depth:
    input: ../samples/HILO_MAIZE55_bams.list, /home/ecalfee/hilo/data/refMaize/random_regions/N1000.L100.regions
    output: results/depthCov/N1000.L100.regions/HILO_MAIZE55.Q20.depthGlobal
    jobid: 0
    threads: 4
    resources: time_min=720, mem=32
# make output directory for logs!
mkdir variant_sites/snake_logs
# now run:
hilo$ snakemake calc_depth --profile slurm # didn't work. Now I've fixed the working directory issue (always just points to hilo/ for variant_sites Snakemake)
hilo$ snakemake calc_depth --profile slurm # 7.7.2020 RUNNING: Submitted job 0 with external jobid '24113628'
# now calling SNPs using a max depth cutoff that's read in from the output of the calc_depth rule
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n --quiet all --profile slurm
Job counts:
        count   jobs
        1       all
        426     call_SNPs
        427
# running 7.7.2020 6pm . oops typo didn't work. trying again:
snakemake -n --quiet all  --profile slurm
Job counts:
        count   jobs
        1       all
        426     call_SNPs
        426     make_sites_file
        853

# most but not all call_SNPs completed. none of the make_sites_file rule was successful.
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n --quiet all --profile slurm
Job counts:
        count   jobs
        1       all
        426     calc_rmap_pos
        116     call_SNPs
        426     make_sites_file
        969
# testing 1 file:
hilo$ snakemake variant_sites/results/HILO_MAIZE55/region_1.rpos --profile slurm
Job counts:
        count   jobs
        1       calc_rmap_pos
        1       make_sites_file
        2
# worked great. Doing 2 more files so I can also test my regions script:
hilo$ snakemake variant_sites/results/HILO_MAIZE55/region_0.rpos variant_sites/results/HILO_MAIZE55/region_300.rpos --profile slurm
# testing regions script with just 0, 1, 300 regions for now (need to change back in global_ancestry/Snakefile!!)
snakemake -n --quiet some --profile slurm
Job counts:
        count   jobs
        1       some
        1       thin_GL_4PCA
        2
No rule to produce variant_sites/results/HILO_MAIZE55/region_3.mafs.gz (if you use input functions make sure that they don't raise unexpected exceptions).
# ok thin_GL_4PCA still has issues (troubleshoot on mac). but re-running the failed jobs from call_SNPs for now:
hilo$ snakemake -n --quiet all --profile slurm --jobs 200
Job counts:
        count   jobs
        1       all
        423     calc_rmap_pos
        116     call_SNPs
        423     make_sites_file
        963
(hilo-env) ecalfee@farm:~/hilo$ snakemake all --profile slurm --jobs 200 # running 7.8.2020 all on medium priority but increased max to 200 jobs
[detached from 32676.snake_run] # ack! not sure what the deal is with farm but I won't use --jobs to override again (maybe overrides all defaults too?? like pinging the server? or could be completely unrelated)
# testing locally with a subset of regions. Need to specify --use-conda to run in updated conda environment
 Erins-MacBook-Air:hilo Erin$ snakemake test --use-conda
# can't update my conda environment on mac because of linux-ony pcangsd install :( but it runs on farm so ok

# rerunning several files because 'regions_dict' wasn't found (I think it doesn't freeze the Snakefile even when running..)
hilo$ snakemake -n --quiet all --profile slurm
Job counts:
        count   jobs
        1       all
        392     calc_rmap_pos
        1       call_SNPs
        339     make_sites_file
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        736
hilo$ snakemake all --profile slurm # running 7.8.2020
[detached from 32676.snake_run]

TO DO: make snakemake rules for all of the pcangsd and ngsadmix plots that you want
# NGSAdmix proportion by elevation, a PCA plot. I'll need metadata for the new maize.
# You can use the very short combined GL file as a template.

# made a file with half of the regions:
ecalfee@c6-89:~/hilo$ awk -v N=2 '(NR + 1) % N == 0' data/refMaize/divide_5Mb/ALL_regions.list > data/refMaize/divide_5Mb/HALF_regions.list
ecalfee@c6-89:~/hilo$ grep 'region_361' data/refMaize/divide_5Mb/HALF_regions.list # problem region that hasn't finished isn't included
ecalfee@c6-89:~/hilo$ head data/refMaize/divide_5Mb/HALF_regions.list
# just picked a small set of regions for a test file (downloading now from farm to laptop):
hilo Erin$ for i in 0 1 50 51 100 101 150 151 200 201 250 251 300 301 350 351 400 401; do scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/variant_sites/results/HILO_MAIZE55/region_$i.beagle.gz variant_sites/results/HILO_MAIZE55/.; done
# running test on mac for just these few regions:
(hilo-env) Erins-MacBook-Air:hilo Erin$ snakemake test
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Unlimited resources: mem, time_min
Job counts:
	count	jobs
	1	test
	1	test_NGSAdmix
	1	test_thin_GL_4PCA
	3

# got script to run to make lists of input files by population,
# but still can't get plotting script to work for just counts (not sure why -- will check)
# now running test of getting allopatric allele frequencies (to thin sites for local ancestry)
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        1       allo_freqs
        1       list_total_samples
        1       some
        3
# ok now that I fixed angsd indexing (needs .bin file too) the example in 'some' did run.
# will try running all of the regions to get MAF for allopatric references
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        851     allo_freqs
        424     index_sites_file
        1       plot_total_samples
        1277
# now running 'all'
[detached from 32676.snake_run] # Jul 19, 11:33pm
# looks like all allopatric mexicana ran overnight but there was a problem with bam file lists for allopatic maize. fixing now:
hilo$ snakemake -f list_total_samples --profile slurm
1 of 1 steps (100%) done
# now using 'touch' to avoid re-doing all allopatric mexicana (just want to re-do maize)
# and fixed small typo in plot_total_samples_pass.R (should work now)
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        426     allo_freqs
        1       plot_total_samples
        428
        # RUNNING 7.20.20 11:37am
# oops still not finding correct MAIZE55 bams. Fixed list of bams. re-did list_total_samples and 'touch' for allopatric mexicana mafs.gz files.
hilo$ snakemake -n all --quiet --profile slurm
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        426     allo_freqs
        1       plot_total_samples
        428 # RUNNING 7.20.20 12:30pm
# these jobs finished. adding PCA plots and running rule 'all' again:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       plot_NGSAdmix
        1       plot_PCAngsd
        1       plot_total_samples
        1       results_NGSAdmix
        5 # runnning 7.20.20 9pm
# SWITCHED to thinning every 100th SNP for PCA/NGSAdmix instead of every .01cM (so I don't get over-representation of high recombination regions)
# moved old results to Old_0.1cM subdirectories for thinnedSNPs, PCA and NGSAdmix. Updated Snakefile to rerun.
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       plot_NGSAdmix
        1       plot_PCAngsd
        1       results_NGSAdmix
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        7 # running 7.21.20 6:25pm
# every Nth is not working in Snakefile. made it's own script and rerunnig:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       plot_NGSAdmix
        1       plot_PCAngsd
        1       results_NGSAdmix
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        7 # running 7.21.20 12:36am
# rerunning with slight adjustments to plots:
hilo$ snakemake -n plot_PCAngsd plot_NGSAdmix --quiet --profile slurm
Job counts:
        count   jobs
        1       plot_NGSAdmix
        1       plot_PCAngsd
        2 # 7.22.20 10:15am
# added latex table output (updated conda environment for r-xtable) and re-running again:
hilo$ snakemake -n -f plot_PCAngsd plot_NGSAdmix --quiet --profile slurm
Job counts:
        count   jobs
        1       plot_NGSAdmix
        1       plot_PCAngsd
        2 # running 7.22.20 11:20am

# HMMM... the cubic spline "hyman" extrapolates out to some negative recombination map distances (!).
# I am calculating rpos based on linear approximation from the extended map, and saving old files in ls variant_sites/results/HILO_MAIZE55/splines_old/
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        426     calc_rmap_pos
        427 # running 8.3.2020 10pm. COMPLETED.

# made pipeline to do 100 bootstraps of global ancestry ~ recombination quintile (1cM window)
snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       define_1cM_windows
        1520    find_SNPs_in_window
        1       make_bed_file_for_windows
        505     make_bootstrap_GL_file
        1       sample_bootstrap_1cM
        2029 # running 8.4.2020 5pm. Did not work b/c it's circular to read file to get the number of 1cM windows -- have to provide that separately
        hilo$ snakemake -n all --quiet --profile slurm

Job counts:
        count   jobs
        1       all
        1       define_1cM_windows
        1520    find_SNPs_in_window
        505     label_k_anc
        1       make_bed_file_for_windows
        505     make_bootstrap_GL_file
        505     run_boostrap_NGSAdmix
        1       sample_bootstrap_1cM
        3039 # running 8.4.2020 11pm
# oops forgot double brackets in R scripts to call snakemake variables. fixed and re-trying.
# awk to find SNPs in windows still not working. added it's own bash script for awk and testing just this piece:
hilo$ snakemake ancestry_by_r/results/GL_1cM/HILO_MAIZE55/W1.beagle.gz --quiet --profile slurm
Job counts:
        count   jobs
        1       find_SNPs_in_window
        1 # finally ran through without errors. Now running all pieces:
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                1519    find_SNPs_in_window
                505     label_k_anc
                505     make_bootstrap_GL_file
                505     run_boostrap_NGSAdmix
                1       sample_bootstrap_1cM
                3036 # running 8.5.2020 10am. only ran through find_SNPs_in_window
hilo$ snakemake all --quiet --profile slurm
Job counts:
      count   jobs
      1       all
      505     label_k_anc
      505     make_bootstrap_GL_file
      505     run_boostrap_NGSAdmix
      1       sample_bootstrap_1cM
      1517 # fixed typo in path of sample_bootstrap_1cM. rerunning.

# ok problem is that the bin_r5 is not a factor with defined levels.
# so I added a column quintile_r5 that has values 1-5.
# I will force rerun define_1cM_windows and make_bed_file_for_windows
# and use touch to not have to redo find_SNPs_in_window, which would be unaffected
hilo$ snakemake -f define_1cM_windows make_bed_file_for_windows --quiet --profile slurm
Job counts:
        count   jobs
        1       define_1cM_windows
        1       make_bed_file_for_windows
        2 # COMPLETED 8.5.2020
hilo$ touch ancestry_by_r/results/GL_1cM/HILO_MAIZE55/W*.beagle.gz
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        505     label_k_anc
        505     make_bootstrap_GL_file
        505     run_boostrap_NGSAdmix
        1       sample_bootstrap_1cM
        1517 # RUNNING 8.5.2020

# NOTE: don't use any kind of shadow when running a bash script through 'shell' (or put script in as a parameter..)
# In general I don't need 'shadow' unless there's lots of reading and writing.
hilo$ snakemake -n all --quiet --profile slurm                                    Job counts:
        count   jobs
        1       all
        505     label_k_anc
        504     make_bootstrap_GL_file
        505     run_boostrap_NGSAdmix
        1515 # RUNNING 8.5.2020 6pm. hmm didn't work. debugged make_bootstrap_GL_file, label_k_anc and run_bootstrap_NGSAdmix one at a time with 1 file each

hilo$ snakemake all --quiet --profile slurm
Job counts:
      count   jobs
      1       all
      504     label_k_anc
      504     run_bootstrap_NGSAdmix
      1009 # running 8.6.2020 11am. Ran through:
# now making plots (remaking some others because I updated colors.R)
Job counts:
        count   jobs
        1       all
        1       plot_NGSAdmix
        1       plot_PCAngsd
        1       plot_bootstrap_anc_by_r
        4 # RUNNING 8.6.2020 4pm. COMPLETED ALL PLOTS
# rerunning population alphas output from ngsadmix to use as input for ancestry_hmm (admixture proportion prior)
hilo$ snakemake -f global_ancestry/results/NGSAdmix/HILO_MAIZE55/K2_alphas_by_symp_pop.txt --quiet --profile slurm
Job counts:
        count   jobs
        1       results_NGSAdmix
        1 # RUNNING 8.6.2020 5pm
hilo$ snakemake -n all --quiet --profile slurm
      Job counts:
      count   jobs
      1       all
      1       plot_NGSAdmix
      1       thin_AIMs
      3
# ERROR in thin_sites_4HMM. problem was NAs in rpos files. remaking those files:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        426     calc_rmap_pos
        1       thin_sites_4HMM
        428 # RUNNING 8.11.2020. Again didn't work. Fixed reading in columns of extended map, deleted rpos files with lots of NAs and rerunning.
# thin sites ran. trying one file to test counts pipeline:
hilo$ snakemake local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO2.counts.txt --quiet --profile slurm
Job counts:
        count   jobs
        1       count_ACGT
        1       count_maj_min
        1       index_sites_4HMM
        3 # made index of sites ok and started on ACGT counts but then I stopped the job I could run all of the samples:
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        403     count_ACGT
        403     count_maj_min
        1       some
        807 # RUNNING 8.11.2020
# note the above is slightly inefficient because I have it make a counts file for all included samples (all_ids) when some sympatric samples will be excluded for coverage < 0.5x
# but rather than have dynamic inputs for each population, I can use all of these counts files as inputs for each of the following rules to make the pop input files.
farm:~/hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        49      count_ACGT
        49      count_maj_min
        1       some
        99 # some need to be rerun with higher maximum time limit (I added a thread and changed from 4 to 12 hrs because MAIZE55 bams are very large). Unfortunately, the "TIMEOUT" didn't trigger a "FAIL" and rerun.
# RUNNING 8.12.2020 9:30am


### HMM... so I want the lists of bams/IDs to be outside of my Snakemake pipeline
# so that I can use them as inputs.
# I can force rerun: rule list_total_samples in filtered_bams/Snakefile
# then use 'touch' to update downstream files (because it will be the same outputs, because results/Maize55/*.bam is symlinked to results/merged_bams/*.bam)
# then I can comment out that code and use the output files to fill a dictionary of IDs for each population
hilo$ snakemake -f list_total_samples --quiet --profile slurm
Job counts:
        count   jobs
        1       list_total_samples
        1 # RUNNING 8.13.2020 11:45am
This fixes ALLOPATRIC MAIZE LOOKS LIKE IT COMES FROM A DIFFERENT DIRECTORY IN THE ALL_byPop files! (but not Over0.5x_byPop files). this is just a symlink issue, both link to the same underlying files. But I should fix so the symlink name pattern is consistent across all bams.
# there are many downstream files that depend on list_total_samples:
        hilo$ snakemake -n all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                852     allo_freqs
                403     count_ACGT
                403     count_maj_min
                1       index_sites_4HMM
                1       plot_total_samples
                1       thin_sites_4HMM
                1662
# touch so I don't have to remake downstream files (because will be the same since inputs changed only by name (symlink) not content)
hilo$ touch variant_sites/results/popFreq/allopatric_*/*
hilo$ touch local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.var.sites
hilo$ touch local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.*
hilo$ touch local_ancestry/results/thinnedSNPs/HILO_MAIZE55/counts_thinned_aims.txt
hilo$ touch samples/HILO_MAIZE55_byInd/*_bams.list
hilo$ touch local_ancestry/results/countsACGT/HILO_MAIZE55/*
hilo$ touch local_ancestry/results/countsMajMin/HILO_MAIZE55/*
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       plot_total_samples
        2 # DONE
# commented out list_total_samples rule and added all sample files to git!

# now trying to run ancestry_hmm with a dummy output file with the POP name (because can't have a rule define individual sample posterior outputs)
hilo$ snakemake -n local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_noBoot/pop360.completed --quiet --profile slurm
Job counts:
        count   jobs
        1       make_allo_counts
        1       make_input_hmm
        1       run_ancestry_hmm
        3 # only make_allo_counts ran. troubleshooting hmm input. need to fix spacing on allo_counts, so deleting that output and rerunning:
hilo$ snakemake local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_noBoot/pop360.completed --quiet --profile slurm
Job counts:
        count   jobs
        1       make_allo_counts
        1       make_input_hmm
        1       run_ancestry_hmm
        3 # redoing same thing -- input file not quite right (repeated maize instead of maize, then mex allele counts! and needed no scientific notation for position difference in Morgans)
# still breaks at run_ancestry_hmm. I needed to fix the dictionary to retrieve alphas. now done:
hilo$ snakemake local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_noBoot/pop360.completed --quiet --profile slurm
Job counts:
        count   jobs
        1       run_ancestry_hmm
        1

# oops ploidy file should be tab separated. rerunning:
hilo$ snakemake local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_noBoot/pop360.completed --quiet --profile slurm
Job counts:
                count   jobs
                1       make_input_hmm
                1       run_ancestry_hmm
                2 # ran but had trouble creating .completed output file . troubleshooting. Now ran.
# testing posterior to ancestry files:
hilo$ snakemake local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_noBoot/anc/pop360.anc.freq --quiet --profile slurm
Job counts:
        count   jobs
        1       summarise_posterior
        1
# running the remaining pops:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        27      make_input_hmm
        27      run_ancestry_hmm
        27      summarise_posterior
        82 # RUNNING 8.25.2020 5pm. COMPLETED.

# hint for screen if ssh disconnects while it's still attached:
screen -rd is pretty much the standard way to attach to an existing screen session.

# now with bootstrapping:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        28      summarise_posterior
        57 RUNNING 8.25.2020 11pm
# OOPS! I identified an error in the count_maj_min.R script (was not sorting to correct order because there are many chr and positions and only sorted by pos.)
# doing 'touch' to ACGT counts (those files are ok) to re-do all downstream steps (starting with maj/min counts):
hilo$ touch local_ancestry/results/countsACGT/HILO_MAIZE55/*.counts.gz
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        403     count_maj_min
        1       make_allo_counts
        28      make_input_hmm
        28      run_ancestry_hmm
        56      summarise_posterior
        545 # RUNNING 8.26.2020 2pm

# only takes a few hours to re-run. make left_join with SNPs for extra security the order is correct. can now re-run:
# also added error rate for read errors 3e-3 to ancestry_hmm scripts and expanded possible time range to 0-10000 generations (10k not 1k. maize made it to the highlands ~7k ago)
hilo$ touch local_ancestry/results/countsACGT/HILO_MAIZE55/*.counts.gz
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        403     count_maj_min
        1       make_allo_counts
        28      make_input_hmm
        28      run_ancestry_hmm
        56      summarise_posterior
        545 # RUNNING 8.26.2020 4pm. COMPLETED.
# making tracts for ancestry calls:
hilo$ snakemake -n local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.bed --quiet --profile slurm
Job counts:
        count   jobs
        1       get_tracts_from_sites
        1 # ran but am force re-running because need to correct 0-index on bed file
        hilo$ snakemake -f local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.bed --quiet --profile slurm
        Job counts:
                count   jobs
                1       get_tracts_from_sites
                1 # DONE (very fast)

# PROBLEM: there is an issue that nInd from maf.gz is counting some individuals with reads that must be subquality (or not maj/min allele)
# so that the allo_counts are not meeting minimum # individuals to use for local ancestry calling, especially in mexicana (some zeros!)
# I moved old -doMaf 8 data to variant_sites/results/popFreq/allopatric_mexicana_doMaf8
# and modified rule allo_freqs in Snakefile/variant_sites
# I'm testing if the same problem persists with -doMaf 1 which relies on a HW assumption but should be find for allopatric pops
hilo$ snakemake -f variant_sites/results/popFreq/allopatric_mexicana/region_0.mafs.gz variant_sites/results/popFreq/allopatric_mexicana/region_1.mafs.gz variant_sites/results/popFreq/allopatric_mexicana/region_300.mafs.gz --quiet --profile slurm
Job counts:
        count   jobs
        3       allo_freqs
        3 # RUNNING FOR JUST A FEW REGIONS FIRST. CAN THEN COMPARE nIND with the tot_mex in ACGT counts for overlapping SNPs
# ADDED -GL 1 and TRYING AGAIN (SAME JOBS). worked. also tested and this gets the right nIND for allopatric counts -- great!
# I moved allo_freqs rule to local_ancestry/Snakefile
# and am rerunning all regions now with gl 1 method.
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        852     allo_freqs
        28      boot_ancestry_hmm
        403     count_ACGT
        403     count_maj_min
        1       get_tracts_from_sites
        1       index_sites_4HMM
        1       make_allo_counts
        28      make_input_hmm
        28      run_ancestry_hmm
        56      summarise_posterior
        1       thin_sites_4HMM
        1803 # RUNNING 8.31.2020. oops need to update output file prefix parameter in rule allo_freqs. rerunning:
        hilo$ snakemake -n all --quiet --profile slurm
      Job counts:
              count   jobs
              1       all
              852     allo_freqs
              28      boot_ancestry_hmm
              403     count_ACGT
              403     count_maj_min
              1       get_tracts_from_sites
              1       index_sites_4HMM
              1       make_allo_counts
              28      make_input_hmm
              28      run_ancestry_hmm
              56      summarise_posterior
              1       thin_sites_4HMM
              1803 # RUNNING 8.31.2020 12:18pm
failed to open:
filtered_bams/merged_bams/MAIZE1.sort.dedup.bam.
oops it's because all_bams is looking for the other end of this symlink: filtered_bams/MAIZE55/MAIZE1.sort.dedup.bam
(LATER I can make a symlink rule for allopatric maize instead of 'merge' for these bams.)
for now I'll just directly specify input bams. and later I can check all places all_bams is used.
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        830     allo_freqs
        28      boot_ancestry_hmm
        403     count_ACGT
        403     count_maj_min
        1       get_tracts_from_sites
        1       index_sites_4HMM
        1       make_allo_counts
        28      make_input_hmm
        28      run_ancestry_hmm
        56      summarise_posterior
        1       thin_sites_4HMM
        1781 # RUNNING 8.31.2020 (note a few allo freqs for mexicana had finished already..only maize had the path errors)
        # check on output files from ancestry_hmm -- did it run? NOT COMPLETELY:
        # some allo_freqs (66) need to be re-run. won't restart on their own if out of time (only if out of memory ..)
                # so I added resources to allo_freqs rule. and will restart.
                # First I test other new rules: make_coding_bedfile:
        hilo$ snakemake make_coding_bedfile --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       make_coding_bedfile
                        1 # didn't run. fixing error and retry:
                        hilo$ snakemake make_coding_bed --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       make_coding_bed
                        1 # Yikes! Can't use 'head' in bash strict mode -- it sends a 141 error down the pipe (who knew?)
                        hilo$ snakemake -p make_coding_bed --profile slurm
                        Building DAG of jobs...
                        Using shell: /bin/bash
                        Provided cluster nodes: 100
                        Job counts:
                                count   jobs
                                1       make_coding_bed
                                1

                        [Tue Sep  1 16:52:20 2020]
                        rule make_coding_bed:
                            input: /home/ecalfee/hilo/data/refMaize/geneAnnotations/Zea_mays.B73_RefGen_v4.41.chr.gff3, /home/ecalfee/hilo/data/refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths
                            output: data/refMaize/geneAnnotations/Zea_mays.B73_RefGen_v4.41.chr.CDS.bed
                            jobid: 0
                            resources: time_min=5, mem=4


                                grep -v '^#' /home/ecalfee/hilo/data/refMaize/geneAnnotations/Zea_mays.B73_RefGen_v4.41.chr.gff3 | awk -v OFS='\t' '{print $1,$4,$5,$3}' |\
                                grep 'CDS' | grep -v '^Mt' | grep -v '^Pt' |\
                                bedtools sort -faidx /home/ecalfee/hilo/data/refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths -i stdin |\
                                bedtools merge -i stdin -c 4 -o distinct > data/refMaize/geneAnnotations/Zea_mays.B73_RefGen_v4.41.chr.CDS.bed
                        # now completed make_coding_bed
hilo$ snakemake -f define_1cM_windows --quiet --profile slurm
    Job counts:
    count   jobs
    1       define_1cM_windows
    1 # COMPlETED. Now I'll rerun rule 'all' for the whole pipeline 9.1.2020.
    hilo$ snakemake all --quiet --profile slurm
    Job counts:
            count   jobs
            1       all
            66      allo_freqs
            28      boot_ancestry_hmm
            403     count_ACGT
            403     count_maj_min
            1520    find_SNPs_in_window
            1       get_tracts_from_sites
            1       index_sites_4HMM
            1010    label_k_anc
            1       make_allo_counts
            1       make_bed_file_for_windows
            1010    make_bootstrap_GL_file
            28      make_input_hmm
            1       plot_bootstrap_anc_by_r
            28      run_ancestry_hmm
            1010    run_bootstrap_NGSAdmix
            2       sample_bootstrap_1cM
            56      summarise_posterior
            1       thin_sites_4HMM
            5571 # RUNNING 9.2.2020 6pm
# because switched to -GL 1 needed to switch from 'phat' to 'knownEM' in thin_sites_4HMM
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        403     count_ACGT
        403     count_maj_min
        1       get_tracts_from_sites
        1       index_sites_4HMM
        1       make_allo_counts
        28      make_input_hmm
        28      run_ancestry_hmm
        56      summarise_posterior
        1       thin_sites_4HMM
        951 # RUNNING 10:45am 9.2.2020. Fixed directory typo for alloFreqs in thin_sites_4HMM:
# just retrying thin_sties_4HMM and summaries of bootstrap data
hilo$ snakemake -n thin_sites_4HMM ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/cd5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/r5_K2.Rdata --quiet --profile slurm
Job counts:
        count   jobs
        2       summarise_bootstrap_anc_by_r
        1       thin_sites_4HMM
        3 # RUNNING 2pm 9.2.2020. had typo in summarise_bootstrap. trying again:
hilo$ snakemake all --quiet --profile slurm                                                                                                   Job counts:
    count   jobs
    1       all
    28      boot_ancestry_hmm
    403     count_ACGT
    403     count_maj_min
    1       get_tracts_from_sites
    1       index_sites_4HMM
    1       make_allo_counts
    28      make_input_hmm
    28      run_ancestry_hmm
    2       summarise_bootstrap_anc_by_r
    56      summarise_posterior
    952 # RUNNING 9.2.2020. ancestry_hmm still failed ^^
# so making a 'test_ancestry_hmm' rule and using old pass2_alloMAIZE input files just to check if it's my script.
hilo$ mkdir -p local_ancestry/results/ancestry_hmm/TEST/input
hilo$ cp local_ancestry/results/ancestry_hmm/pass2_alloMAIZE/input/pop18.anc_hmm.input local_ancestry/results/ancestry_hmm/TEST/input/pop18.counts
hilo$ cp local_ancestry/results/ancestry_hmm/pass2_alloMAIZE/input/pop18.anc_hmm.ids.ploidy local_ancestry/results/ancestry_hmm/TEST/input/pop18.ploidy
hilo$ snakemake -n local_ancestry/results/ancestry_hmm/TEST/Ne10000_noBoot/pop18.completed --quiet --profile slurm
# hmm.. it ran just fine on the old files, no Nan's:
==> local_ancestry/results/ancestry_hmm/TEST/Ne10000_noBoot/posterior/HILO97.posterior <==
chrom	position	2,0	1,1	0,2
1	1961	0.159978	0.330604	0.509418
1	18830	0.191147	0.398637	0.410216
1	20171	0.187717	0.401802	0.410482
1	26286	0.173269	0.415068	0.411663
1	27214	0.301335	0.440951	0.257714
1	28687	0.163653	0.418246	0.418101
1	30261	0.155803	0.419309	0.424888
1	49527	0.0426273	0.354082	0.603291
1	51732	0.107311	0.467275	0.425414
# As compared to the new SNPs/files:
ecalfee@bigmem6:~/hilo$ head local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_noBoot/posterior/HILO97.posterior
chrom	position	2,0	1,1	0,2
1	15329	-nan	-nan	-nan
1	15841	-nan	-nan	-nan
1	18171	-nan	-nan	-nan
1	21860	-nan	-nan	-nan
1	22884	-nan	-nan	-nan
1	26555	-nan	-nan	-nan
1	27127	-nan	-nan	-nan
1	29829	-nan	-nan	-nan
1	30875	-nan	-nan	-nan

# So I'll look more closely at the input files,
e.g. TEST/input/pop18.counts and TEST/input/pop18.ploidy
vs. HILO_MAIZE55/input/pop18.counts and HILO_MAIZE55/input/pop18.ploidy
# ok so in the new dataset I manually set all non-starting rdiff that were == 1 to .00001 and will retry ancestry_hmm:
# these new files are in TEST2/input/ and I'll modify test_ancestry_hmm to run them for pop18
hilo$ snakemake local_ancestry/results/ancestry_hmm/TEST2/Ne10000_noBoot/pop18.completed --quiet --profile slurm
Job counts:
        count   jobs
        1       test_ancestry_hmm
        1 # RUNNING 9.3.2020 10am. TEST2 worked! can delete test rule now test_ancestry_hmm
# I fixed the error in thin_sites_4HMM that was setting the start of every region (not chr) to 1. And I'm rerunning from there:
hilo$ snakemake -n -f thin_sites_4HMM --quiet --profile slurm
Job counts:
        count   jobs
        1       thin_sites_4HMM
        1 # COMPLETED 10:30am. Now I'll rerun the rest. And add the new fig ouputs for ancestry_by_r too
hilo$ snakemake -f all --quiet --profile slurm
Job counts:
    count   jobs
    1       all
    28      boot_ancestry_hmm
    403     count_ACGT
    403     count_maj_min
    1       get_tracts_from_sites
    1       index_sites_4HMM
    1       make_allo_counts
    28      make_input_hmm
    1       plot_bootstrap_anc_by_r
    28      run_ancestry_hmm
    56      summarise_posterior
    951 # 9.3.2020 11:30am OOPS! ACCIDENTALLY RAN -F INSTEAD OF -N !!!
    # BUT all ok because I don't have a bunch of intermediate files (only final files)
    # in my ALL rule...so it's doing basically the same as snakemake -n all.
    # SOME ERROR IN plot_bootstrap_anc_by_r .. it was @params instead of @input for windows. Will rerun.
    # Also, a few count_ACGT and count_maj_min didn't finish properly .. will check on this
    hilo$ snakemake -n all --quiet --profile slurm
Job counts:
    count   jobs
    1       all
    28      boot_ancestry_hmm
    11      count_ACGT
    20      count_maj_min
    1       make_allo_counts
    28      make_input_hmm
    1       plot_bootstrap_anc_by_r
    28      run_ancestry_hmm
    56      summarise_posterior
    174 # RUNNING 9.4.2020 . Didn't all finish running.

# upped the time limit again becasue 24 hours wasn't enough for some of hte ACGT counts (not sure what' going $
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        6       count_ACGT
        10      count_maj_min
        1       make_allo_counts
        28      make_input_hmm
        1       plot_bootstrap_anc_by_r
        28      run_ancestry_hmm
        56      summarise_posterior
        159 # RUNNING 9.5.2020. Many jobs still ran out of time despite high limits. problem is slurm nodes, not job time limits. reducing time and rerunning:
        hilo$ snakemake -n all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                28      boot_ancestry_hmm
                3       count_ACGT
                8       count_maj_min
                1       make_allo_counts
                28      make_input_hmm
                1       plot_bootstrap_anc_by_r
                28      run_ancestry_hmm
                56      summarise_posterior
                154 # RUNNING 9.8.2020 1:30pm (should finish before 5am)

# UGH HILO42 counts got stalled but I want to look at prelim results.
ecalfee@bigmem3:~/hilo$ squeue -u ecalfee
         JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
      25821166      med2 count_AC  ecalfee  S    10:12:14      1 2   8G     c6-89
# SO, FOR THAT FILE ONLY, I'LL USE THE OLDER DATA AND THEN REPLACE LATER:
hilo$ ls -l local_ancestry/results/countsACGT/HILO_MAIZE55/HILO42.*
-rw-rw-r-- 1 ecalfee ecalfee  493268 Sep  3 12:05 local_ancestry/results/countsACGT/HILO_MAIZE55/HILO42.counts.gz
-rw-rw-r-- 1 ecalfee ecalfee 1623390 Sep  3 12:05 local_ancestry/results/countsACGT/HILO_MAIZE55/HILO42.pos.gz
hilo$ mkdir -p local_ancestry/results/countsACGT/BACKUP_OLD
hilo$ ls -l local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO42.*
-rw-rw-r-- 1 ecalfee ecalfee 1895159 Sep  3 18:49 local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO42.counts.txt
hilo$ cp local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO42.counts.txt local_ancestry/results/countsMajMin/BACKUP_OLD/.
hilo$ touch local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO42.counts.txt
ecalfee@bigmem3:~/hilo$ date
Wed Sep  9 06:30:01 PDT 2020 # SO WHEN THE NEW FILE FINALLY RUNS IT WILL HAVE A MORE RECENT TIME STAMP AND
# I'LL HAVE TO FORCE RERUN MAJMIN AND THAT POP'S ANCESTRY_HMM (sympatric maize POP370)
# Now I CTRL+C cancel running snakemake (that one job will still con't running).
hilo$ snakemake plot_bootstrap_anc_by_r --quiet --profile slurm
Job counts:
        count   jobs
        1       plot_bootstrap_anc_by_r
        1 # COMPLETED.
        hilo$ ls -lt snake_logs/plot_boot*.out | head -n 1
        -rw-rw-r-- 1 ecalfee ecalfee 1051 Sep  9 06:42 snake_logs/plot_bootstrap_anc_by_r..25826671.out
        # this output file contains:
        [1] "Genomic feature correlation at 1cM window scale:"
        [1] "cM_Mb ~ frac_bp_coding"
        [1] 0.06325552
        [1] "quintile_r5 ~ quintile_frac5"
        [1] 0.487383
        [1] "log10(cM_Mb) ~ frac_bp_coding"
        [1] 0.4614461
        [1] "log10(cM_Mb_ ~ coding_bp"
        [1] -0.7767665
        [1] "quintile_r5 ~ quintile_cd5"
        [1] -0.8464101
# NOW TEMPORARILY COMMENTED OUT ancestry_by_r/Snakefile from snakefile imports so it builds DAG faster.
# checking which jobs need to be completed:
hilo$ snakemake -n all --profile slurm | less
hilo$ touch local_ancestry/results/countsACGT/HILO_MAIZE55/HILO42.* samples/HILO_MAIZE55_byInd/MAIZE42_bams.list
# wait several minutes
ecalfee@bigmem3:~/hilo$ touch local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO42.counts.txt
# STILL, AFTER 'TOUCH' snake still wants to re-run HILO42 ACGT and MajMin counts.. I'm not sure why
# to get around this for today, I made a temporary array of IDs that doesn't include HILO42:
TEMP_ALL_IDS_EXCEPT_HILO42 = [x for i,x in enumerate(all_ids) if x!="HILO42"]
hilo$ ls -lt snake_logs/count_ACGT* | head -n 1
# see tail, it's on chr10 (good!)

# rerunning the other counts_maj_min jobs that failed:
hilo$ snakemake -n local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO359.counts.txt local_ancestry/results/countsMajMin/HILO_MAIZE55/HILO416.counts.txt --quiet --profile slurm
Job counts:
        count   jobs
        2       count_maj_min
        2 # RUNNING 9.9.2020 8am
# HILO42 finally completed countsACGT. Now running the rest of the pipeline:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        2       combine_pop_ancestry_data
        1       count_maj_min
        1       make_allo_counts
        28      make_input_hmm
        28      run_ancestry_hmm
        56      summarise_posterior
        145 # RUNNING 9.9.10am
# was running but the bootstraps were slowing everything down. Putting them on bigmemm so they don't compete for resources.
hilo$ snakemake all --quiet --profile slurm
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      boot_ancestry_hmm
        2       combine_pop_ancestry_data
        26      run_ancestry_hmm
        56      summarise_posterior
        113 # RUNNING 9.9.2020 10:30am. STILL RUNNING bootstrap but others are working. adding things to 'some' rule.
        hilo$ snakemake -n some --profile slurm --quiet
        Job counts:
                count   jobs
                2       combine_pop_ancestry_data
                1       some
                3 # DIDN'T WORK, DIRECTORY 'LOCKED'. MUST WAIT FOR OTHER JOBS TO FINISH. 9.9.2020 11am
bedtools intersect -b wind.bed -a anc.bed | bedtools map -a stdin -b wind.bed -c 4 -o distinct
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       combine_pop_ancestry_data
        28      map_local_anc_onto_1cM_windows
        31 # SOME FAILED. Added bedtools sort to my pipeline for mapping ancestry onto windows.
        hilo$ snakemake all --quiet --profile slurm

# trying to run again summary by 1cM
Job counts:
                count   jobs
                1       all
                1       plot_bootstrap_anc_by_r
                28      summarise_local_anc_by_1cM
                30 # RUNNING 9.9.2020 2:45pm
hilo$ snakemake plot_bootstrap_anc_by_r --quiet --profile slurm
# getting timing of admixture:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        28      get_admix_times
        1       plot_bootstrap_anc_by_r
        30 # RUNNING 9.15.2020 12:30pm. Why did plot bootstrap fail? not clear. but get_admix_times ran ok.
        (hilo-env) ecalfee@farm:~/hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                28      boot_ancestry_hmm
                2       combine_pop_ancestry_data
                28      get_admix_times
                28      map_local_anc_onto_1cM_windows
                2       plot_admix_times
                1       plot_bootstrap_anc_by_r
                28      summarise_local_anc_by_1cM
                28      summarise_posterior
                146 # running with 100,000 as Ne. Ran through complete.
Wanted to re-do combining ancestry freqs so it saves the mean across individuals in the .RData file as well:
So I force a rerun:
farm:~/hilo$ snakemake -f local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_yesBoot/anc/mexicana.combined.anc.bed local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_yesBoot/anc/maize.combined.anc.bed local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne100000_yesBoot/anc/maize.combined.anc.bed local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne100000_yesBoot/anc/mexicana.combined.anc.bed --quiet --profile slurm
Job counts:
        count   jobs
        4       combine_pop_ancestry_data
        4 # RUNNING 9.16.2020
# then uncomment ancestry_by_r, add ZAnc/Snakefile, and run 'all' rule.
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       calc_K_matrix_zea
        2       plot_admix_times
        2       simulate_MVN
        7 # RUNNING. simulate_MVN ran out of time but should be very fast. trying again with low time threshold .. probably computer problem.
# commented out local_ancestry and ancestry_by_r (b/c those rules all completed)
farm:~/hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       calc_K_matrix_zea
        2       fit_lm_elev
        2       simulate_MVN
        7 # RUNNING 9.17.2020 on high2. (small # jobs. but I'll change the script back to med2 for safety)
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                2       fit_lm_elev
                2       simulate_MVN
                5 # FIXED SOME TYPOS. NOW RUNNING. SIMULATION FINISHED BUT STILL GOING ON MODEL FITS
                hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       all
                        2       fit_lm_elev
                        3 # got rid of log file -- messed things up. rerunning now on high2.
# adding Ne=1k as option (note: uncommented several subsnake files before running. builds DAG slowly)
include: "local_ancestry/Snakefile"
include: "ancestry_by_r/Snakefile"
include: "ZAnc/Snakefile"
hilo$ snakemake -n all --quiet --profile slurm

# I want to re-do the MVN simulations with 10^6 instead of 10^5 sims
# so that the ratios for shared outliers are more stable:
hilo$ snakemake -n -f ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/mexicana.MVN.RData ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/maize.MVN.RData --quiet --profile slurm
Job counts:
        count   jobs
        2       simulate_MVN
        2 # RUNNING 12pm 9.29.2020
hilo$ snakemake -n all --quiet --profile slurm # TO RUN
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       fit_lm_elev
        2       fit_zAnc
        2       plot_mean_anc
        2       plot_shared_peaks
        2       plot_slope_elev
        11 # RUNNING 10.12.2020. uses 100k MVN simulations to get FDR thresholds for lm and zAnc outlier methods
# Oops fit_zAnc was too slow (large number of sims). I increased memory and time and made FDR calculations less time burdensome to rerun:
# outlier peaks also ran out of memory, so I increased that too. Rerunning what failed (and extra things that depend on FDR script):
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       fit_lm_elev
        2       fit_zAnc
        2       plot_mean_anc
        2       plot_shared_peaks
        2       plot_slope_elev
        11 # running 10.13.2020. fit_zAnc did not run through. I split into 2 scripts -- one to do fits and one to calc FDR:
# I'm also finding homozygous ancestry tracts in this run & fixed an error in file output names for plot_shared_peaks that should run now:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       fdr_zAnc
        2       fit_zAnc
        56      get_homozygous_ancestry_bams
        56      get_homozygous_ancestry_tracts
        2       plot_shared_peaks
        119 # RUNNING 10.13.2020
# most rules ran through but I still had to fix one type in fdr_zAnc and scaling in plot_shared_peaks. now rerunning:
Job counts:
        count   jobs
        1       all
        2       fdr_zAnc
        2       plot_shared_peaks
        5 # RUNNING 10.13.2020. re-running fdr_zAnc -- just needed fdr functions file as input
        hilo$ snakemake -n all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                2       fdr_zAnc
                3 # RUNNING 10.15.2020
# fixed one more typo on fdr_zAnc. Also TRACTS script did not work: filter bams to reads with high confidence homozygous ancestry (keeps whole genome)
# so I fixed that typo too but have to delete tracts output to rerun that rule.
hilo$ rm -r local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       fdr_zAnc
        56      get_homozygous_ancestry_bams
        56      get_homozygous_ancestry_tracts
        115 # RUNNING 10.15.2020
# attempting to run f4 stats. first testing with rule 'some':
hilo$ snakemake -n "ancestry_by_r/results/f4/pop360.size" --quiet --profile slurm
Job counts:
        count   jobs
        1       input_abba_baba
        1 # RAN 10.22.2020
        hilo$ snakemake -n some --quiet --profile slurm
        Job counts:
                count   jobs
                1       do_abba_baba
                1       some
                2 # fixed some errors on farm, now running this 1 test 10.22.2020 4:50pm
                # oops used -rf when I mean -r and forgot -doAbbaBaba2. trying again:
                # completed.
hilo$ snakemake all --quiet --profile slurm
    Job counts:
      count   jobs
      1       all
      44079   do_abba_baba
      28      input_abba_baba
      2       plot_mean_anc
      2       plot_slope_elev
      44112. Moved to low2 for so many jobs & now running. 10.23.2020
# cancelled because many jobs timing out at 20 min. Upped to 60 min.
# also testing if it's because I'm trying to read the same files at once..
# so I'm testing just 1 file that timed-out last time:
hilo$ snakemake ancestry_by_r/results/f4/pop360/W1330.abbababa2 --quiet --profile slurm
Job counts:
        count   jobs
        1       do_abba_baba
        1 # 10.23.2020 7:30pm. Finished in only 7min.
# now running rule all (takes a long time to make DAG -- giving it 30 min)
8:45pm 10.23.2020. ugh. some kind of python error probably because of output files from half-completed jobs.
# I delete those files from pop360 and it can make a DAG again.
# I add shadow "minimal" so this hopefully doesn't happen again and the directory stays clean
# also made rule 'some' with just the sympatric maize + allopatric_maize group.
# so I can try to just run those over the weekend now instead of all of the jobs. saving symp. mexicana for later.
# testing w/ 1 file:
hilo$ snakemake ancestry_by_r/results/f4/pop360/W1.abbababa2 --quiet --profile slurm
Job counts:
        count   jobs
        1       do_abba_baba
        1 # ran in < 1 min
# now running all maize f4's:
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        22799   do_abba_baba
        13      input_abba_baba
        1       some
        22813 # RUNNING 10.23.2020 10:.
I cancelled part way through afte 3 days of running: allopatric_maize abba baba's failed to run.
because the bams were listing just the simlink in the ALL_byPop/allopatric_maize_bams.list file, e.g. fitlered_bams/merged_bams/MAIZE1.sort.dedup.bam
but input files didn't include symlink, just original bam in filtered_bams/results/MAIZE55/MAIZE1.sort.dedup.bam
# cancelled a few pending/stalled jobs, but otherwise let running ones finish:
hilo$ scancel 26629104 26629107
So I changed how ALL_byPop/allopatric_maize bams were listed. And did 'touch' to not repeat local ancestry results.
# files I updated just using touch b/c of this change:
hilo$ touch local_ancestry/results/alloFreqs/HILO_MAIZE55/allopatric_maize/region_*
hilo$ touch local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.var.sites local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.rdiff local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.rpos local_ancestry/results/thinnedSNPs/HILO_MAIZE55/counts_thinned_aims.txt
hilo$ touch local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.bed
hilo$ touch local_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.var.sites.*
# okay actual 'touch' didn't work on countsACGT, but I'll use snakemake's built in --touch for pipelines:
# first tried on just 1 file & works:
hilo$ snakemake --touch local_ancestry/results/countsACGT/HILO_MAIZE55/MAIZE1.counts.gz --profile slurm
# now running on whole pipeline:
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        84      boot_ancestry_hmm
        2       calc_K_matrix_zea
        6       combine_pop_ancestry_data
        402     count_ACGT
        403     count_maj_min
        2       fdr_zAnc
        2       fit_lm_elev
        2       fit_zAnc
        84      get_admix_times
        56      get_homozygous_ancestry_bams
        56      get_homozygous_ancestry_tracts
        28      get_maximum_a_posterior
        1       make_allo_counts
        28      make_input_hmm
        84      map_local_anc_onto_1cM_windows
        3       plot_admix_times
        1       plot_bootstrap_anc_by_r
        2       plot_mean_anc
        2       plot_shared_peaks
        2       plot_slope_elev
        2       simulate_MVN
        84      summarise_local_anc_by_1cM
        84      summarise_posterior
        1421
      hilo$ snakemake --touch all --quiet --profile slurm # 10.27.2020
      hilo$ snakemake -n all --quiet --profile slurm
      Job counts:
              count   jobs
              1       all
              2       plot_mean_anc
              2       plot_slope_elev
              5 # run just these ones that must be updated not with simply 'touch'
# testing 1 window for allopatric_maize:
hilo$ snakemake ancestry_by_r/results/f4/allopatric_maize/W1.abbababa2 --quiet --profile slurm
Job counts:
        count   jobs
        1       do_abba_baba
        1       input_abba_baba
        2 # see how many minutes it takes.. only 1.5min (not so bad!)
# now rerunning pop361, allopatric_maize, sympatric_maize as a group (and later will run sympatric mexicana)
hilo$ snakemake some --quiet --profile slurm # RUNNING 10.27.2020 5:23pm. oops actually got stuck on DAG b/c of wildcards for sympatric maize
hilo$ snakemake some --quiet --profile slurm # RUNNING 10.27.2020 8:39pm
Job counts:
        count   jobs
        3046    do_abba_baba
        1       input_abba_baba
        1       some
        3048
# MOST JOBS ARE RUNNING OUT OF TIME, BUT HAVE LOW CPU TIME
# AND VERY HIGH WALL TIME. I WANT TO ASK JEFF ABOUT THIS:
# W1:
# hilo$ less snake_logs/do_abba_baba.POP=allopatric_maize,WINDOW=W1.26676305.err
# Number of sites retained after filtering: 383460
# [ALL done] cpu-time used =  88.11 sec
# [ALL done] walltime used =  109.00 sec
# 1 of 1 steps (100%) done
# # ~60k lines in .err file
# slurm .out file:
# Reserved walltime   : 04:00:00
# Used walltime       : 00:01:53
# Used CPU time       : 00:01:29

# W1351:
# hilo$ less snake_logs/do_abba_baba.POP=allopatric_maize,WINDOW=W1351.26677882.err
# Number of sites retained after filtering: 1364706
# [ALL done] cpu-time used =  151.76 sec
# [ALL done] walltime used =  9801.00 sec
# slurmstepd: error: *** JOB 26677882 ON c6-67 CANCELLED AT 2020-10-28T00:32:43 DUE TO TIME LIMIT ***
# ~60k lines in .err file
# slurm .out file:
# Reserved walltime   : 04:00:00
# Used walltime       : 04:00:39
# Used CPU time       : 00:02:32
# note: this job times out nearly an hour after angsd thinks it's completed.

# SOLUTION: split whole genome bams into much smaller bams! (1 bam per 1cM segment). I can also re-header/re-sort and, if I want, cap with BAQ, when I do this.
# 1st check how much space I have in my dir on farm for duplicating bams.

# trying just 1 abbababa (with low r, so large region):
hilo$ snakemake ancestry_by_r/results/f4/allopatric_maize/W1331.abbababa2 --quiet --profile slurm
57399486 - ran out of memory
36294779-82156624

# what is the reading/writing capacity?
# try 5 short jobs at the same time:
hilo$ snakemake ancestry_by_r/results/f4/allopatric_maize/W34.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W1442.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W254.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W253.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W610.abbababa2 --quiet --profile slurm
Job counts:
        count   jobs
        5       do_abba_baba
        5 # RUNNING 10.29.2020. Finished fast in < 5min.
# ok, what about 10 jobs of cM/Mb bin 4? (so slightly bigger?)
hilo$ snakemake ancestry_by_r/results/f4/allopatric_maize/W611.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W174.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W175.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W628.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W629.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W648.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W649.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W680.abbababa2
ancestry_by_r/results/f4/allopatric_maize/W1346.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W1466.abbababa2 --quiet --profile slurm
Job counts:
        count   jobs
        10      do_abba_baba
        10 # RUNNING 10.29.2020. Takes about double walltime (6-7min) as cpu time (~3min).
# now try only 5 jobs at once from the bin 4 cM/Mb:
hilo$ Job counts:
        count   jobs
        5       do_abba_baba
        5 # RUNNING 10.29.2020. Not too much extra walltime -- only 20s per job.
# So ~5 jobs accessing the same files is a computational limit for I/O efficiency.
# Now checking that sympatric_maize runs equally smoothly with similar time/memory constraints:
hilo$ snakemake ancestry_by_r/results/f4/sympatric_maize/W199.abbababa2 ancestry_by_r/results/f4/sympatric_maize/W263.abbababa2 ancestry_by_r/results/f4/sympatric_maize/W270.abbababa2 ancestry_by_r/results/f4/sympatric_maize/W271.abbababa2 ancestry_by_r/results/f4/sympatric_maize/W272.abbababa2 --quiet --profile slurm
Job counts:
        count   jobs
        5       do_abba_baba
        1       input_abba_baba
        6 # RUNNING 10.29.2020
# can I limit the jobs in the snake command?
hilo$ snakemake ancestry_by_r/results/f4/sympatric_mexicana/W199.abbababa2 ancestry_by_r/results/f4/sympatric_mexicana/W263.abbababa2 ancestry_by_r/results/f4/sympatric_mexicana/W270.abbababa2 ancestry_by_r/results/f4/sympatric_mexicana/W271.abbababa2 --quiet --profile slurm --jobs 2
Job counts:
        count   jobs
        4       do_abba_baba
        1       input_abba_baba
        5 # try limiting to 2 running jobs at a time. Works!
        # so I just have to add the --jobs 5 flag when I run snakemake :)
# last test before running all:
hilo$ snakemake ancestry_by_r/results/f4/sympatric_maize/W309.abbababa2 ancestry_by_r/results/f4/allopatric_maize/W310.abbababa2 --quiet --profile slurm --jobs 1 --restart-times 1
Job counts:
        count   jobs
        2       do_abba_baba
        2 # RAN SMOOTHLY. 10.29.2020 5:40pm.
# now run rule 'some' has allopatric_maize, sympatric_maize, sympatric_mexicana. I estimate 24hrs to run per group.
hilo$ snakemake -n some --quiet --profile slurm --jobs 5
# mystery error keepincomplete=keep_incomplete fixed  ...
raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
Fixed by deleting some of the output files that maybe were from cancelled jobs (?)
Now running:
hilo$ snakemake -n some --quiet --profile slurm --jobs 5
Job counts:
        count   jobs
        4528    do_abba_baba
        1       some
        4529 # STARTED 10.29.2020 8:36pm. hmm, I'd prefer that it start with
        # allopatric_maize, then sympatric_maize. Will cancel and restart with just those 2:
# status when I cancelled the snakemake job:
hilo$ squeue -u ecalfee
         JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
      26725636     high2     bash  ecalfee  R       47:30      1 1   2G     c6-92
      26726207      med2 do_abba_  ecalfee  R        6:29      1 2   7G     c6-92
      26726147      med2 do_abba_  ecalfee  R       11:24      1 2   5G     c6-92
      26726035      med2 do_abba_  ecalfee  R       19:01      1 2   6G     c6-92
      26725668      med2 do_abba_  ecalfee  R       35:37      1 2   6G     c6-92
      26725673      med2 do_abba_  ecalfee  R       35:37      1 3   8G     c6-92
      # then cancelled remaining running jobs (1 finished, but maybe created bad files)
      hilo$ scancel 26726207 26726147 26726035 26725673
hilo$ snakemake some --quiet --profile slurm --jobs 5
Job counts:
        count   jobs
        3012    do_abba_baba
        1       some
        3013 # RUNNING 10.29.2020 9:30pm. Order is random .. here it's starting with sympatric_maize regions.
# to find out which file is running, e.g.
hilo$ grep 'wildcards:' -A 1 snake_logs/*abba*26726313.err
# so all but 6 abba baba files ran .. mostly seems like by Saturday morning.
# some were rerun because of memory & I'm checking right now the ones that didn't complete:
hilo$ snakemake -n some --quiet --profile slurm --jobs 5
Job counts:
        count   jobs
        6       do_abba_baba
        1       some
        7
# which files are missing?
hilo$ for i in {1..1520}; do ls ancestry_by_r/results/f4/allopatric_maize/W$i.abbababa2; done | grep 'cannot'
ls: cannot access 'ancestry_by_r/results/f4/allopatric_maize/W113.abbababa2': No such file or directory
ls: cannot access 'ancestry_by_r/results/f4/allopatric_maize/W892.abbababa2': No such file or directory
ls: cannot access 'ancestry_by_r/results/f4/allopatric_maize/W1264.abbababa2': No such file or directory
# same ones from sympatric_maize too
# memory not enough: 12G, 13G, 12G
# I added memory so it starts at 8G + extra , then goes to 16G + extra, then 24G + extra on attempt 3
hilo$ snakemake some --quiet --profile slurm --jobs 5
# running Sunday Nov 1 8:50pm
# W892 still ran out of memory. Time is very dependent on region length, but not memory.
hilo$ snakemake ancestry_by_r/results/f4/allopatric_maize/W892.abbababa2 ancestry_by_r/results/f4/sympatric_maize/W892.abbababa2 --quiet --profile slurm --jobs 5 --restart-times 0 --config mem=32G
Job counts:
        count   jobs
        2       do_abba_baba
        2 # RUNNING 11.1.2020 10:36pm. CANCELLED BECAUSE DIDN'T USE THE 32G of memory.
# I changed to each job having 8, 16, or 32G memory
# added in sympatric_mexicana and pop361 to run abba baba (can do other pops later if needed)

hilo$ snakemake some --quiet --profile slurm --jobs 5
Job counts:
        count   jobs
        1521    do_abba_baba
        1       some
        1522 # RUNNING 11.1.2020 11:30pm. Note: W892 still hasn't completed.
# cancelled with just a few that didn't run/stalled Nov 2 at 11pm. Adding memory up to 40G for 3rd attempt.
# also adding in a population to test that pattern holds also at individual population level.
hilo$ snakemake some --quiet --profile slurm --jobs 5 # RUNNING 11.3.2020 1am
# sympatric_maize and sympatric_mexicana still ran out of memory with 40G for W892, so now rerunning just those
# with 48G, 64G, 80G attempts. Will see how much memory they use.
hilo$ snakemake some --quiet --profile slurm --jobs 5 # RUNNING 11.3.2020 1am
Job counts:
        count   jobs
        2       do_abba_baba
        1       some
        3 # completed quickly with between 40 and 48G memory usage.

# testing ANGSD D-STAT to find out order of abbababa2 output file:
hilo$ Rscript ../software/angsd_0.932/R/estAvgError.R angsdFile="ancestry_by_r/results/f4/sympatric_maize/W1" out="test_Dstat" sizeFile="ancestry_by_r/results/f4/sympatric_maize.size"

# installed R package pracma
hilo$ Rscript ../software/angsd_0.932/R/estAvgError.R angsdFile="ancestry_by_r/results/f4/sympatric_maize/W1" out="test_Dstat" sizeFile="ancestry_by_r/results/f4/sympatric_maize.size" nameFile="ancestry_by_r/results/f4/sympatric_maize.names"
----------
angsdFile:  ancestry_by_r/results/f4/sympatric_maize/W1   errFile:  FALSE  nameFile:  ancestry_by_r/results/f4/sympatric_maize.names  sizeFile:  ancestry_by_r/results/f4/sympatric_maize.size  out:  test_Dstat  addErr:  FALSE  nIter:  100 maxErr:  0.02
-----------
--- Building tree error matrices ---
 --- Time Spent  0  sec
Error in `colnames<-`(`*tmp*`, value = colnames(outDataTotal)) :
  attempt to set 'colnames' on an object with less than two dimensions
Execution halted
# maybe it needs more than 1 region to run calcs:
hilo$ cat ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2 > test_W1-9_sympatric_maize.abbababa2
hilo$ cat ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2 > test_W1-100_sympatric_maize.abbababa2
ecalfee@c6-67:~/hilo$ for i in {2..100}; do tail -n 3 ancestry_by_r/results/f4/sympatric_maize/W$i.abbababa2 >> test_W1-100_sympatric_maize.abbababa2; done
ecalfee@c6-67:~/hilo$ Rscript ../software/angsd_0.932/R/estAvgError.R angsdFile="test_W1-100_sympatric_maize" out="test_Dstat_W1-100_sympatric_maize" sizeFile="ancestry_by_r/results/f4/sympatric_maize.size" nameFile="ancestry_by_r/results/f4/sympatric_maize.names"

hilo/ancestry_by_r/results/f4$ for i in {890..893}; do head -n 2 sympatric_maize/W$i.abbababa2 | tail -n 1 | cut -f1,4-6; done
6	-3.199246	459.730621	57905.000000
6	-14.845173	921.008586	216003.000000
head: cannot open 'sympatric_maize/W892.abbababa2' for reading: No such file or directory
6	-86.311113	1830.236866	371792.000000

# skipping W892 for now. summarising f4s. sympatric_maize.1.f4 is line 1 results, .2.f4 is line 2 results etc.
# only population orders change:
ecalfee@c6-67:~/hilo$ (head -n 1 ancestry_by_r/results/f4/allopatric_maize/W1.abbababa2 | cut -f1-6 | awk '{print "window" "\t" $0 "\t" "pop1" "\t" "pop2" "\t" "pop3" "\t" "pop4"}'; for i in {1..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/allopatric_maize/W$i.abbababa2 | cut -f1-6| awk -v i=$i 'NR%3==1 {print "W"i "\t" $0 "\t" "parviglumis" "\t" "allopatric_mexicana" "\t" "allopatric_maize" "\t" "tripsacum"}';  done) > ancestry_by_r/results/f4/allopatric_maize.1.f4

# all the other files too:
ecalfee@c6-67:~/hilo$ (head -n 1 ancestry_by_r/results/f4/allopatric_maize/W1.abbababa2 | cut -f1-6 | awk '{print "window" "\t" $0 "\t" "pop1" "\t" "pop2" "\t" "pop3" "\t" "pop4"}'; for i in {1..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/allopatric_maize/W$i.abbababa2 | cut -f1-6| awk -v i=$i 'NR%3==2 {print "W"i "\t" $0 "\t" "parviglumis" "\t" "allopatric_mexicana" "\t" "allopatric_maize" "\t" "tripsacum"}';  done) > ancestry_by_r/results/f4/allopatric_maize.2.f4
ecalfee@c6-67:~/hilo$ (head -n 1 ancestry_by_r/results/f4/allopatric_maize/W1.abbababa2 | cut -f1-6 | awk '{print "window" "\t" $0 "\t" "pop1" "\t" "pop2" "\t" "pop3" "\t" "pop4"}'; for i in {1..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/allopatric_maize/W$i.abbababa2 | cut -f1-6| awk -v i=$i 'NR%3==0 {print "W"i "\t" $0 "\t" "parviglumis" "\t" "allopatric_mexicana" "\t" "allopatric_maize" "\t" "tripsacum"}';  done) > ancestry_by_r/results/f4/allopatric_maize.3.f4

ecalfee@c6-67:~/hilo$ (head -n 1 ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2 | cut -f1-6 | awk '{print "window" "\t" $0 "\t" "pop1" "\t" "pop2" "\t" "pop3" "\t" "pop4"}'; for i in {1..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/sympatric_maize/W$i.abbababa2 | cut -f1-6| awk -v i=$i 'NR%3==1 {print "W"i "\t" $0 "\t" "parviglumis" "\t" "allopatric_mexicana" "\t" "sympatric_maize" "\t" "tripsacum"}';  done) > ancestry_by_r/results/f4/sympatric_maize.1.f4
ecalfee@c6-67:~/hilo$ (head -n 1 ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2 | cut -f1-6 | awk '{print "window" "\t" $0 "\t" "pop1" "\t" "pop2" "\t" "pop3" "\t" "pop4"}'; for i in {1..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/sympatric_maize/W$i.abbababa2 | cut -f1-6| awk -v i=$i 'NR%3==2 {print "W"i "\t" $0 "\t" "parviglumis" "\t" "allopatric_mexicana" "\t" "sympatric_maize" "\t" "tripsacum"}';  done) > ancestry_by_r/results/f4/sympatric_maize.2.f4
ecalfee@c6-67:~/hilo$ (head -n 1 ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2 | cut -f1-6 | awk '{print "window" "\t" $0 "\t" "pop1" "\t" "pop2" "\t" "pop3" "\t" "pop4"}'; for i in {1..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/sympatric_maize/W$i.abbababa2 | cut -f1-6| awk -v i=$i 'NR%3==0 {print "W"i "\t" $0 "\t" "parviglumis" "\t" "allopatric_mexicana" "\t" "sympatric_maize" "\t" "tripsacum"}';  done) > ancestry_by_r/results/f4/sympatric_maize.3.f4


# D - stats all windows but 892:
hilo$ (cat ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2; for i in {2..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/sympatric_maize/W$i.abbababa2; done) > ancestry_by_r/results/f4/sympatric_maize.all.abbababa2
hilo$ Rscript ../software/angsd_0.932/R/estAvgError.R angsdFile="ancestry_by_r/results/f4/sympatric_maize.all" out="ancestry_by_r/results/f4/sympatric_maize.Dstats" sizeFile="ancestry_by_r/results/f4/sympatric_maize.size" nameFile="ancestry_by_r/results/f4/sympatric_maize.names"
hilo$ (cat ancestry_by_r/results/f4/allopatric_maize/W1.abbababa2; for i in {2..891} {893..1520}; do tail -n +2 ancestry_by_r/results/f4/allopatric_maize/W$i.abbababa2; done) > ancestry_by_r/results/f4/allopatric_maize.all.abbababa2
hilo$ Rscript ../software/angsd_0.932/R/estAvgError.R angsdFile="ancestry_by_r/results/f4/allopatric_maize.all" out="ancestry_by_r/results/f4/allopatric_maize.Dstats" sizeFile="ancestry_by_r/results/f4/allopatric_maize.size" nameFile="ancestry_by_r/results/f4/allopatric_maize.names"
# results have format: sympatric_maize.Dstats.Observed.txt

# hmm.. seems like the blocks didn't work quite how I expected
# some files have 7 lines instead of 4 because windows span multiple blocks:
hilo$ nano ancestry_by_r/results/f4/sympatric_maize/W95.abbababa2
W95: 1       119583369       154171190

# making the code above into snakemake rules and rerunning:
hilo$ snakemake some --quiet --profile slurm
# can't set attribute error line 417 (run D-stat rule). Trying to fix syntax of shell command.
# I think it was because I can't set 'size' or possible 'names' as a named input file. Fixed!
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        5       calc_Dstat
        5       combine_abba_baba_for_Dstat
        5       f4_from_abba_baba
        1       some
        16 # RUNNING 11.3.2020 11:37am. Hm says some issue with POP wildcard? fixing by adding to params.
# adding 2 more individual maize pops from across the range for f4 stats: pop362 & 366 (Santa Clara & Jicaltepec)
hilo$ snakemake some --quiet --profile slurm --jobs 5
# RUNNING 11.3.2020

# had typo in rule (fixed) but to run quickly I just did in command line:
hilo$ (head -n 1 ancestry_by_r/results/f4/sympatric_maize/W1.abbababa2 | cut -f1-6 | awk -v OFS="\t" '{print "window",$0,"pop1","pop2","pop3","pop4"}'; for i in {1..1520}; do tail -n +2 ancestry_by_r/results/f4/sympatric_maize/W$i.abbababa2 | cut -f1-6 | awk -v i=$i -v pop=sympatric_maize -v OFS="\t" 'NR%3==1 {print "W"i,$0,"parviglumis","allopatric_mexicana",pop,"tripsacum"}'; done) > ancestry_by_r/results/f4/sympatric_maize.f4

# trying out making fasta file for tripsacum (test on small region):
hilo$ snakemake filtered_bams/results/SRR7758238/TRIP.sort.dedup.baq.fasta --quiet --profile slurm
Job counts:
        count   jobs
        1       make_fasta
        1 # didn't work. (different output format). Trying again on small region:
        hilo$ snakemake filtered_bams/results/SRR7758238/TRIP.fa.gz --quiet --profile slurm
        Job counts:
                count   jobs
                1       make_fasta
                1. COMPLETED. Now removing output file and fixing rule to run on whole genome.

# decided to make f4's with Amecameca (pop22) instead of using all allopatric mexicana
# because this population has no evidence of admixture.
hilo$ snakemake -n all --quiet --profile slurm --jobs 6
# missing samples/ALL_byPop/pop22_bams.list b/c apparently only copied over sympatric pop lists.
# copied over pop20, pop22 and pop33 to ALL_byPop. Unfortunately tried to use a for loop for samples/Over0.5x/byPop and it copied ALL pop files, including sympatric. oops!
# this will trigger a big re-run :(
for i in 20 22 33; do scp -P 2022 samples/Over0.5x_byPop/pop$i_*.list ecalfee@farm.cse.ucdavis.edu:~/hilo/samples/Over0.5x_byPop/.; done
# so I will try to 'touch' on snakemake to not rerun all the local ancestry rules etc.
# should take a min to ID what files need to be 'touched' to fix this .. :/
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        84      boot_ancestry_hmm
        2       calc_K_matrix_zea
        6       combine_pop_ancestry_data
        2       fdr_zAnc
        2       fit_lm_elev
        2       fit_zAnc
        84      get_admix_times
        56      get_homozygous_ancestry_bams
        56      get_homozygous_ancestry_tracts
        28      get_maximum_a_posterior
        28      make_input_hmm
        84      map_local_anc_onto_1cM_windows
        3       plot_admix_times
        1       plot_bootstrap_anc_by_r
        2       plot_mean_anc
        2       plot_shared_peaks
        2       plot_slope_elev
        2       simulate_MVN
        84      summarise_local_anc_by_1cM
        84      summarise_posterior
        615
# run 'touch' to update timestamps for all outputs:
hilo$ snakemake --touch all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        84      boot_ancestry_hmm
        2       calc_K_matrix_zea
        6       combine_pop_ancestry_data
        2       fdr_zAnc
        2       fit_lm_elev
        2       fit_zAnc
        84      get_admix_times
        56      get_homozygous_ancestry_bams
        56      get_homozygous_ancestry_tracts
        28      get_maximum_a_posterior
        28      make_input_hmm
        84      map_local_anc_onto_1cM_windows
        3       plot_admix_times
        1       plot_bootstrap_anc_by_r
        2       plot_mean_anc
        2       plot_shared_peaks
        2       plot_slope_elev
        2       simulate_MVN
        84      summarise_local_anc_by_1cM
        84      summarise_posterior
        615
# check nothing else needs to be re-run:
hilo$ snakemake -n all --quiet --profile slurm
(all clear)
# then try again to redo f4s and make tripsacum fasta:
hilo$ snakemake -n all --quiet --profile slurm --jobs 6
Job counts:
        count   jobs
        1       all
        3       calc_Dstat
        3       combine_abba_baba_for_Dstat
        4560    do_abba_baba
        3       f4_from_abba_baba
        3       input_abba_baba
        1       make_fasta
        4574 # RUNNING 11.13.2020 4:19pm
        hilo$ squeue -u ecalfee
                 JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
              26997260     high2     bash  ecalfee  R       17:54      1 1   2G     c6-95
              26997750      med2 do_abba_  ecalfee  R        0:21      1 3   8G     c6-96
              26997744      med2 do_abba_  ecalfee  R        0:43      1 3   8G     c6-94
              26997745      med2 do_abba_  ecalfee  R        0:43      1 3   8G     c6-94
              26997743      med2 do_abba_  ecalfee  R        0:46      1 3   8G     c6-95
              26997712      med2 make_fas  ecalfee  R        2:05      1 13  48G    c6-94
# everything but summary files and Dstats ran. Couldn't find snake logs, so just trying these files again:
hilo$ snakemake all --quiet --profile slurm --jobs 6
Job counts:
    count   jobs
    1       all
    3       calc_Dstat
    3       combine_abba_baba_for_Dstat
    3       f4_from_abba_baba
    10 # RUNNING 11.16.2020 12pm. hmm, getting weird error:
    RuleException in line 353 of /home/ecalfee/hilo/ancestry_by_r/Snakefile:
    IndexError: list index out of range # fixed problem -- needed double brackets in for i in {{1..1250}}
hilo$ snakemake all --quiet --profile slurm
# RUNNING 11.16.2020 9:12pm. oops fixed 2nd for loop {{}} too. Fixed {POP} to {params.POP} in next rule.
# added a bunch of rules to the pipeline to look at flowering time genes.
# flowering time genes
# downloaded Data S7 candidate genes compiled by Li et al 2016
data/flowering_time/candidate_flowering_time_genes_Li_et_al_2016_data_S7.csv
# copy-pasted first column (gene name) into new file
data/flowering_time/v2_gene_names_Li_et_al_2016.csv
# found unique gene names
Erin$ cat data/flowering_time/v2_gene_names_Li_et_al_2016.csv | sort | uniq  > data/flowering_time/v2_unique_gene_names_Li_et_al_2016.csv
# download gene model cross reference from GRAMENE:
https://www.maizegdb.org/gene_center/gene#ncbi
(Complete B73 v4 (Zm00001d.2) gene model cross reference)
data/refMaize/geneAnnotations/gene_model_xref_v4_from_gramene.txt

hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       add_10kb_flowering_time_genes
        1       all
        3       calc_Dstat
        3       calc_and_plot_f4
        24      calc_overlap
        3       combine_abba_baba_for_Dstat
        2       f4_from_abba_baba
        1       find_flowering_time_genes_v4
        2       fit_lm_elev
        2       make_bed_anc_outliers
        2       make_bed_slope_elev
        24      merge_bed_outliers
        2       plot_slope_elev
        70 # 11.20.2020: RUNNING 11:23pm. Many jobs didn't run:
        hilo$ snakemake -n all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                3       calc_Dstat # error is I must have accidentally written over angsd DStat script with a blank script. will download new one. (ls -lh says: -rw-r--r-- 1 ecalfee ecalfee    0 Nov 12 14:58 ../software/angsd_0.932/R/estAvgError.R)
                3       calc_and_plot_f4 # error: inv_file must be character string or connection -fixed.
                24      calc_overlap
                2       make_bed_anc_outliers # error: fdr_pos file not found (to write to)
                2       make_bed_slope_elev # fixed output file name errors in R script.
                24      merge_bed_outliers
                59 # trying again. RUNNING 10:26am 11.23.2020. Oops still some errors. fixed make_bed_slope_outliers but still troubleshooting calc_and_plot_f4
                /hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       all
                        2       calc_and_plot_f4
                        12      calc_overlap
                        2       make_bed_slope_elev
                        12      merge_bed_outliers
                        29 # RUNNING 11.24.2020 11:55am
# problem with errors in calc_and_plot_f4:
`summarise()` regrouping output by 'bin_cd5' (override with `.groups` argument)
Error: Aesthetics must be either length 1 or the same as the data (5000): colour
Backtrace: ggplot2. # I fixed a problem with 'zea' variable being NA. rerunning:
# COMPLETED.
Oops I switched around my f4 populations for the f4 ratio. I'm rerunning these analyses f4(trip, parv; X, allo_maize)
where X is a sympatric population (numerator) or allopatric mexicana (pop 22 = Amecameca, denominator)
# new files will be in output directory f4/ . I have moved all previous f4's in this folder to a new folder f4_allopatric_mexicana/
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        3       calc_Dstat
        2       calc_and_plot_f4
        3       combine_abba_baba_for_Dstat
        4560    do_abba_baba
        3       f4_from_abba_baba
        3       input_abba_baba
        4575 # RUNNING 11.25.2020 2:15pm. CANCELLED because forgot to limit --jobs to just 6. rerunning:
b/c I cancelled jobs I had to 'unlock' the directory to rerun snakemake:
Error: Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory:
/home/ecalfee/hilo
If you are sure that no other instances of snakemake are running on this directory, the remaining lock was likely caused by a kill signal or a power loss. It can be removed with the --unlock argument.
(hilo-env) ecalfee@farm:~/hilo$ snakemake all --quiet --profile slurm --jobs 5 --unlock
# Note: should just do snakemake --unlock I think next time!! Doesn't run when you use --unlock
ecalfee@farm:~/hilo$ snakemake all --quiet --profile slurm --jobs 5
# still running, but moving on to sympatric_maize from mexicana. will check on one missing files:
hilo$ for i in {1..1520}; do ls ancestry_by_r/results/f4/sympatric_mexicana/*W$i.abbababa2; done
ls: cannot access 'ancestry_by_r/results/f4/sympatric_mexicana/*W635.abbababa2': No such file or directory
OOPS ran out of time on this one. Will need to rerun after with extra time:
ecalfee@c6-89:~/hilo$ tail snake_logs/do_abba_baba.POP=sympatric_mexicana,WINDOW=W635.27964807.out
End                 : 2020-11-30T04:29:32
Reserved walltime   : 02:38:00
Used walltime       : 02:38:10
# FARM had some kind of 16hr downtime (?). My screen and jobs all stopped... just logged back in Dec 2, 1pm
ecalfee@farm:~/hilo$ snakemake all --quiet --profile slurm --jobs 6
Job counts:
        count   jobs
        1       all
        3       calc_Dstat
        2       calc_and_plot_f4
        3       combine_abba_baba_for_Dstat
        2208    do_abba_baba
        3       f4_from_abba_baba
        2220 # RUNNING 12/2/2020 1:20pm. Weird it only ran like 100 jobs then quit.
        # like the snakemake run just stopped. So I'm starting snakemake again:
        hilo$ snakemake all --quiet --profile slurm --jobs 5
        Job counts:
                count   jobs
                1       all
                3       calc_Dstat
                2       calc_and_plot_f4
                3       combine_abba_baba_for_Dstat
                2100    do_abba_baba
                3       f4_from_abba_baba
                2112 12/2/2020 10:45pm
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       calc_Dstat
        2       calc_and_plot_f4
        2       combine_abba_baba_for_Dstat
        2       do_abba_baba
        2       f4_from_abba_baba
        11 # running 12.9.2020 . Checked in on Dec 11 and weirdly looks like it got frozen and didn't run through. so stopped & re-started same command:
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                1       calc_Dstat
                1       calc_and_plot_f4
                1       combine_abba_baba_for_Dstat
                1       do_abba_baba
                1       f4_from_abba_baba
                6 # running 12.11.2020. Abba-baba is job 28550210 for W635 sympatric_mexicana

# making input bams lists:
# test one:
hilo$ snakemake local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/bams/pop363_bams.list --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       index_and_list_homozygous_ancestry_bams
        1 # grr there is something wrong with the input bams! they are empty :(
        # I fixed file naming error in get_homozygous_ancestry_bams.sh script.
        # also deleting 'completed' files for bams so they are re-run.
        hilo$ rm local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/*/bams/*.completed
# re-run them all to make bams and then also create input bams lists:
Job counts:
        count   jobs
        1       all
        56      get_homozygous_ancestry_bams
        56      list_homozygous_ancestry_bams
        113 # RUNNING 12.29.2020 1:26pm

# some missing files: pop18, pop23, pop28, pop29, pop34 for maize ancestry bams list.
# weird that it's a timeout error for such a small task (echo)
# I'll just rerun without edits .. maybe some issue with the computer nodes
hilo$ snakemake all --quiet --profile slurm                                      Job counts:
        count   jobs
        1       all
        14      list_homozygous_ancestry_bams
        15 # RUNNING 12.31.2020 5pm


## Get pi estimates within homozygous ancestry across the whole genome.
# - test pi script with small genome segment. then decide memory for running a whole pop.
# trying out pipeline to get pi estimates across the genome for each population within homozygous ancestry segments
# test pipeline using a small 500 kb test region on chr1
hilo$ snakemake diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.thetas.gz --quiet --profile slurm
hilo$ snakemake diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.thetas.gz --quiet --profile slurm
Job counts:
        count   jobs
        1       calc_thetas
        1       estimate_saf_pop
        1       estimate_sfs
        3 # RUNNING 1.1.2021 12:30pm. Fixed the double .. in file extension output and re-trying:

hilo$ snakemake diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.thetas.gz --quiet --profile slurm --restart-times 0
# estimate_saf_pop took ~15sec and esitmate_sfs took ~10sec computing time for .5Mb ..
# so scaling that to whole genome is about 18 hrs and 12 hrs based on scaling up to 2.2Tb
# calc_thetas failed:
Problem opening file: 'diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.gz'
# hilo$ realSFS saf2theta diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.idx -sfs diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.sfs -outname diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small
	-> Version of fname:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.idx is:2
	-> Assuming .saf.gz file: diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.gz
	-> Assuming .saf.pos.gz: diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.pos.gz
	-> args: tole:0.000001 nthreads:4 maxiter:100 nsites:0 start:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.sfs chr:(null) start:-1 stop:-1 fstout:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small oldout:0 seed:1609607311 bootstrap:0 resample_chr:0 whichFst:0 fold:0 ref:(null) anc:(null)
	-> Will read chunks of size: 4096
	-> Reading: diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.sfs assuming counts (will normalize to probs internally)
File:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.sfs looks empty
# the sfs file is empty!! But that rule runs fine manually when I create sfs in command line..
Deleted intermediate files and running test again:
hilo$ snakemake diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.thetas.gz --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       calc_thetas
        1       estimate_saf_pop
        1       estimate_sfs
        3 # 1.2.2020. calc_thetas rule didn't run!
        # errors:
        -> Version of fname:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.gz is:2
        /bin/bash: line 1: 41159 Segmentation fault      realSFS saf2theta diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.gz -sfs diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.sfs -outna$
# ok so I fixed .saf.gz -> .saf.idx typo. Now I need to figure out what's wrong with .sfs file:
File:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.sfs looks empty

# hmm... in rule estimate_sfs logs I have this error:
Problem opening file: 'diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.gz
# and it keeps giving me a warning about version fname is 2:
Version of fname:diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.saf.idx is:2
# got rid of 'shadow' for these rules, deleted all intermediate output files, and restarting snakemake:
hilo$ rm diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.*
hilo$ snakemake diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize/pop362.small.thetas.gz --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       calc_thetas
        1       estimate_saf_pop
        1       estimate_sfs
        3 # ran through!! Now figure out best resources/timing for whole genome. removed all 'small' and genome segment/regions portions of code to run on full genome.         #"-r 1:1000000-1500000 "
        (hilo-env) ecalfee@farm:~/hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                56      calc_thetas
                56      estimate_saf_pop
                56      estimate_sfs
                56      get_homozygous_ancestry_bams
                56      get_homozygous_ancestry_tracts
                56      list_homozygous_ancestry_bams
                337 # RUNNING 1.4.2020. some ran out of memory for estimate_saf_pop. so I'm increasing and rerunning:
                hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       all
                        35      calc_thetas
                        35      estimate_saf_pop
                        35      estimate_sfs
                        4       list_homozygous_ancestry_bams
                        110 # 1.6.2020 9pm. some error could not assign memory. not sure issue but lessened total possible memory for 1 rule nad trying again.
                        hilo$ snakemake all --quiet --profile slurm
                        Job counts:
                                count   jobs
                                1       all
                                35      calc_thetas
                                35      estimate_saf_pop
                                35      estimate_sfs
                                4       list_homozygous_ancestry_bams
                                110
                        Traceback (most recent call last):
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/__init__.py", line 665, in snakemake
                            keepincomplete=keep_incomplete,
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/workflow.py", line 908, in execute
                            success = scheduler.schedule()
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/scheduler.py", line 378, in schedule
                            self.run(job)
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/scheduler.py", line 397, in run
                            error_callback=self._error,
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/executors.py", line 915, in run
                            shell=True,
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 356, in check_output
                            **kwargs).stdout
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 423, in run
                            with Popen(*popenargs, **kwargs) as process:
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 729, in __init__
                            restore_signals, start_new_session)
                          File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 1295, in _execute_child
                            restore_signals, start_new_session, preexec_fn)
                        OSError: [Errno 12] Cannot allocate memory
                        # HMM.. problem with rule estimate_saf_pop on cluser, e.g. job 28893369
# pausing on this for a min to re-do some results for bootstrapping ancestry ~ r:
# first remove results
hilo$ rm ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/*.Rdata
# then remake them:
hilo$ snakemake ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/cd5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/r5_K2.Rdata --quiet --profile slurm
Job counts:
        count   jobs
        2       summarise_bootstrap_anc_by_r
        2 # RUNNING 1.9.2020 12:30pm. needed to add na.rm to quantiles after correlation
        # because one bootstrap sample for cd's within allopatric maize has no variance (all ancestry = 100% maize)
        # rerunning:
        snakemake ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/cd5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/r5_K2.Rdata --quiet --profile slurm
        Job counts:
        count   jobs
        2       summarise_bootstrap_anc_by_r
        2 # completed
# Checking status of pi estimate scripts
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       calc_and_plot_f4
        22      calc_thetas
        22      estimate_saf_pop
        22      estimate_sfs
        1       plot_bootstrap_NGSadmix_by_r
        1       plot_bootstrap_local_ancestry_by_r
        71 # errors with calc_and_plot_f4 as well as estimates_saf_pop:
        Error in rule estimate_saf_pop -- attempt 1 ran out of memory (at 40G)
        angsd -out diversity/results/pi/HILO_MAIZE55/Ne100000_yesBoot/HOMOZYG/maize/pop19
        cluster_jobid: 29386913 # will wait to see if it's a larger problem across populations/attempts.
        Job counts:
                count   jobs
                1       all
                2       calc_and_plot_f4
                21      cqalc_thetas
                21      estimate_saf_pop
                21      estimate_sfs
                1       plot_bootstrap_NGSadmix_by_r
                1       plot_bootstrap_local_ancestry_by_r
                68 # trying again with more memory for estimate_saf_pop and an updated environment for R scripts. 1.19.2021 3pm
# all sfs and saf estimates ran! yay more memory!
# error in calc_and_plot_f4: wrong filename from Snakefile (fixed)
hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       calc_and_plot_f4
        12      calc_overlap
        1       combine_genome_scan_plots
        2       make_bed_anc_outliers
        12      merge_bed_outliers
        1       plot_NGSAdmix
        1       plot_bootstrap_NGSadmix_by_r
        1       plot_bootstrap_local_ancestry_by_r
        2       plot_mean_anc
        1       plot_shared_peaks
        2       plot_slope_elev
        38 # RUNNING 1.20.2021
        # what's wrong with plot_mean_anc? job id 29409323 -- object 'rds' not found
        # and plot_bootstrap_NGSadmix_by_r? job id 29409322 -- Error in read.table(inv_file, stringsAsFactors = F, header = F) :
  'file' must be a character string or connection
        # also these 2 and plot_shared_peaks are being held up by priority on med2 (I could run on bigmemm or on high2)
        # resolved these variable naming issues.
        snakemake -n all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                12      calc_overlap
                1       combine_genome_scan_plots
                2       make_bed_anc_outliers
                12      merge_bed_outliers
                1       plot_bootstrap_NGSadmix_by_r
                1       plot_bootstrap_local_ancestry_by_r
                2       plot_mean_anc
                1       plot_shared_peaks
                33 # RUNNING 1.21.2021 11a:15m. changed on farm temporarily some rules to bigmemm node because wasn't getting priority to run on med2
# fixed variable naming in plot_bootstrap_NGSadmix_by_r R script
# error in plot_shared_peaks: Error in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  :
  invalid font type.... so I changed the default font in theme_graph to a common font:
  theme_graph(base_family = 'Helvetica'). Alt option would be to import Arial Narrow with extrafonts package.
  hilo$ snakemake all --quiet --profile slurm
  Job counts:
          count   jobs
          1       all
          1       plot_bootstrap_NGSadmix_by_r
          1       plot_shared_peaks # object 'places' not found
          3 # RUNNING 2.21.2021 1:45pm
          # Neither worked: 29499646 29535376
          hilo$ snakemake all --quiet --profile slurm
          Job counts:
                  count   jobs
                  1       all
                  1       plot_bootstrap_NGSadmix_by_r 29578835
                  1       plot_shared_peaks 29578687 - 4G is too little memory.
                  3 # RUNNING 1.21.2021 5pm. Not getting priority on queue. So I upped to high2 and bigmemh for just 2 jobs here. Cancelled and will rerun:
          hilo$ snakemake all --quiet --profile slurm --restart-times 0

# oops I accidentally ran all my pi/diversity stats wiht Ne = 100k instead of 10k. Will need to rerun.
# but first to test 1 pop on new rules with what I have (so I can move forward with how to summarise/plot results):
# getting pi in 5kb non-overlapping windows.
hilo$ snakemake diversity/results/pi/HILO_MAIZE55/Ne100000_yesBoot/HOMOZYG/mexicana/pop366.pi.allChr.pestPG diversity/results/pi/HILO_MAIZE55/Ne100000_yesBoot/HOMOZYG/mexicana/pop366.pi.windows.5000.5000.pestPG --quiet --profile slurm
# I decided to just run all of them overnight instead. so I added these 2 files to rule 'some'
hilo$ snakemake all some --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        57      calc_pi_all_chr
        57      calc_pi_by_windows
        56      calc_thetas
        56      estimate_saf_pop
        56      estimate_sfs
        1       some
        284. Mostly ran except calc_pi rules didn't get priority. I reduced time and memory and am trying on low2
# cancelling pending jobs:
hilo$ squeue -u ecalfee | grep 'calc_pi' | awk '{print $1}' | xargs -n 1 scancel
# restarting snakemake:
# also made one test file interactively (took < 5 min and 2G):
cp pop366.thetas.idx TEST_pop366.thetas.idx
cp pop366.thetas.gz TEST_pop366.thetas.gz
thetaStat do_stat TEST_pop366.thetas.idx -win 5000 -step 5000 -outnames TEST_pop366.pi.windows.5000.5000.pestPG
thetaStat do_stat TEST_pop366.thetas.idx -outnames TEST_pop366.pi.allChr.pestPG
copied these to laptop into test directory: hilo/test/.
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        51      calc_pi_all_chr
        52      calc_pi_by_windows
        104 # TRYING TO RUN on low2 1.23.2021 4pm. COMPLETED.
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                56      calc_mean_pi_in_outliers_vs_not
                57 # summarising pi by outliers vs not # RUNNING 1.24.2021 10am. COMPLETED (very fast). But rerunning b/c missing column in output ('windows')
                hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       all
                        56      calc_mean_pi_in_outliers_vs_not
                        57 # COMPLETED
# try doing fst between sympatric maize and mexicana within mexicana ancestry for 1 location: Jicaltepec
hilo$ snakemake all "diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop366.pop24.fst.allChr.txt" --quiet --profile slurm
Job counts:
        count   jobs
        1       calc_fst_all_chr
        1       calc_fst_idx
        1       estimate_2D_sfs
        3 # RUNNING 1.26.2021 2:30pm (I tested this fst pipeline with pop362.small paired with itself for fst, see eg test/pop362.pop362.fst.allChr.txt)
# ADDED A BUNCH OF MEMORY before running all jobs because don't want it to restart each one:
# estimate_2D_sfs now starts w/ 64G of memory (not 48) and adds 8G from there.
# hmmm. 'zip' isn't working how I expected:
hilo$ snakemake -n  all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       calc_fst_all_chr
        2       calc_fst_by_windows
        2       calc_fst_idx
        2       estimate_2D_sfs
        9 # SHOULD be 14 * 2 jobs per rule because there's 14 symp_maize_pops and 14 symp_mexicana_pops. trying again only putting in symp_maize_pops and symp_mexicana_pops as parameters to zip:
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                27      calc_fst_all_chr
                28      calc_fst_by_windows
                27      calc_fst_idx
                27      estimate_2D_sfs
                110 # RUNNING 1.27.2021
# test making bam file lists for all other sympatric individuals maize or mexicana except focal population:
# first cancel pending jobs on 'low2' priority (calc_fst_all_chr and calc_fst_by_windows)
hilo$ squeue -u ecalfee | grep 'calc_fst' | awk '{print $1}' | xargs -n 1 scancel # 2.28.2021
farm:~/hilo$ snakemake "local_ancestry/results/ancestry_hmm/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/bams/not360_bams.list" --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       list_not_pops
        1 # COMPLETED 1.28.2021 (should have 111 # individuals in file output. CHECK!)
# now added to list all. will check on SAF calculations later if memory is failing because of large # of bams..
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        55      calc_fst_all_chr
        56      calc_fst_by_windows
        28      calc_fst_idx
        28      estimate_2D_sfs
        28      estimate_saf_pop
        27      list_not_pops
        223 # RUNNING 1.28.2021. Not  enough memory, needs > 131Gb (!) for estimate_2D_sfs.
        # also many estimate_saf_pop's failed too!
      # trying with just 1 thread (not 6) and one job how much memory is needed:
      hilo$ snakemake diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop363.not363.fst.allChr.txt --quiet --profile slurm --restart-times 0
      Job counts:
              count   jobs
              1       calc_fst_all_chr
              1       calc_fst_idx
              1       estimate_2D_sfs
              3 # attempting with 2D_sfs getting 140Gb of memory and 1 attempt.
              # note: for 1D SFS for not pops (all but 1 population) I either need to increase memory or set -nSites. And increase time to 48 hrs (1 day is not enough)
              JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
           30172287      med2 estimate  ecalfee  R        3:32      1 36  140G   c6-88
# taking a break from fst calculations (commented those out from rule 'all') and remaking some plots (deleting old plots first).
hilo$ snakemake all --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       all
        2       calc_and_plot_f4
        1       combine_genome_scan_plots
        1       plotQQ
        1       plot_NGSAdmix
        1       plot_bootstrap_NGSadmix_by_r
        1       plot_bootstrap_local_ancestry_by_r
        2       plot_mean_anc
        1       plot_shared_peaks
        2       plot_slope_elev
        13 # RUNNING 2.4.2020 11:15pm. 2 failed rules: plot_shared_peaks (ggupset) adn plot_bootstrap_local_ancestry_by_r (too many digits columns)
        hilo$ snakemake all --quiet --profile slurm --restart-times 0
        Job counts:
                count   jobs
                1       all
                1       plot_bootstrap_local_ancestry_by_r
                1       plot_shared_peaks
                3 # still error in plot_bootstrap_local_ancestry_by_r -- renamed file in spearmans:
        hilo$ snakemake ancestry_by_r/tables/spearmans_rho_local_ancestry.tex --quiet --profile slurm --restart-times 0
        Job counts:
        count   jobs
        1       plot_bootstrap_local_ancestry_by_r
        1 # DONE
        # still need maps:
        hilo$ snakemake map/plots/mexico_lines_elev_teo_color.png --quiet --profile slurm --restart-times 0
        Job counts:
        count   jobs
        1       plot_map
        1 # COMPLETED 2.5.2020 1am
# try getting fst (just for maize)
ecalfee@farm:~/hilo$ snakemake fst --quiet --profile slurm
Job counts:
        count   jobs
        13      calc_fst_all_chr
        14      calc_fst_by_windows
        13      calc_fst_idx
        13      estimate_2D_sfs
        12      estimate_saf_pop
        1       fst
        66 # RUNNING 2.5.2020 1:23am. OUT-OF-MEMORY in SAF stage.
        hilo$ snakemake -n fst --quiet --profile slurm
Job counts:
        count   jobs
        12      calc_fst_all_chr
        12      calc_fst_by_windows
        12      calc_fst_idx
        12      estimate_2D_sfs
        12      estimate_saf_pop
        1       fst
        61 # ok. takign break from this task. Just trying to get genomewide fst for mexicana ancestry between
        # all pairs of sympatric maize and sympatric mexicana:
        hilo$ snakemake -n fst --quiet --profile slurm
        /hilo$ snakemake fst --quiet --profile slurm
        Job counts:
          count   jobs
          182     calc_fst_all_chr
          182     calc_fst_idx
          182     estimate_2D_sfs
          1       fst
          547 # RUNNING 2.5.2020 6:13pm. COMPLETED. BUT found fst is being calculated
          # with old Weir and Cockerham method, when I want Bhatia 2013 method
          # so I'm adding -whichFst 1 based on https://github.com/ANGSD/angsd/issues/239
          # put old fst.gz, fst.idx, fst.allChr.txt and fst.windows.*.txt files into new subdirectory
          #hilo/diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/maize$ mv *pop*fst* old_Weir_Cockerham_fst/.
          # and now re-running. Including this time not only all maize-mexicana pairings but also within maize and within mexicana pairings
          # note: 574 is the correct number of jobs: 14*14*2+(13+12+11+10+9+8+7+6+5+4+3+2+1)*2.
          # and using snakemake -n fst --profile slurm| less without --quiet shows reasonable jobs ready to run.
          hilo$ snakemake fst --quiet --profile slurm
          Job counts:
                  count   jobs
                  574     calc_fst_all_chr
                  574     calc_fst_idx
                  364     estimate_2D_sfs
                  1       fst
                  1513 # RUNNING 2.7.2021 12am. COMPLETED. Add comparisons within native ancestry between sympatric pops of the same subspecies.
# weird b/c it seems like it's locked for some jobs run above (?) but I can't unlock it.
# the only thing unusual was having to do ctrl+c for the snake run (even though squeue indicated all jobs had run to completion and no errors were showing in screen)
# so.. error only applies to 1 line in the fst rule (which I am commenting out):
'input' line in fst:
expand("diversity/results/fst/" + prefix_all + "/Ne10000_yesBoot/HOMOZYG/mexicana/{POP1}.{POP2}.fst.allChr.txt", POP1 = symp_maize_pops, POP2 = symp_mexicana_pops),
more specifically error only occurs wiht one output file:
expand("diversity/results/fst/" + prefix_all + "/Ne10000_yesBoot/HOMOZYG/mexicana/{POP1}.{POP2}.fst.allChr.txt", POP1 = symp_maize_pops[3], POP2 = symp_mexicana_pops[5]),
smallest command to reproduce error:
snakemake -n diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop21.fst.allChr.txt --quiet --profile slurm
error:
Traceback (most recent call last):
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/__init__.py", line 665, in snakemake
    keepincomplete=keep_incomplete,
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/workflow.py", line 619, in execute
    dag.check_incomplete()
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/dag.py", line 286, in check_incomplete
    incomplete = self.incomplete_files
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/dag.py", line 410, in incomplete_files
    filterfalse(self.needrun, self.jobs),
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/dag.py", line 407, in <genexpr>
    job.output
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 205, in incomplete
    return any(map(lambda f: f.exists and marked_incomplete(f), job.output))
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 205, in <lambda>
    return any(map(lambda f: f.exists and marked_incomplete(f), job.output))
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 203, in marked_incomplete
    return self._read_record(self._metadata_path, f).get("incomplete", False)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 322, in _read_record_cached
    return self._read_record_uncached(subject, id)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 328, in _read_record_uncached
    return json.load(f)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/__init__.py", line 299, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
# I cleared this error by deleting the offending file:
hilo$ cp diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop21.fst.allChr.txt diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop21.fst.allChr.txt_BACKUP
hilo$ rm diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop21.fst.allChr.txt
hilo$ snakemake -n diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop21.fst.allChr.txt --quiet --profile slurm
Job counts:
        count   jobs
        1       calc_fst_all_chr
        1
        # BUT ERROR RETURNED! CLEARLY NOT ONLY OFFENDING FILE.
        hilo$ snakemake -n fst --quiet --profile slurm
# found 2nd problem file!
expand("diversity/results/fst/" + prefix_all + "/Ne10000_yesBoot/HOMOZYG/mexicana/{POP1}.{POP2}.fst.allChr.txt", POP1 = symp_maize_pops[3], POP2 = symp_mexicana_pops[10])
hilo$ snakemake -n diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop23.fst.allChr.txt --quiet --profile slurm
hilo$ mv diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop23.fst.allChr.txt diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop370.pop23.fst.allChr.txt_BACKUP
# having removed that output, snakemake now runs without errors:
hilo$ snakemake fst --quiet --profile slurm
Job counts:
        count   jobs
        237     calc_fst_all_chr # ACTUALLY LOOKS LIKE SOME OF THESE DIDN'T FINISH RUNNING (?). MAYBE THAT CAUSED THE SNAKEMAKE ERROR.
        182     calc_fst_idx
        182     estimate_2D_sfs
        1       fst
        602 # RUNNING 2.7.2021 11pm. didn't update git for Snakefile (b/c git status is being super slow and not returning anythin. ok though b/c fst outputs running match what's on laptop)
        # some timed out, so adding time and rerunning:
hilo$ snakemake fst --quiet --profile slurm
Job counts:
        count   jobs
        21      calc_fst_all_chr
        1       fst
        22 # RUNNING 2.9.2020. COMPLETED.
added new rule to combine fst results into 1 file using grep.
hilo$ snakemake fst --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       fst
        1       print_fst_one_file
        2 # COMPLETED
# number of overlapping sites can be pretty low but still many (not necessarily variable) between pops for homozygous ancestry, e.g.:
ecalfee@c6-93:~/hilo$ zcat diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop371.pop18.fst.gz | wc -l
676618
ecalfee@c6-93:~/hilo$ zcat diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop371.pop34.fst.gz | wc -l
661388
ecalfee@c6-93:~/hilo$ zcat diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop371.pop35.fst.gz | wc -l
722343
ecalfee@c6-93:~/hilo$ zcat diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop371.pop373.fst.gz | wc -l
393742
ecalfee@c6-93:~/hilo$ zcat diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop371.pop361.fst.gz | wc -l
293350
# 3 types of mexicana:
1) chalco central platue
2) plateau
3) nabogame is weird and different
# plotting fst:
hilo$ snakemake -n fst --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       fst
        1       plot_within_ancestry_fst
        2 # 2.13.2020. had to add in viridis package to run. (fixed script)
# rerun flowering time overlap analysis with 20kb buffer around genes:
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        24      calc_overlap
        25 # RUNNING 2.17.2021 11:36pm
        hilo$ snakemake -n all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                3       plot_admix_times
                1       test_key_genes
                5 # RUNNING 2.18.2021
# fixed an error affecting ZCMCCT10 in flowering time genes and re-running that enrichment analysis:
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       add_20kb_flowering_time_genes
        1       all
        24      calc_overlap
        1       find_flowering_time_genes_v4
        27 # 2.19.2021 10am

        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                1       plot_shared_peaks
                2 # remaking shared peaks plots
        # testing making regions file for getting pi/fst for a subset of the genome (outlier regions)
        hilo$ snakemake ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/maize/pop362.1pop.outliers.regions --quiet --profile slurm
        Job counts:
                count   jobs
                1       make_rf_outliers_by_pop
                1 # COMPLETED. But want to change file naming to not have a 'ZEA' wildcard.
        hilo$ snakemake ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/pop25.4pop.outliers.regions --quiet --profile slurm --restart-times 0
        Job counts:
                count   jobs
                1       make_rf_outliers_by_pop
                1
# getting pi and local Fst for introgression outlier regions:
# will pipeline start? can cancel.
hilo$ snakemake -n diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop371.pop29.4pop.outlierspop371.fst.allChr.txt --quiet --profile slurm --restart-times 0
Job counts:
        count   jobs
        1       calc_fst_all_outliers
        1       calc_fst_idx_outliers
        2       estimate_saf_pop_outliers
        1       make_rf_outliers_by_pop
        5 # DID NOT RUN, just checked what files it would make.
        hilo$ snakemake fst_mexicana_anc_outliers --quiet --profile slurm
        Job counts:
                count   jobs
                28      calc_fst_all_outliers
                28      calc_fst_idx_outliers
                56      calc_thetas_outliers
                56      estimate_saf_pop_outliers
                1       fst_mexicana_anc_outliers
                14      make_rf_outliers_by_pop
                183 # RUNNING 2.20.2021 4pm. weird error in fst files that the -sites weren't in order. Only didn't finish a few files despite error, but could affect many (?)
                hilo$ grep 'Potential big problem' -A 5 snake_logs/calc_fst_idx_outliers*.err
                # so I deleted outputs and redid the script to make contiguous regions files and am rerunning:
                hilo$ rm ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/*.regions
                hilo$ rm diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/*1pop.outlierspop*
                hilo$ rm diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/*4pop.outlierspop*
                /hilo$ snakemake fst_mexicana_anc_outliers --quiet --profile slurm
                Job counts:
                        count   jobs
                        28      calc_fst_all_outliers
                        28      calc_fst_idx_outliers
                        56      calc_pi_all_outliers
                        56      calc_thetas_outliers
                        56      estimate_saf_pop_outliers
                        1       fst_mexicana_anc_outliers
                        14      make_rf_outliers_by_pop
                        239 # RUNNING 2.21.2021
# making summary file for all fst estimates for mexicana outlier regions:
hilo$ snakemake fst_mexicana_anc_outliers --quiet --profile slurm
hilo$ snakemake -n fst_mexicana_anc_outliers --quiet --profile slurm
Job counts:
        count   jobs
        1       fst_mexicana_anc_outliers
        1       print_fst_outliers_one_file
        2 # RUNNING 2.24.2021
# copy over output files to laptop:
Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/summary*.txt diversity/results/fst/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/.
Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/pop*allChr.pestPG diversity/results/pi/HILO_MAIZE55/Ne10000_yesBoot/HOMOZYG/mexicana/.
# actually make a new rule so you don't have to copy individual files (summarises into 1 file)
hilo$ snakemake -n pi_fst --quiet --profile slurm
Job counts:
        count   jobs
        1       pi_fst
        1       print_fst_mexicana_peaks_one_file
        1       print_fst_one_file
        1       print_pi_mexicana_peaks_one_file
        1       print_pi_one_file
        5 # redo so pi results go to their own directory
        hilo$ snakemake pi_fst --quiet --profile slurm
        Job counts:
                count   jobs
                1       pi_fst
                1       print_pi_mexicana_peaks_one_file
                1       print_pi_one_file
                3 # COMPLETED 2.25.2021. oops want tP (pairwise theta) not tW (watterson's theta). re-doing:
                hilo$ snakemake pi_fst --quiet --profile slurm
Job counts:
        count   jobs
        1       pi_fst
        1       print_pi_mexicana_peaks_one_file
        1       print_pi_one_file
        3 # COMPLETED 2.25.2021.
        hilo$ snakemake -n pi_fst --quiet --profile slurm
        Job counts:
                count   jobs
                1       pi_fst
                1       plot_within_ancestry_fst_pi_peaks
                2 # COMPLETED 2.25.2021
# running new scripts to get PCA for putative inversion at mhl1 on chr9:
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       get_GL_mhl1_inv
        1       get_mhl1_inv_coord
        1       run_PCAngsd_mhl1_inv
        4 # RUNNING 2.27.2021. error w/ awk. fixed:
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                1       get_GL_mhl1_inv
                1       get_mhl1_inv_coord
                1       run_PCAngsd_mhl1_inv
                4 # fixed awk. but only that rule ran. needed to fix params -> input for regions_file
                hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       all
                        1       get_GL_mhl1_inv
                        1       run_PCAngsd_mhl1_inv
                        3 # RUNNING 2.27.2021 10:45am. issue with typo in output prefix (no dir found). fixed and retrying:
                        hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       get_GL_mhl1_inv
        1       run_PCAngsd_mhl1_inv
        3 # RUNNING 2.27.2021 COMPLETED.
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                1       combine_genome_scan_plots
                1       test_key_genes
                3 # REDOING domestication genes list and genome scan plots (adding labels)
                # ACK NEED TO DEBBUG GENOME SCAN PLOTTING SCRIPT -- ADD INPUTS??
                hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       all
                        1       combine_genome_scan_plots
                        1       plot_mhl1_inv
                        1       plot_mhl1_inv_PCAngsd
                        1       plot_within_ancestry_fst
                        1       plot_within_ancestry_fst_pi_peaks
                        6 # UPDATING SEVERAL PLOTS (incl. genomewide scan). All but pi_within_ancestry_fst ran. Fixed errors there and rerunning:
                        hilo$ snakemake all --quiet --profile slurm
                        Job counts:
                                count   jobs
                                1       all
                                1       plot_within_ancestry_fst
                                2 # COMPLETED. 2.28.2021
# double-checking number of merged bams:
hilo$ ls -lh filtered_bams/merged_bams/ | grep -v "home/ecalfee" > test/list_of_merged_bams.txt

# I realized bedtools intesect should have -u option for flowering time genes (reports original gene once if it overlaps any outliers). this should be the same as without -u unless genes are very big and can span 2+ disjoint outliers
# so I moved old flowering time overlap results to a new subdirectory: ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/old_intersect/.
# and these will rerun when I do rule 'all'
hilo$ snakemake some --quiet --profile slurm
Job counts:                                                                                                                      count   jobs
        24      calc_overlap
        2       calc_overlap_hits
        1       filter_raisd_hits
        1       get_fasta_hits
        1       id_raisd_hits
        1       some
        1       summarise_overlap_flowering_time
        31 # COMPLETED 3.3.2021. Oops! Need to reset maxN to 0.5 (not 0.001!)
        hilo$ snakemake some --quiet --profile slurm
        Job counts:
                count   jobs
                2       calc_overlap_hits
                1       filter_raisd_hits
                1       get_fasta_hits
                1       some
                5 # RUNNING after I did 'touch' to trigger rerun: touch domestication_scan/results/raisdHits.bed
# deleted outputs startign with raisdHits.bed and rerunning after talking w/ Jeff:
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        2       calc_overlap_hits
        1       filter_raisd_hits
        1       get_fasta_hits
        1       id_raisd_hits
        1       some
        6 # OOPS didn't pull from git. Retry:
        hilo$ snakemake some --quiet --profile slurm
        Job counts:
                count   jobs
                2       calc_overlap_hits
                1       filter_raisd_hits
                1       get_fasta_hits
                1       id_raisd_hits
                2       plot_overlap_hits
                1       some
                8 # RUNNING hilo$ touch ZAnc/results/flowering_time_genes_v4.plus20kb.bed
                # running simple enrichment test for domestication genes. Also re-doing test of key genes to use 20kb (not 10kb) buffer. And re-doing flowering time enrichment with N=1000 shuffles (not 100)
                touch ZAnc/results/flowering_time_genes_v4.plus20kb.bed
                touch data/key_genes.csv
                hilo$ snakemake all --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       add_20kb_domestication_genes
                        1       all
                        24      calc_overlap
                        2       calc_overlap_domestication_genes
                        1       combine_genome_scan_plots
                        1       make_bed_domestication_genes_from_lit
                        1       summarise_overlap_domestication_genes
                        1       summarise_overlap_flowering_time
                        1       test_key_genes
                        33 # oops! new bed file didn't run for domestication genes. will fix script:
                        hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       add_20kb_domestication_genes
        1       all
        2       calc_overlap_domestication_genes
        1       make_bed_domestication_genes_from_lit
        1       summarise_overlap_domestication_genes
        6 # RUNNING. Fixed typo in summarise script:
        hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       summarise_overlap_domestication_genes
        2 # ok but accidentally deleted 1st gene instead of header. also want ot use 5% cutoff. 2% is very very strict ~1% introgression into maize
        hilo$ rm domestication_scan/results/HILO_MAIZE55/Ne10000_yesBoot/*
        hilo$ rm domestication_scan/results/domestication_genes_from_lit.*
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       add_20kb_domestication_genes
                1       all
                24      calc_overlap
                2       calc_overlap_domestication_genes
                2       get_ancestry_domestication_genes
                2       make_bed_anc_outliers
                1       make_bed_domestication_genes_from_lit
                2       make_bed_slope_elev
                24      merge_bed_outliers
                1       summarise_overlap_domestication_genes
                1       summarise_overlap_flowering_time
                61 # RUNNING 3.4.2021
      hilo$ rm ZAnc/results/HILO_MAIZE55/Ne10000_yesBoot/*.perc02.bed # also removed these old results files
      # oops forgot to change summarise overlap to 5% cutoff
      hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       summarise_overlap_domestication_genes
        1       summarise_overlap_flowering_time
        3 # need to remake. printed wrong ancestry out:
        hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       make_table_domestication_genes
        2 # DONE>
# should have taken min/max of 5th column (ancestry) not 4th (length). redo:
hilo$ rm domestication_scan/results/HILO_MAIZE55/Ne10000_yesBoot/domestication_genes_from_lit.plus20kb.*.*_mexicana_ancestry.bed
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        2       get_ancestry_domestication_genes
        3 # done
# updated legend to have dashed line on shared peaks plots for individual chromosomes
hilo$ rm ZAnc/plots/Ne10000_yesBoot/*_chr_*.png
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       plot_shared_peaks
        2 # DONE but I want to change to only plotting inv4m (not all non-segregating inversions)
        hilo$ rm ZAnc/plots/Ne10000_yesBoot/*_chr_*.png
        hilo$ snakemake all --quiet --profile slurm
        Job counts:
                count   jobs
                1       all
                1       plot_shared_peaks
                2 # oops! getting rid of the inversions didn't quite work as I planned. redo:
                hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       plot_shared_peaks
        2 # COMPLETED. Uploaded new figs and submitting to bioRxiv 3.5.2021, 1:45am.


genes in maize introgression deserts:
1	4959130	5014850	    zagl1
1	23605800	23647370	gt1
1	270533675	270574776	tb1*
4	46330596	46375118	tga1*
4	61295574	61341350	bt2*
5	130767029	130809864	sweet4c
7	113552409	113592937	ra1

genes in mexicana introgression deserts:
1	228660489	228705551	ZmSh1-1
1	270533675	270574776	tb1*
3	12138279	12179065	ra2
4	43090568	43139167	su1
4	46330596	46375118	tga1*
4	61295574	61341350	bt2*
5	172392994	172450415	ae1
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        1       make_table_domestication_genes
        2 # making table

# making PREFIX be more flexible as a wildcard (HILO_MAIZE55 or HILO_MAIZE55_PARV50)
# so I made a list of bams and ids for the new set HILO_MAIZE55_PARV50:
hilo$ cp samples/HILO_MAIZE55_bams.list samples/HILO_MAIZE55_PARV50_bams.list
hilo$ cat samples/ALL_byPop/parv_bams.list >> samples/HILO_MAIZE55_PARV50_bams.list
hilo$ cp samples/HILO_MAIZE55_ids.list samples/HILO_MAIZE55_PARV50_ids.list
hilo$ cat samples/ALL_byPop/parv_ids.list >> samples/HILO_MAIZE55_PARV50_ids.list
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       add_parv_2_meta
        1       all
        1       calc_depth
        426     call_SNPs
        2       plot_PCAngsd
        1       results_NGSAdmix
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        435 # oops an error with PREFIX wildcards vs prefix_all for HILO_MAIZE55.
        # so I'm rerunning.
# got new domestication scan results from Dan (fixing error with mop/raisd):
domestication_scan/results/RAiSD_Output2.raisd
# now running through pipeline to test for enrichment.
# deleted previous output files.
hilo$ snakemake -n all --profile slurm

hilo$ snakemake domestication_scan/plots/HILO_MAIZE55/Ne10000_yesBoot/raisdOverlap.maize_neg_meanAnc_outliers.perc05.png domestication_scan/plots/HILO_MAIZE55/Ne10000_yesBoot/raisdOverlap.mexicana_pos_meanAnc_outliers.perc05.png --quiet --profile slurm
Job counts:
        count   jobs
        2       calc_overlap_hits
        1       filter_raisd_hits
        1       get_fasta_hits
        1       id_raisd_hits
        2       plot_overlap_hits
        7 # no priority. low number of jobs (only 2 max in parallel, 8G each), so I'm switching to high2 partition
        hilo$ snakemake domestication_scan/plots/HILO_MAIZE55/Ne10000_yesBoot/raisdOverlap.maize_neg_meanAnc_outliers.perc05.png domestication_scan/plots/HILO_MAIZE55/Ne10000_yesBoot/raisdOverlap.mexicana_pos_meanAnc_outliers.perc05.png --quiet --profile slurm
        Job counts:
                count   jobs
                2       calc_overlap_hits
                1       filter_raisd_hits
                1       get_fasta_hits
                1       id_raisd_hits
                2       plot_overlap_hits
                7 # COMPLETED 3.15.2021
# after raisd runs, re-start SNP calling for 3-way structure/PCA with HILO_MAIZE55_PARV50 dataset:
hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        426     call_SNPs
        1       plot_PCAngsd
        1       results_NGSAdmix
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        432 # RUNNING 3.15.2021. I CANCELLED to run domestication scan small job 'some' (not getting priority anyways)
        hilo$ squeue -u ecalfee | grep 'call_SNP' | awk '{print $1}' | xargs -n 1 scancel
        hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        6       calc_overlap_hits
        6       plot_overlap_hits
        1       some
        13 # COMPLETED 3.15.2021
        hilo$ snakemake -n all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        426     call_SNPs
        1       plot_PCAngsd
        1       results_NGSAdmix
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        432 # RUNNING AGAIN 3.16.2021 (12:47am) -- check on this tomorrow to see if it ever got priority.
# I cancelled 2 jobs that have been stalled for a long time:
ecalfee@farm:~$ squeue -u ecalfee
         JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
      32158210      med2 call_SNP  ecalfee  S     4:30:28      1 4   16G    c6-89
      32159545      med2 call_SNP  ecalfee  S     2:38:15      1 4   16G    c6-89
ecalfee@farm:~$ scancel 32158210 32159545
# rerunning snakemake to get the remaining jobs running:
ecalfee@farm:~/hilo$ snakemake all --quiet --profile slurm
Job counts:
        count   jobs
        1       all
        32      call_SNPs
        1       plot_PCAngsd
        1       results_NGSAdmix
        1       run_NGSAdmix
        1       run_PCAngsd
        1       thin_GL_4PCA
        38 # RUNNING 3.19.2021
        # thin_GL_4PCA didn't find output file. but did prefix_all = HILO_MAIZE55 instead, so wrote over the HILO_MAIZE55/whole_genome.beagle.gz so will be re-triggering PCAs/NGSAdmix from there..nbd
        hilo$ ls -l global_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.beagle.gz
        -rw-rw-r-- 1 ecalfee ecalfee 330450138 Mar 20 05:44 global_ancestry/results/thinnedSNPs/HILO_MAIZE55/whole_genome.beagle.gz
        fixed output file naming to use PREFIX wildcard and rerunning:
        hilo$ snakemake all --quiet --profile slurm
      Job counts:
              count   jobs
              1       all
              1       plot_NGSAdmix
              2       plot_PCAngsd
              2       results_NGSAdmix
              2       run_NGSAdmix
              2       run_PCAngsd
              1       thin_GL_4PCA
              11 # RUNNING 3.22.2021

# downloaded supporting file S3 from ogut 2015 (original 0.2cM rmap on v2) from https://www.panzea.org/publications
# used excel (and copy paste into a regular text using nano) to convert marker positions into a bed file.
chr, start (=original position - 1), end (= original position), marker name, marker cM position
# send that bed file into Assembly Converter:
https://plants.ensembl.org/Zea_mays/Tools/AssemblyConverter
input file: hilo/data/linkage_map/ogut_2015_v2_coord_from_supp_file_S3.bed
Assembly Converter	R5sWAIAz2D1GNGtF	Assembly conversion of ogut supp S3 v2 -> v4 EC in Zea_maysRunning	20/03/2021, 17:56 (GMT)
# so made a new linkage_map/Snakefile and replaced rmap_ext with the new EXTENDED v2->v4 recombination map output file to be used in the rest of the pipeline
# this is going to trigger a large rerun from everything that builds off of ancestry_by_r or local_ancestry

# I want to first trigger just a rerun of PCA/NGSAdmix plotting
# starting with results_NGSAdmix for both K = 2 and K = 3
hilo$ snakemake -n global_ancestry/plots/HILO_MAIZE55_pca.png global_ancestry/plots/HILO_MAIZE55_PARV50_pca.png global_ancestry/plots/HILO_MAIZE55_PARV50_structure_K3.png global_ancestry/plots/global_anc_multi.png --quiet --profile slurm
Job counts:
        count   jobs
        1       plot_NGSAdmix
        1       plot_NGSAdmix_parv
        2       plot_PCAngsd
        4 # hmm. I want it to rerun results_NGSAdmix too
touch global_ancestry/results/NGSAdmix/HILO_MAIZE55_PARV50/K3.qopt
touch global_ancestry/results/NGSAdmix/HILO_MAIZE55/K2.qopt
hilo$ snakemake global_ancestry/plots/HILO_MAIZE55_pca.png global_ancestry/plots/HILO_MAIZE55_PARV50_pca.png global_ancestry/plots/HILO_MAIZE55_PARV50_structure_K3.png global_ancestry/plots/global_anc_multi.png --quiet --profile slurm
Job counts:
        count   jobs
        1       plot_NGSAdmix
        1       plot_NGSAdmix_parv
        2       plot_PCAngsd
        2       results_NGSAdmix
        6 # RUNNING 3.25.2021

# oops forgot to make lzw versions and there was a typo in plot_NGSAdmix_parv. rerunning:
# also adding in supp plot for new rmap and EXTENDED rmap
hilo$ snakemake -n linkage_map/results/ogut_2015_rmap_v2_to_v4_EXTENDED.txt ../hilo_manuscript/figures_supp/ogut_2015_v2_to_v4_rmap.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_pca.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_PARV50_pca.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_PARV50_structure_K3.tif ../hilo_manuscript/figures_main/global_anc_multi.tif --quiet --profile slurm
Job counts:
        count   jobs
        1       clean_ogut_2015_map
        1       extend_ogut_2015_map
        1       plot_NGSAdmix
        1       plot_NGSAdmix_parv
        2       plot_PCAngsd
        1       plot_ogut_2015_map
        7 # RUNNING 3.25.2021. fixed some typos:
        hilo$ snakemake linkage_map/results/ogut_2015_rmap_v2_to_v4_EXTENDED.txt ../hilo_manuscript/figures_supp/ogut_2015_v2_to_v4_rmap.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_pca.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_PARV50_pca.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_PARV50_structure_K3.tif ../hilo_manuscript/figures_main/global_anc_multi.tif --quiet --profile slurm
        Job counts:
                count   jobs
                1       plot_NGSAdmix
                1       plot_NGSAdmix_parv
                2       plot_PCAngsd
                4 fixed some more typos for last rule to run:
                hilo$ snakemake linkage_map/results/ogut_2015_rmap_v2_to_v4_EXTENDED.txt ../hilo_manuscript/figures_supp/ogut_2015_v2_to_v4_rmap.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_pca.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_PARV50_pca.tif ../hilo_manuscript/figures_supp/HILO_MAIZE55_PARV50_structure_K3.tif ../hilo_manuscript/figures_main/global_anc_multi.tif --quiet --profile slurm
                Job counts:
                        count   jobs
                        1       plot_NGSAdmix_parv
                        1 # COMPLETED.
# cleaned up rule all 3.26.2021 to make sure I'm re-doing all main and supp figures/tables. now will try to rerun rule all (will trigger b/c of new map)
hilo$ snakemake -n all --quiet --profile slurm
SyntaxError in line 197 of /home/ecalfee/hilo/Snakefile:
Expecting rule keyword, comment or docstrings inside a rule definition. (Snakefile, line 197)
# hmm... not sure of error. It was having a comment as first line of input (not allowed). now fixed:
# now saving what it's going to run so I can check it over first:
hilo$ snakemake -n all --quiet --profile slurm > to_run.txt
# make and inspect 1cM recombination map before proceeding with full pipeline.
MissingInputException in line 142 of /home/ecalfee/hilo/Snakefile:
Missing input files for rule all:
../hilo_manuscript/figures_main/Ne10000_yesBoot_maize_shared_outliers_chr_4.tif # brackets issue (now fixed) and will list files:
hilo$ snakemake -n ../hilo_manuscript/figures_main/Ne10000_yesBoot_maize_shared_outliers_chr_4.tif --quiet --profile slurm

# also saved old map, including 1cM windows, to hilo/data/linkage_map/old/
# hilo$ cp ancestry_by_r/results/map_pos_1cM_windows.txt data/linkage_map/old/.
hilo$ snakemake ancestry_by_r/results/map_pos_1cM_windows.txt --quiet --profile slurm
Job counts:
        count   jobs
        1       define_1cM_windows
        1 # COMPLETED 3.26.2021 to check on this first.
        # what jobs will run for rule 'all'. let's see:
        hilo$ snakemake -n all --quiet --profile slurm > to_run.txt
        Job counts:
        	count	jobs
        	1	all
        	84	boot_ancestry_hmm
        	2	calc_K_matrix_zea
        	2	calc_and_plot_f4
        	2	calc_fdr_mean_anc
        	756	calc_fst_all_chr # lots of runs b/c all possible pairs.
        	28	calc_fst_all_outliers
        	756	calc_fst_idx
        	28	calc_fst_idx_outliers
        	24	calc_overlap
        	2	calc_overlap_domestication_genes
        	2	calc_overlap_hits
        	56	calc_pi_all_chr
        	56	calc_pi_all_outliers
        	426	calc_rmap_pos
        	56	calc_thetas
        	56	calc_thetas_outliers
        	1	combine_genome_scan_plots
        	6	combine_pop_ancestry_data
        	403	count_ACGT
        	403	count_maj_min
        	4560	do_abba_baba
        	756	estimate_2D_sfs
        	56	estimate_saf_pop
        	56	estimate_saf_pop_outliers
        	56	estimate_sfs
        	3	f4_from_abba_baba
        	1520	find_SNPs_in_window # this is too many windows. will fix. with new map should now be 1477 windows/runs
        	2	fit_lm_elev
        	1	get_GL_mhl1_inv
        	2	get_ancestry_domestication_genes
        	56	get_homozygous_ancestry_bams
        	56	get_homozygous_ancestry_tracts
        	1	get_mhl1_inv_coord
        	1	get_tracts_from_sites
        	1	index_sites_4HMM
        	1010	label_k_anc # because it's 100 bootstraps X 10 categories (r/cd/5 bins) + the 10 runs for the nonbootstrap originals
        	56	list_homozygous_ancestry_bams
        	1	make_allo_counts
        	2	make_bed_anc_outliers
        	1	make_bed_file_for_windows
        	2	make_bed_slope_elev
        	1010	make_bootstrap_GL_file
        	28	make_input_hmm
        	14	make_rf_outliers_by_pop
        	1	make_table_domestication_genes
        	28	map_local_anc_onto_1cM_windows
        	24	merge_bed_outliers
        	1	plotQQ
        	3	plot_admix_times
        	1	plot_bootstrap_NGSadmix_by_r
        	1	plot_bootstrap_local_ancestry_by_r
        	1	plot_map
        	2	plot_mean_anc
        	1	plot_mhl1_inv
        	1	plot_mhl1_inv_PCAngsd
        	2	plot_overlap_hits
        	1	plot_shared_peaks
        	2	plot_slope_elev
        	1	plot_total_samples
        	1	plot_within_ancestry_fst
        	1	plot_within_ancestry_fst_pi_peaks
        	1	print_fst_mexicana_peaks_one_file
        	1	print_fst_one_file
        	1	print_pi_mexicana_peaks_one_file
        	1	print_pi_one_file
        	1	run_PCAngsd_mhl1_inv
        	1010	run_bootstrap_NGSAdmix
        	2	sample_bootstrap_1cM
        	2	simulate_MVN
        	2	summarise_bootstrap_anc_by_r
        	28	summarise_local_anc_by_1cM
        	1	summarise_overlap_domestication_genes
        	1	summarise_overlap_flowering_time
        	84	summarise_posterior
        	1	test_key_genes
        	1	thin_sites_4HMM
        	13610
hilo$ snakemake -n all --quiet --profile slurm > to_run.txt # checked one more time. now running:
hilo$ snakemake all --quiet --profile slurm
Job counts:
	count	jobs
	1	all
	84	boot_ancestry_hmm
	2	calc_K_matrix_zea
	2	calc_and_plot_f4
	2	calc_fdr_mean_anc
	756	calc_fst_all_chr
	28	calc_fst_all_outliers
	756	calc_fst_idx
	28	calc_fst_idx_outliers
	24	calc_overlap
	2	calc_overlap_domestication_genes
	2	calc_overlap_hits
	56	calc_pi_all_chr
	56	calc_pi_all_outliers
	426	calc_rmap_pos
	56	calc_thetas
	56	calc_thetas_outliers
	1	combine_genome_scan_plots
	6	combine_pop_ancestry_data
	403	count_ACGT
	403	count_maj_min
	4431	do_abba_baba
	756	estimate_2D_sfs
	56	estimate_saf_pop
	56	estimate_saf_pop_outliers
	56	estimate_sfs
	3	f4_from_abba_baba
	1477	find_SNPs_in_window
	2	fit_lm_elev
	1	get_GL_mhl1_inv
	2	get_ancestry_domestication_genes
	56	get_homozygous_ancestry_bams
	56	get_homozygous_ancestry_tracts
	1	get_mhl1_inv_coord
	1	get_tracts_from_sites
	1	index_sites_4HMM
	1010	label_k_anc
	56	list_homozygous_ancestry_bams
	1	make_allo_counts
	2	make_bed_anc_outliers
	1	make_bed_file_for_windows
	2	make_bed_slope_elev
	1010	make_bootstrap_GL_file
	28	make_input_hmm
	14	make_rf_outliers_by_pop
	1	make_table_domestication_genes
	28	map_local_anc_onto_1cM_windows
	24	merge_bed_outliers
	1	plotQQ
	3	plot_admix_times
	1	plot_bootstrap_NGSadmix_by_r
	1	plot_bootstrap_local_ancestry_by_r
	1	plot_map
	2	plot_mean_anc
	1	plot_mhl1_inv
	1	plot_mhl1_inv_PCAngsd
	2	plot_overlap_hits
	1	plot_shared_peaks
	2	plot_slope_elev
	1	plot_total_samples
	1	plot_within_ancestry_fst
	1	plot_within_ancestry_fst_pi_peaks
	1	print_fst_mexicana_peaks_one_file
	1	print_fst_one_file
	1	print_pi_mexicana_peaks_one_file
	1	print_pi_one_file
	1	run_PCAngsd_mhl1_inv
	1010	run_bootstrap_NGSAdmix
	2	sample_bootstrap_1cM
	2	simulate_MVN
	2	summarise_bootstrap_anc_by_r
	28	summarise_local_anc_by_1cM
	1	summarise_overlap_domestication_genes
	1	summarise_overlap_flowering_time
	84	summarise_posterior
	1	test_key_genes
	1	thin_sites_4HMM
	13438 # RUNNING 3.26.2021 5:26pm
hmm.. not sure why calculating and plotting f4s would fail: nano snake_logs/*32568936.err
I did crtl+c to cancel snakemake pipeline and restart because it wasn't running any jobs in squeue
# what is left to run?
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n all --quiet --profile slurm > to_run.txt

# TO DO:
- check on snakemake pipeline -- still running smoothly?
- add supp. plot for raisd output.
- check on the regions you dropped markers from the map by hand -- did you plot all of them?
- ask Graham about that one sentence you re-worded
- get feedback on Graham and Jeff about 4 updates
- try the rainbow map of mexico plot

### TO DO FOR K=3 GENERALIZATION:
- local ancestry by R scripts need to be updated.
- re-running global ancestry by R scripts. Need to update plotting script only:
hilo$ snakemake -n ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/r5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/cd5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55_PARV50/r5_K3.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55_PARV50/cd5_K3.Rdata --quiet --profile slurm
# oops needed to make some generalizations/changes upstream (e.g. for gl files that get bootstrapped)
# now running:
hilo$ snakemake -n ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/r5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55/cd5_K2.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55_PARV50/r5_K3.Rdata ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55_PARV50/cd5_K3.Rdata --quiet --profile slurm
Job counts:
        count   jobs
        1477    find_SNPs_in_window
        1010    label_k_anc
        1010    make_bootstrap_GL_file
        1010    run_bootstrap_NGSAdmix
        2       sample_bootstrap_1cM
        2       summarise_bootstrap_anc_by_r
        4511 # label_k_anc had errors - CHECK ON THESE! e.g. cluster_jobid: 32772540.
        problem:
        `summarise()` has grouped output by 'zea'. You can override using the `.groups` argument.
          Error: Each row of output must be identified by a unique combination of keys.
          Keys are shared for 906 rows:
          * 1, 2
          * 4, 5
          * 7, 8
          * 10, 11
          * 13, 14
          * 16, 17 ... etc. error message goes on
# I've removed all hard coded references to all_bams and all_ids b/c that's not compatible with alt. prefix HILO_MAIZE55_PARV50
# now instead input files call the rules get_all_bams() and get_all_bais() that takes in a prefix
# -all_bams, all_ids # all input bams needs to include parviglumis bams too (rule allo_freqs)

# use rule some to get allopatric allele freqs at all sites:
rule some:
    input:
        expand("local_ancestry/results/alloFreqs/{PREFIX}/{GROUP}/{REGION}.mafs.gz", PREFIX = ["HILO_MAIZE55_PARV50", "HILO_MAIZE55"], GROUP = ["allopatric_maize", "allopatric_mexicana", "parv"], REGION = list(regions_dict.keys())))
    params:
        p = "med2"
    resources:
        time_min = 5,
        mem = 2
        hilo$ snakemake some --quiet --profile slurm
        Job counts:
                count   jobs
                1704    allo_freqs
                426     index_sites_file
                426     make_sites_file
                1       some
                2557 # RUNNING 4.14.2021. Problem in allo_freqs lookign for folder with {PREFIX} in it. Trying again with wildcards.PREFIX there:
# errorno 12 Cannot allocate memory. Looking now at one failed job:
32963112
hilo$ nano snake_logs/*32963112.err
Failed to open file "filtered_bams/results/PalmarChico/PARV1.sort.dedup.baq.bam" : No such file or directory
# ORIGINAL ERRORS (in addition to individual jobs failing):
ecalfee@farm:~$ screen -r 1161.snake

    keepincomplete=keep_incomplete,
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/workflow.py", line 908, in execute
    success = scheduler.schedule()
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/scheduler.py", line 378, in schedule
    self.run(job)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/scheduler.py", line 397, in run
    error_callback=self._error,
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/executors.py", line 915, in run
    shell=True,
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 356, in check_output
    **kwargs).stdout
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 423, in run
    with Popen(*popenargs, **kwargs) as process:
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 729, in __init__
    restore_signals, start_new_session)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/subprocess.py", line 1295, in _execute_child
    restore_signals, start_new_session, preexec_fn)
OSError: [Errno 12] Cannot allocate memory

# NOW STOPS WITH ERRORS:
(hilo-env) ecalfee@farm:~/hilo$ snakemake -n some --quiet --profile slurm
Traceback (most recent call last):
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/__init__.py", line 665, in snakemake
    keepincomplete=keep_incomplete,
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/workflow.py", line 619, in execute
    dag.check_incomplete()
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/dag.py", line 286, in check_incomplete
    incomplete = self.incomplete_files
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/dag.py", line 410, in incomplete_files
    filterfalse(self.needrun, self.jobs),
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/dag.py", line 407, in <genexpr>
    job.output
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 205, in incomplete
    return any(map(lambda f: f.exists and marked_incomplete(f), job.output))
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 205, in <lambda>
    return any(map(lambda f: f.exists and marked_incomplete(f), job.output))
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 203, in marked_incomplete
    return self._read_record(self._metadata_path, f).get("incomplete", False)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 322, in _read_record_cached
    return self._read_record_uncached(subject, id)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/site-packages/snakemake/persistence.py", line 328, in _read_record_uncached
    return json.load(f)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/__init__.py", line 299, in load
    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/home/ecalfee/.conda/envs/hilo-env/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

# SAME ERRORS WHEN I USE 'UNLOCK'
hilo$ snakemake -n some --unlock --quiet --profile slurm
# but some individual files run without errors:
hilo$ snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/allopatric_maize/region_82.mafs.gz --quiet --profile slurm
# TESTING WHICH FILES ARE A PROBLEM SO I CAN SELECTIVELY DELETE THEM:
hilo$ for g in allopatric_maize allopatric_mexicana parv; do (for i in {0..425}; do echo "region " $i; snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/$g/region_$i.mafs.gz --quiet --profile slurm; done) > test_list_$g_files.txt; done
# CANCELLED THIS SCREEN SESSION TO STOP IT. TRYING NEW APPROACH:
hilo$ (for g in allopatric_maize allopatric_mexicana parv; do for i in {0..425}; do echo "group " $g "  region " $i; snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/$g/region_$i.mafs.gz --quiet --profile slurm; done; done) > test_list_files.txt

# problem files:
# output file does not exist (!) for these 3 files
hilo$ snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/allopatric_maize/region_224.mafs.gz --quiet --profile slurm
hilo$ snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/allopatric_mexicana/region_224.mafs.gz --quiet --profile slurm
hilo$ snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/parv/region_224.mafs.gz --quiet --profile slurm
TROUBLESHOOTING (for parv file region 224):
--unlock # didn't do anything
--cleanup-metadata # didn't change error
--ignore-incomplete # WORKS! But where is the incomplete file? digging deeper...
hilo$ snakemake -n variant_sites/results/HILO_MAIZE55_PARV50/region_224.var.sites.idx --profile slurm --quiet
# problem!
hilo$ snakemake -n variant_sites/results/HILO_MAIZE55_PARV50/region_224.var.sites --profile slurm --quiet
# no problem.
# so I delete the .idx and .bin files:
hilo$ rm variant_sites/results/HILO_MAIZE55_PARV50/region_224.var.sites.*
# testing:
hilo$ snakemake -n local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/parv/region_224.mafs.gz local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/allopatric_maize/region_224.mafs.gz local_ancestry/results/alloFreqs/HILO_MAIZE55_PARV50/allopatric_mexicana/region_224.mafs.gz --profile slurm --quiet
Job counts:
        count   jobs
        3       allo_freqs
        1       index_sites_file
        4 # YAY! I don't need to ignore incomplete files anymore..the var.sites.idx or .bin must've been incomplete
        hilo$ snakemake some --profile slurm --quiet
        Job counts:
                count   jobs
                941     allo_freqs
                303     index_sites_file
                266     make_sites_file
                1       some
                1511 # RUNNING 4.26.2021. Did ^C 4.27.2021 at 10:24am b/c no jobs were running on squeue.
# make a small regions file for 3 regions that are done for mex/maize/parv to test the new thinning script:
(base) Erins-Air:hilo Erin$ awk '$4==33' data/refMaize/divide_5Mb/ALL_regions.list > test/TEST3_regions.list
(base) Erins-Air:hilo Erin$ awk '$4==82' data/refMaize/divide_5Mb/ALL_regions.list >> test/TEST3_regions.list
(base) Erins-Air:hilo Erin$ awk '$4==412' data/refMaize/divide_5Mb/ALL_regions.list >> test/TEST3_regions.list
hilo Erin$ for i in 33 82 412; do scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/variant_sites/results/HILO_MAIZE55_PARV50/region_$i.var.sites variant_sites/results/HILO_MAIZE55_PARV50/.; done
hilo Erin$ for i in 33 82 412; do scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/variant_sites/results/HILO_MAIZE55_PARV50/region_$i.rpos variant_sites/results/HILO_MAIZE55_PARV50/.; done
# also copied over var.sites files for HILO_MAIZE55 and HILO_MAIZE55_PARV50 and used variant_sites/approx_rpos.R to get rpos files to test thinning SNPs for K2 and K3 cases with just a few files

# where did the previous run fail/stop?
snakemake -n maize --quiet --profile slurm
# UGH getting same error as earlier about incomplete files missing. json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
# region 8 is the problem:
hilo$ rm variant_sites/results/HILO_MAIZE55_PARV50/region_8.var.sites.*
# the indexing job was cancelled because it ran out of time.

# trying to rerun out to thinning snps
hilo$ snakemake local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/whole_genome.var.sites local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/whole_genome.var.sites --quiet --profile slurm
Job counts:
        count   jobs
        124     allo_freqs
        426     calc_rmap_pos
        62      index_sites_file
        29      make_sites_file
        1       thin_sites_4HMM_K2
        1       thin_sites_4HMM_K3
        643 # RUNNING 4.27.2021 11:48am. oops! rule didn't have all the parv files as input so those didn't run.
        hilo$ snakemake local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/whole_genome.var.sites local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/whole_genome.var.sites --quiet --profile slurm
        Job counts:
                count   jobs
                62      allo_freqs
                1       thin_sites_4HMM_K2
                1       thin_sites_4HMM_K3
                64 # RUNNING evening 4.27.2021. K2 timed out :( so didn't rerun. will have to edit time and try again. also set it to top 15% b/c maybe just had way too many snps left
                rule some:
                    input:
                        expand("local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne{Ne}_yesBoot/{POP}.completed", Ne = Nes, POP = symp_pops),
                        expand("local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne{Ne}_yesBoot/anc/{POP}.anc.freq", Ne = Nes, POP = symp_pops)
                    params:
                        p = "med2"
                    resources:
                        time_min = 60,
                        mem = 2
                # using rule 'some' above to get local ancestry calls for K2 and K3. w/ K2 I can summarise hte ancestry, for K3 I need to look at the output first before writing a script for that.
                hilo$ snakemake -n some --quiet --profile slurm
                Job counts:
                        count   jobs
                        84      boot_ancestry_hmm_K2
                        84      boot_ancestry_hmm_K3
                        856     count_ACGT
                        856     count_maj_min
                        2       index_sites_4HMM
                        1       make_allo_counts_K2
                        1       make_allo_counts_K3
                        56      make_input_hmm
                        50      merge_bams # UH OH NO BAMS SHOULD BE MERGING! CHECK BAM INPUTS FOR PARV.
                        1       some
                        84      summarise_posterior_K2
                        1       thin_sites_4HMM_K2
                        2076 # inspecting DAG further and sending to output file test/testing_dag_long.txt
                        hilo$ snakemake local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/whole_genome.var.sites local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/whole_genome.var.sites --profile slurm > test/testing_dag_long.txt
                        # COMPLETED! 4.29.2021
                        # HMM THIS JUST RAN, but didn't check for the merging files for parv that were unexpected.
                        hilo$ snakemake -n some --dag | dot -Tsvg > test/dag.svg #CANCELLED. HADN'T DONE ANYTHING IN LIKE A DAY. Maybe DAG is way too big? I thought it was a summary. Trying again with 'quiet' option:
                        hilo$ snakemake -n some --quiet --dag | dot -Tsvg > test/dag.svg # 4.30.2021 10:38am
                        # it's because count_ACGT has input "filtered_bams/merged_bams/{ID}.sort.dedup.bam" but I could get it from bam list instead!
                        "samples/{PREFIX}_K{K}_byInd/{ID}_bams.list". Try again with new rule and function to lookup the correct bam (so no need for symlinks of maize and parv files to merged_bams/ directory)
                        hilo$ snakemake -n some --quiet --profile slurm
                        Job counts:
                                count   jobs
                                84      boot_ancestry_hmm_K2
                                84      boot_ancestry_hmm_K3
                                856     count_ACGT
                                856     count_maj_min
                                2       index_sites_4HMM
                                1       make_allo_counts_K2
                                1       make_allo_counts_K3
                                56      make_input_hmm
                                453     make_single_bam_list
                                1       some
                                84      summarise_posterior_K2
                                2478 # yay! will add some mafs.gz files for Jeff G to rule some before running.
                        hilo$ snakemake some --quiet --profile slurm
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        84      boot_ancestry_hmm_K2
        84      boot_ancestry_hmm_K3
        856     count_ACGT
        856     count_maj_min
        2       index_sites_4HMM
        1       make_allo_counts_K2
        1       make_allo_counts_K3
        56      make_input_hmm
        453     make_single_bam_list
        34      pop_allele_freqs
        1       some
        84      summarise_posterior_K2
        2512 # RUNNING 4.30.2021 5:30pm
        RuleException in line 201 of /home/ecalfee/hilo/local_ancestry/Snakefile:
'OutputFiles' object has no attribute 'bam_list'
# also a bunch of jobs had failed launches?
      33604420      med2 make_sin  ecalfee PD        0:00      1 1   2G     (launch failed requeued held)
squeue -u ecalfee | grep 'launch failed' | awk '{print $1}' | xargs -n 1 scancel
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        84      boot_ancestry_hmm_K2
        84      boot_ancestry_hmm_K3
        856     count_ACGT
        856     count_maj_min
        1       make_allo_counts_K2
        1       make_allo_counts_K3
        56      make_input_hmm
        14      make_single_bam_list
        34      pop_allele_freqs
        1       some
        84      summarise_posterior_K2
        2071 # RUNNING 5.1.2021. ARG, this exception comes up when tryign to run:
        RuleException in line 4 of /home/ecalfee/hilo/wavelets/Snakefile:
        'Params' object has no attribute 'region'
        # Fixed rule in wavelets: pop_allele_freqs and Rerunning
        hilo$ snakemake some --quiet --profile slurm
        # RUNNING 5.2.2021. SQUEUE SHOWS MANY JOBS GOING.
# HMM.. some error with some of the maize files counting ACGT (?)
hilo$ less snake_logs/*33676903.err
Failed to open file "filtered_bams/merged_bams/MAIZE41.sort.dedup.bam" : No such file or directory
# oops these didn't make over new single bam files..they are all old in the HILO_MAIZE55_byInd folder
# which won't make a difference for sympatric individuals but won't find the file (b/c of shadow)
# for allopatric maize (palmar chico) b/c it's pointing to the symlink not the file, e.g.
-rw-rw-r-- 1 ecalfee ecalfee 49 Oct 27  2020 samples/HILO_MAIZE55_byInd/MAIZE41_bams.list
hilo$ cat samples/HILO_MAIZE55_byInd/MAIZE41_bams.list
filtered_bams/merged_bams/MAIZE41.sort.dedup.bam
# when i want it to now point to the original file (not the symlink) like the _PARV50 files do:
hilo$ cat samples/HILO_MAIZE55_PARV50_byInd/MAIZE41_bams.list
filtered_bams/results/Maize55/MAIZE41.sort.dedup.bam
# so I am now removing these old files so the pipeline will create new ones
hilo$ rm samples/HILO_MAIZE55_byInd/MAIZE*_bams.list
# mostly just stalled jobs in squeue right now. I will cancel and restart snakemake pipeline
JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
33718876     high2     bash  ecalfee  R       16:13      1 1   2G     c6-93
33666776      med2 pop_alle  ecalfee  R     4:10:40      1 5   16G    c6-95
33666782      med2 count_AC  ecalfee  S     6:04:32      1 2   8G     c6-93
33666704      med2 count_AC  ecalfee  S     6:00:48      1 2   8G     c6-93
33666698      med2 pop_alle  ecalfee  S     5:33:27      1 5   16G    c6-93
33666701      med2 count_AC  ecalfee  S     6:52:57      1 2   8G     c6-93
hilo$ scancel 33666776 33666782 33666704 33666698 33666701
# also added some pop_allele_freqs outputs requested to rule 'some':
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        84      boot_ancestry_hmm_K2
        84      boot_ancestry_hmm_K3
        91      count_ACGT
        91      count_maj_min
        1       make_allo_counts_K2
        1       make_allo_counts_K3
        56      make_input_hmm
        59      make_single_bam_list
        4       pop_allele_freqs
        1       some
        84      summarise_posterior_K2
        556 # BUT I ADDED SOME POP ALLELE FREQS OUTPUTS REQUESTED IN RULE 'some'
# a few stalled jobs and a few had problems making majmin counts files because
# counts and pos.gz files from count_ACGT don't match in # SNPs because of a hidden error:
e.g. hilo$ less snake_logs/count_ACGT.ID=MAIZE39,K=2,PREFIX=HILO_MAIZE55.33733486.err
-> Printing at chr: 6 pos:156070436 chunknumber 2227700 contains 368 sites
[master2] Problem spawning thread
Resource temporarily unavailable at chunknumber:2227770
[Wed May  5 08:47:59 2021]
Finished job 0.
1 of 1 steps (100%) done
# squeue:
JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
33779371     high2     bash  ecalfee  R       16:10      1 1   2G     c6-68
33741222      med2 pop_alle  ecalfee  R     1:26:28      1 9   32G    c6-92
33733546      med2 count_AC  ecalfee  S     6:46:29      1 2   8G     c6-95
33719793      med2 count_AC  ecalfee  S     2:07:12      1 2   8G     c6-72
33719794      med2 pop_alle  ecalfee  S     2:07:12      1 5   16G    c6-72
33719795      med2 pop_alle  ecalfee  S     2:07:12      1 5   16G    c6-72
33731714      med2 count_AC  ecalfee  S     3:31:19      1 2   8G     c6-76
# cancelling running jobs
hilo$ squeue -u ecalfee | grep 'med2' | awk '{print $1}' | xargs -n 1 scancel
# which ones are potentially problem files?
hilo$ for i in $(cat samples/HILO_MAIZE55_ids.list); do ls local_ancestry/results/countsMajMin/HILO_MAIZE55/K2/$i.counts.txt; done
MAIZE1 - no ACGT
MAIZE6
MAIZE32 - no ACGT
MAIZE39
MAIZE40 - no ACGT
MAIZE41 - no ACGT
MAIZE48 - no ACGT
MAIZE49 - no ACGT
# confirmed those 2 have messed up files:
hilo$ for i in MAIZE6 MAIZE39; do zcat local_ancestry/results/countsACGT/HILO_MAIZE55/K2/$i.counts.gz | wc -l; zcat local_ancestry/results/countsACGT/HILO_MAIZE55/K2/$i.pos.gz | wc -l; done
277370
280604
228879
232913
# remove files to fix:
hilo$ rm local_ancestry/results/countsACGT/HILO_MAIZE55/K2/MAIZE6.counts.gz
hilo$ rm local_ancestry/results/countsACGT/HILO_MAIZE55/K2/MAIZE6.pos.gz
hilo$ rm local_ancestry/results/countsACGT/HILO_MAIZE55/K2/MAIZE39.counts.gz
hilo$ rm local_ancestry/results/countsACGT/HILO_MAIZE55/K2/MAIZE39.pos.gz
hilo$ for i in $(cat samples/HILO_MAIZE55_PARV50_ids.list); do ls local_ancestry/results/countsMajMin/HILO_MAIZE55_PARV50/K3/$i.counts.txt; done
MAIZE30 - no ACGT
MAIZE38 - # pos doesn't match # counts
MAIZE42 - no ACGT
MAIZE49 - no ACGT
hilo$ rm local_ancestry/results/countsACGT/HILO_MAIZE55_PARV50/K3/MAIZE38.counts.gz
hilo$ rm local_ancestry/results/countsACGT/HILO_MAIZE55_PARV50/K3/MAIZE38.pos.gz
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        84      boot_ancestry_hmm_K2
        84      boot_ancestry_hmm_K3
        12      count_ACGT
        12      count_maj_min
        1       make_allo_counts_K2
        1       make_allo_counts_K3
        56      make_input_hmm
        5       pop_allele_freqs
        1       some
        84      summarise_posterior_K2
        340 # RUNNING 5.5.2021.
        ecalfee@c6-87:~/hilo$ squeue -u ecalfee
                 JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
              33951205     high2     bash  ecalfee  R        6:32      1 1   2G     c6-87
              33786709      med2 pop_alle  ecalfee  S    21:23:03      1 13  48G    c6-89
        ecalfee@c6-87:~/hilo$ scancel 33786709 # cancelled this stalled job. Added memory, threads and time to pop_allele_freqs
        # all but allopatric_maize and parv have run (out of time and out of memory always) for wavelet pop allele freqs
        ecalfee@farm:~$ screen -ls # my screen must have shut down, I'll make another one
        No Sockets found in /run/screen/S-ecalfee.
        # oops I forgot to add make_allo_counts_ancestry_hmm_K3.R to github. done now.
        hilo$ snakemake some --quiet --profile slurm                           Job counts:                                                                                                 count   jobs
        84      boot_ancestry_hmm_K3
        1       make_allo_counts_K3
        28      make_input_hmm
        3       pop_allele_freqs
        1       some
        5       summarise_posterior_K2
        122 # still locked :/ probably because my screen just ended unexpectedly with the stalled job?
        snakemake some --quiet --profile slurm
        Error: Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory:
        /home/ecalfee/hilo
        If you are sure that no other instances of snakemake are running on this directory, the remaining lock was likely caused by a kill signal or a power loss. It can be removed with the --unlock argument.
        hilo$ snakemake some --quiet --unlock --profile slurm # unlocked. now I can rerun my pipeline:
        hilo$ snakemake some --quiet --profile slurm
        Job counts:
        count   jobs
        84      boot_ancestry_hmm_K3
        1       make_allo_counts_K3
        28      make_input_hmm
        3       pop_allele_freqs
        1       some
        5       summarise_posterior_K2
        122 # RUNNING 5.10.2021
        RuleException in line 360 of /home/ecalfee/hilo/local_ancestry/Snakefile:
        NameError: The name 'parms' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}
        (hilo-env) ecalfee@farm:~/hilo$ squeue -u ecalfee
                 JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
              33952720      med2 pop_alle  ecalfee  R     1:50:13      1 5   16G    c6-72
              33952724      med2 pop_alle  ecalfee  R     1:50:13      1 5   16G    c6-71
              33952727      med2 pop_alle  ecalfee  R     1:50:13      1 5   16G    c6-71
        # cancelling these jobs so I can rerun. fixed typo parms -> params in rule boot_ancestry_hmm_K3
        hilo$ snakemake some --quiet --profile slurm
        Job counts:
                count   jobs
                84      boot_ancestry_hmm_K3
                3       pop_allele_freqs
                1       some
                88 # RUNNING 5.10.2021 11:30am
  # files sent to Jeff (still waiting on parv and allopatric maize freq files):
  hilo$ scp wavelets/results/alleleFreqs/HILO_MAIZE55/K2/*.mafs.gz ecalfee@169.237.66.199:~/Documents/gitErin/hilo/wavelets/results/alleleFreqs/HILO_MAIZE55/K2/.
  hilo$ scp wavelets/results/alleleFreqs/HILO_MAIZE55_PARV50/K3/*.mafs.gz ecalfee@169.237.66.199:~/Documents/gitErin/hilo/wavelets/results/alleleFreqs/HILO_MAIZE55_PARV50/K3/.
  # SNP/site information
  hilo$ scp local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/whole_genome* ecalfee@169.237.66.199:~/Documents/gitErin/hilo/local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/.
  hilo$ scp local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/whole_genome* ecalfee@169.237.66.199:~/Documents/gitErin/hilo/local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/.
  # he also will need population meta data
  hilo$ scp samples/HILO_MAIZE55_PARV50_meta.RData ecalfee@169.237.66.199:~/Documents/gitErin/hilo/samples/.
  hilo$ scp samples/population_metadata.csv ecalfee@169.237.66.199:~/Documents/gitErin/hilo/samples/.
  # and the maize recombination map:
  hilo$ scp linkage_map/results/ogut_2015_rmap_v2_to_v4_EXTENDED.txt ecalfee@169.237.66.199:~/Documents/gitErin/hilo/linkage_map/results/.
  # and maize chromosome lengths:
  hilo$ scp data/refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths ecalfee@169.237.66.199:~/Documents/gitErin/hilo/data/refMaize/.

  # ok so these are taking forever and only on chr 1 for parv and allopatric_maize allele freqs...
  # I will cancel and make them run 1 region at a time instead.
  hilo$ squeue -u ecalfee
           JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
        33998743      med2 pop_alle  ecalfee PD        0:00      1 1   80G    (Priority)
        33999744     high2     bash  ecalfee  R        0:00      1 1   2G     c6-72
        33956477      med2 pop_alle  ecalfee  R  1-00:32:26      1 13  48G    c6-95
        33956485      med2 pop_alle  ecalfee  R  1-00:32:26      1 13  48G    c6-97
  ecalfee@c6-72:~/hilo$ scancel 33998743 33956485 33956477
  hilo$ snakemake -n some --quiet --profile slurm
  Job counts:
          count   jobs
          84      boot_ancestry_hmm_K3
          3       concatenate_group_mafs
          1278    group_allele_freqs_by_region
          1       some
          1366 # HMM..check on why the ancestry_hmm with K=3 isn't running within the time limit. it's trying to do the bootstrapping, right?
          # as a first step I'm just adding time for it to run..no obvious errors in how we've set up the demographic model or ancestry_hmm or order of alleles in input file.
          hilo$ snakemake -n some --quiet --profile slurm
          # trying to run snakemake on srun high2 (144 hrs, 2Gb) at 5.22.2021 2:13pm
          hilo$ snakemake some --quiet --profile slurm
          # tried to start 5.11.2021 at 9:30pm -- check on this soon.
          (hilo-env) ecalfee@c6-75:~/hilo$ snakemake some --quiet --profile slurm
          Killed
          # hmm...I'm not sure why this is getting killed. I will start a new high2 instance on screen with more memory (8Gb)
          # and make the DAG smaller by commenting out the wavelets alleFreq stuff for now.
          srun: error: c6-75: task 0: Out Of Memory
          ecalfee@farm:~/hilo$ srun -p high2 --mem 16G -t 10-00:00:00 --pty bash
          srun: job 34020939 queued and waiting for resources
          srun: job 34020939 has been allocated resources
          (hilo-env) ecalfee@c6-75:~/hilo$ snakemake some --quiet --profile slurm
          Job counts:
              count   jobs
              84      boot_ancestry_hmm_K3
              3       concatenate_group_mafs
              1278    group_allele_freqs_by_region
              1       some
              1366 # RUNNING 5.12.2021 12:45pm. check on it in a bit and see if it's starting any boot_ancestry_hmm_K3 jobs
5.16.2021 status:
ecalfee@farm:~$ squeue -u ecalfee
         JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
      34020939     high2     bash  ecalfee  R  4-09:42:04      1 5   16G    c6-75
      34053837      med2 group_al  ecalfee  S       31:56      1 4   16G    c6-68
      34021626      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021627      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021628      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021629      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021630      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021631      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021636      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021643      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021650      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021654      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021655      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34021656      med2 group_al  ecalfee  S     2:57:29      1 4   16G    c6-67
      34045146      med2 group_al  ecalfee  S     7:31:49      1 4   16G    c6-92
      34045152      med2 group_al  ecalfee  S     1:12:38      1 4   16G    c6-92
      # so I'm cancelling all the med2 runs and seeing what still needs to run.
      hilo$ squeue -u ecalfee | grep 'group_al' | awk '{print $1}' | xargs -n 1 scancel
      # addign more time to srun:
      hilo$ srun -p high2 --mem 16G -t 10-00:00:00 --pty bash
      srun: job 34394319 queued and waiting for resources
      srun: job 34394319 has been allocated resources
      hilo$ snakemake some --quiet --profile slurm
      Job counts:
           count   jobs
           84      boot_ancestry_hmm_K3
           1       some
           85 # RUNNING 5.16.2021

           farm:~$ squeue -u ecalfee
         JOBID PARTITION     NAME     USER ST        TIME  NODES CPU MIN_ME NODELIST(REASON)
      34394319     high2     bash  ecalfee  R  1-12:17:53      1 5   16G    c6-66
      34395214      med2 boot_anc  ecalfee  S    17:26:55      1 3   8G     c6-66
      34395215      med2 boot_anc  ecalfee  S    21:26:12      1 3   8G     c6-66
      34395197      med2 boot_anc  ecalfee  S    17:06:05      1 3   8G     c6-66
      34395198      med2 boot_anc  ecalfee  S    17:24:13      1 3   8G     c6-66
      34395199      med2 boot_anc  ecalfee  S    17:36:40      1 3   8G     c6-66
      34395134      med2 boot_anc  ecalfee  S    10:52:06      1 3   8G     c6-66
      34395135      med2 boot_anc  ecalfee  S    10:52:06      1 3   8G     c6-66
      34395136      med2 boot_anc  ecalfee  S    10:52:06      1 3   8G     c6-66
      34395137      med2 boot_anc  ecalfee  S    10:52:06      1 3   8G     c6-66
      34395140      med2 boot_anc  ecalfee  S    14:41:08      1 3   8G     c6-93
      34395141      med2 boot_anc  ecalfee  S    14:41:08      1 3   8G     c6-93
      # looks like 24 hrs isn't enough time for K=3. Do 48 hrs. Also I don't need to be running a bootstrap on the Ne = 1000, or Ne = 100,000 runs. Fix that to reduce the bottleneck there.
      # would be good to have it continue on to the ancestry summary too.
      # and to get the alleleFreqs
      # SOOO...Cancelled the snakemake run above and also cancelling stalled jobs:
      hilo$ squeue -u ecalfee | grep 'boot_anc' | awk '{print $1}' | xargs -n 1 scancel
      hilo$ snakemake some --quiet --profile slurm
      Job counts:
              count   jobs
              26      boot_ancestry_hmm_K3
              3       concatenate_group_mafs
              25      group_allele_freqs_by_region
              84      run_ancestry_hmm_K3
              1       some
              84      summarise_posterior_K2
              112     summarise_posterior_K3
              335 # RUNNING 5.18.2021. COMPLETED 5.21.2021.

# oops! accidentally switched maize and mexicana for summarising ancetry from K=2 only.
# fixed script for summarise_posterior_K2 and now deleting output so that these files rerun:
hilo$ rm local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne*/anc/*/pop*.alpha.ind
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        1890    calc_fst_all_chr
        1890    calc_fst_idx
        2       combine_pop_ancestry_data_K2
        2       combine_pop_ancestry_data_K3
        1890    estimate_2D_sfs
        140     estimate_saf_pop
        112     get_admix_times_K2
        112     get_admix_times_K3
        140     get_homozygous_ancestry_bams
        56      get_homozygous_ancestry_tracts_K2
        84      get_homozygous_ancestry_tracts_K3
        2       get_tracts_from_sites
        140     list_homozygous_ancestry_bams
        1       plot_admix_times_K2
        2       plot_within_ancestry_fst
        1       print_fst_one_file_K2
        1       print_fst_one_file_K3
        84      run_ancestry_hmm_K2
        1       some
        84      summarise_posterior_K2 # good this part is rerunning :)
        6634 # RUNNING 5.24.2021

# remove old formatted files for ancestry summaries:
hilo$ rm local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne*_*Boot/anc/pop*
# now these are divided into anc/maize/ and anc/mexicana/ subdirectories
# some rules failed, e.g. 34740199 and nothing currently running on squeue so cancelling my snakemake job

hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        604     calc_fst_all_chr
        600     calc_fst_idx # added time
        594     estimate_2D_sfs # also added memory
        24      estimate_saf_pop # I added memory when for re-runs
        13      get_admix_times_K3 # added time
        12      list_homozygous_ancestry_bams # added time
        1       plot_admix_times_K2 # added missing parenthesis
        2       plot_within_ancestry_fst
        1       print_fst_one_file_K2
        1       print_fst_one_file_K3
        1       some
        5       summarise_posterior_K2 # added time (based on hilo$ grep 'err' snake_logs/*summarise_posterior_K2*)
        1858

hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        604     calc_fst_all_chr
        600     calc_fst_idx
        594     estimate_2D_sfs
        24      estimate_saf_pop
        13      get_admix_times_K3
        12      list_homozygous_ancestry_bams
        1       plot_admix_times_K2
        2       plot_within_ancestry_fst
        1       print_fst_one_file_K2
        1       print_fst_one_file_K3
        1       some
        5       summarise_posterior_K2
        1858 # RUNNING 5.27.2021 11:47am


# transferred 5.27.2021 from farm
# all files for Jeff Groh finished. now re-transferring all files for Jeff Groh (hilo/ on Nancy's was moved to SHARED. BY ME .. LOL)
hilo$ scp wavelets/results/alleleFreqs/HILO_MAIZE55/K2/*.mafs.gz ecalfee@169.237.66.199:~/Documents/gitErin/hilo/wavelets/results/alleleFreqs/HILO_MAIZE55/K2/.
hilo$ scp wavelets/results/alleleFreqs/HILO_MAIZE55_PARV50/K3/*.mafs.gz ecalfee@169.237.66.199:~/Documents/gitErin/hilo/wavelets/results/alleleFreqs/HILO_MAIZE55_PARV50/K3/.
# SNP/site information
hilo$ scp local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/whole_genome* ecalfee@169.237.66.199:~/Documents/gitErin/hilo/local_ancestry/results/thinnedSNPs/HILO_MAIZE55_PARV50/K3/.
hilo$ scp local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/whole_genome* ecalfee@169.237.66.199:~/Documents/gitErin/hilo/local_ancestry/results/thinnedSNPs/HILO_MAIZE55/K2/.
# he also will need population meta data
hilo$ scp samples/HILO_MAIZE55_PARV50_meta.RData ecalfee@169.237.66.199:~/Documents/gitErin/hilo/samples/.
# metadata by population, RIMME022 = pop22
hilo$ scp samples/population_metadata.csv ecalfee@169.237.66.199:~/Documents/gitErin/hilo/samples/.
# metadata by individual (note: there are 3 'allopatric' mexicana reference populations, Amecameca is the one with no evidence of admixtures. the other 2 likely have parviglumis admixture and all 3 have the potential for gene flow .. not mexicana is apparently truly allopatric)
hilo$ scp samples/HILO_MAIZE55_PARV50_meta.txt ecalfee@169.237.66.199:/Users/Shared/wavelets/hilo/samples/.
# and the maize recombination map (use approx in R to get cM <-> bp for new positions):
hilo$ scp linkage_map/results/ogut_2015_rmap_v2_to_v4_EXTENDED.txt ecalfee@169.237.66.199:~/Documents/gitErin/hilo/linkage_map/results/.
# and maize chromosome lengths:
hilo$ scp data/refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths ecalfee@169.237.66.199:~/Documents/gitErin/hilo/data/refMaize/.
# posterior probabilities for local ancestry from ancestry_hmm
# for K=2 mixing populations: maize, mexicana
hilo$ scp local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/posterior/* ecalfee@169.237.66.199:/Users/Shared/wavelets/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/posterior/.
# for K = 3 mixing populations: mexicana, parv, maize
hilo$ scp local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne10000_yesBoot/posterior/* ecalfee@169.237.66.199:/Users/Shared/wavelets/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne10000_yesBoot/posterior/.

# waiting on jobs to get priority to start...
ecalfee@c6-95:~$ squeue -u ecalfee --start
             JOBID PARTITION     NAME     USER ST          START_TIME  NODES SCHEDNODES           NODELIST(REASON)
          34789079      med2 estimate  ecalfee PD 2021-05-29T22:33:45      1 (null)               (Priority)
          34789081      med2 estimate  ecalfee PD 2021-05-29T22:33:45      1 (null)               (Priority)
...

# copying some completed files over for plotting:
# global ancestry
Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/global_ancestry/results/NGSAdmix/HILO_MAIZE55_PARV50/* global_ancestry/results/NGSAdmix/HILO_MAIZE55_PARV50/.
# timing of admixture estimates K2
Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/*.times local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/.
# timing of admixture estimates K3
hilo Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne10000_yesBoot/*.times local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne10000_yesBoot/.
# metadata and ancestry proportions (to plot admix times) K2 and K3
hilo Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/anc/*.pop.meta.RData local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/anc/.
hilo Erin$ scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne10000_yesBoot/anc/*.pop.meta.RData local_ancestry/results/ancestry_hmm/HILO_MAIZE55_PARV50/K3/Ne10000_yesBoot/anc/.
# Ne 1000, 100,000 results for timing (non-bootstrap) and ind.alpha estimates and mexicana pop freq estimates

check on these jobs? do they need to have fewer threads to make it memory-wise?
34867890      med2 estimate  ecalfee  R        1:24      1 32  128G   c6-86
34867609      med2 estimate  ecalfee  R        5:54      1 32  128G   c6-66
34867469      med2 estimate  ecalfee  R       15:19      1 32  128G   c6-56
# reduced to 2 threads and added memory also.
# delete previous times summaries for K=2 so that these rerun with 'mexicana' label for 2nd column
hilo$ ls local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne*_*Boot/*.times
hilo$ rm local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne*Boot/*.times
# moved 'diversity' results to it's own rule (out of rule some so I can run other things 1st)
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        112     get_admix_times_K2
        1       plot_admix_times_K2
        1       plot_admix_times_K3
        1       some
        115 # RUNNING 6.1.2021. oops typose in plot_admix_times. fixed.
        # also adding lm fits and mean ancestry FDR threshold calcs to rule some.
        # Fixed thesholds -> threshold in FDR.R script too.
        hilo$ snakemake -n some --quiet --profile slurm
        Job counts:
                count   jobs
                4       calc_K_matrix_zea
                4       calc_fdr_mean_anc
                48      calc_overlap
                4       fit_lm_elev
                10      make_bed_anc_outliers
                4       make_bed_slope_elev
                48      merge_bed_outliers
                1       plot_admix_times_K2
                1       plot_admix_times_K3
                4       simulate_MVN
                1       some
                2       summarise_overlap_flowering_time
                131 # RUNNING 6.1.2021 1pm
# removing previous version of ancestry saved in .RData to make a new version where it's a list
hilo$ rm local_ancestry/results/ancestry_hmm/HILO_MAIZE55*/K*/Ne10000_yesBoot/anc/*.pops.anc.RData
#### re-ran something at ~3pm.
# some jobs didn't run..error in anc[["mexicana"]] for fit_lm_elev. fixed typo in . not sure what's wrong with plot_admix_times_K2.R BUT
# I had accidentally loaded R module R/3.6.3
# which may be related to errors below:
hilo$ less snake_logs/plot_admix_times_K2.Ne=10000,PREFIX=HILO_MAIZE55,YESNO=yes.34991431.err
R script job uses conda environment but R_LIBS environment variable is set. This is likely not intended, as R_LIBS can interfere with R packages deployed via conda. Consider running `unset R_LIBS` or remove it entirely before executing Snakemake.
Error: Argument 1 must have names
# not sure what that error is, but unloaded R module and running snakemake pipeline again:
# on local computer, plotting times seems fine for K=2
scp -P 2022 ecalfee@farm.cse.ucdavis.edu:~/hilo/local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/*.times local_ancestry/results/ancestry_hmm/HILO_MAIZE55/K2/Ne10000_yesBoot/.
hilo$ snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        24      calc_overlap
        4       fit_lm_elev
        4       make_bed_slope_elev
        24      merge_bed_outliers
        1       plot_admix_times_K2
        1       some
        2       summarise_overlap_flowering_time # only thing that didn't run. problem was with input file names within R script.
        60 # RUNNING 6.1.2021 11:10pm
hilo$ snakemake some --quiet --profile slurm # RUNNING 6.2.2021 8:45am. included plots for mean anc
2       summarise_overlap_flowering_time # oops 'K' is missing

hilo$ snakemake some --quiet --profile slurm # RUNNING 6.2.2021 9:06am.
Job counts:
        count   jobs
        2       combine_genome_scan_plots
        4       plot_slope_elev
        1       some
        2       summarise_overlap_flowering_time
        2       test_key_genes # not sure why this failed. WORK ON THIS FILE!
        11
hilo$ snakemake some --quiet --profile slurm # RUNNING 6.2.2021
# hmmm... some K ancestries for bootstraps are ambiguous, e.g. ancestry_by_r/results/bootstrap_1cM/HILO_MAIZE55_PARV50/cd5_5/K3/boot60.qopt
# I will make these produce NAs. can't be many bootstraps with issues since most ran without errors.
snakemake some --quiet --profile slurm
Job counts:
        count   jobs
        2       combine_genome_scan_plots # id error later?
        7       label_k_anc
        1       plot_NGSAdmix_K3 # fixed typo p_symp_maize_maize
        2       run_bootstrap_NGSAdmix # not sure why failed. adding time and memory.
        1       some
        2       summarise_bootstrap_anc_by_r
        2       test_key_genes # fix later?
        17 # RUNNING 6.2.2021 10:49pm
        # several did not run:
        hilo$ snakemake -n some --quiet --profile slurm
        Job counts:
        count   jobs
        2       combine_genome_scan_plots # don't see any error logs
        5       label_k_anc # fixed error with d_out[a,] needing to be d_out[,a] for ambiguous ancestry assignment bootstraps
        1       some
        2       summarise_bootstrap_anc_by_r
        2       test_key_genes # made some edits to incorporate new format for K=3 ancestries
        12 # cancelled b/c jobs pending and I wanted to add jobs.
# ok so the ancestry summaries coming out of K=2 are definitely flipped mex-maize..affecting genomewide ancestry scan plots
# I think what happened is that I didn't re-run (?) after fixing the weights to be (0,.5,1) for mexicana (and 1, .5, 0 for maize)
# because there are no .alpha.ind files. so I've added those to the inputs of rule 'some' to trigger a rerun.
# the homozygous ancestry tracts appear to be correctly identified (maize has more maize-maize and mex has more mex-mex), see:
hilo$ wc -l local_ancestry/results/ancestry_hmm/HILO_MAIZE55*/K*/Ne10000_yesBoot/HOMOZYG/*/tracts/HILO18.bed

hilo$ snakemake -n some --quiet --profile slurm #6.3.2021
# oops forgot to write down a run that was run...
# some failed though:
35091109 # summarise_bootstrap_anc_by_r
35091123 # test_key_genes
35091100 # plot_sensitivity_to_Ne_K3
# fixed inputs to some to trigger reruns for summarise_posterior_K2 (Ne10000_yesBoot)
hilo$ snakemake -n some --quiet --profile slurm
Job counts:
        count   jobs
        2       combine_genome_scan_plots
        1       get_GL_mhl1_inv
        1       plot_mhl1_inv_PCAngsd
        1       plot_sensitivity_to_Ne_K3
        1       run_PCAngsd_mhl1_inv
        1       some
        2       summarise_bootstrap_anc_by_r
        2       test_key_genes
        11

# TO DO:
# mhl1 inversion analyses/plots
# QQ plots
# test robustness of ancestry_hmm to choice of Ne
# Language : modeled ancestry variance-covariance marginally, marginal K matrix
# FINISH FIXING UP 'TEST_KEY_GENES' rule -- look at all input files, fix prefixes to be maize_anc and lm_elev

# to get all diversity stuff to run I need to ID ancestry outliers: "ZAnc/results/{PREFIX}/K{K}/Ne10000_yesBoot/{POP2}.{n}pop.outliers.regions"

# what do I need to rerun for other results?
- plot_bootstrap_NGSadmix_by_r. results are ready to plot. f4s don't depend on K, so they'll be the same (just harder to interpret for sympatric mexicana)


# ASK IF THE EQUATION IN SORAGGI SHOULD HAVE A NEGATIVE IN THE NUMERATOR
# PREP FOR PRESENTATION:
# (1) Get list of flowering time GWAS hits and compare overlap. If time, at end, control for recombination rate using BEDTools
# (2) Plot freq in maize vs. freq in mexicana with domestication loci highlighted a different color
