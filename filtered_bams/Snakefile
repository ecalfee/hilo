from collections import defaultdict
import os

workdir: path_hilo + "filtered_bams/"

wildcard_constraints:
    ID = "[A-Za-z0-9]+",
    LANE = "[April2020_?|March2018|Jan2019_?|TEST|TEST2]"

#ref = "/home/ecalfee/hilo/data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa"
ref = path_hilo + "data/refMaize/Zea_mays.B73_RefGen_v4.dna.toplevel.fa"
fai = ref + ".fai"

#prefix = "April2020"
prefix = "TEST2"
#prefix = "Combined"

# list of sample IDs
with open("../data/HILO_raw_reads/" + prefix + "_IDs.list") as f:
    ids = f.read().splitlines()
# list of lanes for sample IDs above
with open("../data/HILO_raw_reads/" + prefix + "_lanes.list") as f:
    lanes = f.read().splitlines()

# dictionary mapping lanes to library names
libraries_dict = {"April2020_1": "April2020",
		"April2020_2": "April2020",
		"March2018": "March2018",
		"Jan2019_6": "March2018",
		"Jan2019_7": "March2018",
		"Jan2019_8": "March2018",
        "Maize55": "Maize55",
		"TEST": "TEST_LIBRARY",
        "TEST2": "TEST2_LIBRARY"
}
# make dictionary with a list of lanes for each id
merge_dict = defaultdict(list)
for i, id in enumerate(ids):
    merge_dict[id].append(lanes[i])

# reads 1 and 2 in paired sets, e.g. _1.fq.gz
reads = ["1", "2"]

rule all:
    input:
        expand(["results/merged/{ID}.sort.dedup.bam",
                "results/merged/{ID}.sort.dedup.bam.bai"], zip, ID=list(merge_dict.keys())),
        #expand(["results/{LANE}/{ID}.sort.dedup.bam",
        #        "results/{LANE}/{ID}.sort.dedup.bam.bai"], zip, LANE=lanes, ID=ids),
        #"metrics/fastQC/multiqc/multiqc_report.html",
        #"metrics/fastQC_trimmed/multiqc/multiqc_report.html",
        "metrics/picard/multiqc/multiqc_report.html",
        "metrics/flagstat/multiqc/multiqc_report.html"

    params:
        p = "med2"
    resources:
        time_min = 5,
        mem = 2

# alternative to all for running part of the pipeline (e.g. testing or pipeline incomplete)
rule some:
    input:
        "metrics/fastQC/multiqc/multiqc_report.html",
        "metrics/fastQC_trimmed/multiqc/multiqc_report.html",
        expand("results/{LANE}/{ID}.sort.bam", zip, LANE=lanes, ID=ids)
    params:
        p = "med2"
    resources:
        time_min = 5,
        mem = 2

rule index_ref:
    input:
        ref
    output:
        fai
    params:
        p = "med2"
    conda:
        "../envs/environment.yaml"
    shell:
        "bwa index {input}"

# trim out adapter contamination
rule trimmomatic:
    input:
        r1 = path_hilo + "data/HILO_raw_reads/{LANE}/{ID}_1.fq.gz", # fastq1
        r2 = path_hilo + "data/HILO_raw_reads/{LANE}/{ID}_2.fq.gz" # fastq2
    output:
        r1 = temp(path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_1.fq.gz"),
        r2 = temp(path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_2.fq.gz"),
        # reads where trimming entirely removed the mate
        r1_unpaired = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_unpaired_1.fq.gz",
        r2_unpaired = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_unpaired_2.fq.gz"
    params:
        # list of trimmers (see manual)
        trimmer=["ILLUMINACLIP:/share/apps/Trimmomatic-0.36/adapters/NexteraPE-PE.fa:2:30:10:2:TRUE", "MINLEN:30"],
        # optional parameters
        extra="",
        compression_level="-9",
        p = "bigmemm"
    shadow:
        "minimal"
    resources:
        time_min = lambda wildcards, attempt: attempt * 24 * 60,
        mem = lambda wildcards, attempt: attempt * 24
    threads:
        16
    wrapper:
        "0.59.1/bio/trimmomatic/pe"

rule bwa_map:
    input:
        ref = ref,
        fai = fai,
        fq1 = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_1.fq.gz", # fastq1
        fq2 = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_2.fq.gz" # fastq2
    output:
        temp("results/{LANE}/{ID}.bam")
    params:
        lib = lambda wildcards: libraries_dict[wildcards.LANE],
        p = "bigmemm"
    conda:
        "../envs/environment.yaml"
    shadow:
        "minimal"
    resources:
        time_min = lambda wildcards, attempt: attempt * 24 * 60,
        mem = lambda wildcards, attempt: attempt * 24
    threads:
        16
    shell:
        "(bwa mem -R '@RG\\tID:{wildcards.LANE}\\tSM:{wildcards.ID}\\tPL:ILLUMINA\\tLB:{params.lib}\\tPU:{wildcards.ID}.{wildcards.LANE}' "
        "-t {threads} -v 3 {input.ref} {input.fq1} {input.fq2} | "
        "samtools view -Shu -) > {output}"


rule samtools_sort:
    input:
        "results/{LANE}/{ID}.bam"
    output:
        temp("results/{LANE}/{ID}.sort.bam")
    params:
        p = "bigmemm"
    conda:
        "../envs/environment.yaml"
    shadow:
        "minimal"
    threads:
        4
    resources:
        time_min = lambda wildcards, attempt: attempt * 12 * 60,
        mem = 32
    shell:
        "mkdir -p tmp_sorting_reads/{wildcards.ID}_{wildcards.LANE} &&"
        "samtools sort -m 6G -@ {threads} "
        "-T tmp_sorting_reads/{wildcards.ID}_{wildcards.LANE} "
        "-O bam {input} > {output};"
        "rm -r tmp_sorting_reads/{wildcards.ID}_{wildcards.LANE}"

rule mark_duplicates:
    input:
        ref = ref,
        fai = fai,
        bam = "results/{LANE}/{ID}.sort.bam"
    output:
        bam = "results/{LANE}/{ID}.sort.dedup.bam",
        metrics = "metrics/picard/{LANE}/{ID}.metrics.txt"
    params:
        p = "med2"
    conda:
        "../envs/environment.yaml"
    shadow:
        "minimal"
    # shadow minimal should make the temporary directory on the --shadow-prefix location, i.e. set to local node storage
    resources:
        time_min = lambda wildcards, attempt: attempt * 8 * 60,
        mem = 8
    # java 1.8 and picard-tools 2.7.1 loaded from cluster
    # note: I only filter mapQ > 1 at this step, though in downstream analyses often use a more stringent threshold mapQ > 30
    shell:
        "module load java &&"
        "module load picardtools/2.7.1 &&"
        "mkdir -p tmp_dedup_reads/{wildcards.ID}_{wildcards.LANE} &&"
        "java -Xmx6g -jar ${{PICARD}}/picard.jar MarkDuplicates "
        "INPUT= {input.bam} OUTPUT=/dev/stdout QUIET=true "
        "REMOVE_DUPLICATES=true "
        "TMP_DIR=tmp_dedup_reads/{wildcards.ID}_{wildcards.LANE} "
        "METRICS_FILE={output.metrics} | "
        "samtools view -bS -q 1 - > {output.bam} &&"
        "rm -r tmp_dedup_reads/{wildcards.ID}_{wildcards.LANE}"
        # remove temporary directory at the end

rule samtools_index:
    input:
        "results/{LANE}/{ID}.sort.dedup.bam"
    output:
        "results/{LANE}/{ID}.sort.dedup.bam.bai"
    params:
        p = "med2"
    conda:
        "../envs/environment.yaml"
    shell:
        "samtools index {input}"

# merge multiple bams for the same HILO sample (only a subset of samples were run on multiple lanes)
def bams2merge(wildcards):
    return expand("results/{LANE}/{ID}.sort.dedup.bam", LANE = merge_dict[wildcards.ID], ID = wildcards.ID)
def bais2merge(wildcards):
    return expand("results/{LANE}/{ID}.sort.dedup.bam.bai", LANE = merge_dict[wildcards.ID], ID = wildcards.ID)
def count_bams(wildcards):
    return len(merge_dict[wildcards.ID])

rule merge_bams:
#I need equivalent of r's paste in python to create input list of file names from ID
    input:
        # zip in expand is like :::+ in gnu parallel (doesn't create all combinations of inputs)
        bams = bams2merge,
        bais = bais2merge
    output:
        bam = "results/merged/{ID}.sort.dedup.bam",
        bai = "results/merged/{ID}.sort.dedup.bam.bai"
    params:
        p = "med2",
        n_input_bams = count_bams,
        path = path_hilo + "filtered_bams/"
    conda:
        "../envs/environment.yaml"
    shell:
        """
        if [ {params.n_input_bams} -gt 1 ]; then
                echo 'merging files n={params.n_input_bams}'
                samtools merge {output.bam} {input.bams}
                sleep 5s
                samtools index {output.bam}
        else
                echo 'no merge required -- creating symlink to file'
                ln -s {params.path}{input.bams} {output.bam}
                sleep 5s
                ln -s {params.path}{input.bais} {output.bai}
        fi
        """

# run quality checks and metrics
rule fastQC:
    input:
        "../data/HILO_raw_reads/{LANE}/{ID}_{READ}.fq.gz"
    output:
        "metrics/fastQC/{LANE}/{ID}_{READ}_fastqc.html"
    params:
        p = "med2"
    threads:
        4
    conda:
        "../envs/environment.yaml"
    shell:
        "fastqc -o metrics/fastQC/{wildcards.LANE} -t {threads} {input}"

rule fastQC_trimmed:
    input:
        "/home/ecalfee/hilo/data/HILO_trimmed_reads/{LANE}/{ID}_{READ}.fq.gz"
    output:
        "metrics/fastQC_trimmed/{LANE}/{ID}_{READ}_fastqc.html"
    params:
        p = "med2"
    threads:
        4
    conda:
        "../envs/environment.yaml"
    shell:
        "fastqc -o metrics/fastQC_trimmed/{wildcards.LANE} -t {threads} {input}"


rule multiQC_fastQC:
    input:
        expand(["metrics/fastQC/{LANE}/{ID}_1_fastqc.html",
                "metrics/fastQC/{LANE}/{ID}_2_fastqc.html"], zip, LANE=lanes, ID=ids)
    output:
        "metrics/fastQC/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 2*60,
        mem = 8
    conda:
        "../envs/environment.yaml"
    shell:
        "multiqc -f metrics/fastQC -o metrics/fastQC/multiqc --dirs --ignore TEST/"

rule multiQC_fastQC_trimmed:
    input:
        expand(["metrics/fastQC_trimmed/{LANE}/{ID}_1_fastqc.html",
                "metrics/fastQC_trimmed/{LANE}/{ID}_2_fastqc.html"], zip, LANE=lanes, ID=ids)
    output:
        "metrics/fastQC_trimmed/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 2*60,
        mem = 8
    conda:
        "../envs/environment.yaml"
    shell:
        "multiqc -f metrics/fastQC_trimmed -o metrics/fastQC_trimmed/multiqc --dirs --ignore TEST/"

rule samtools_flagstat:
    input:
        "results/{LANE}/{ID}.sort.dedup.bam"
    output:
        "metrics/flagstat/{LANE}/{ID}.flagstat"
    params:
        p = "med2"
    threads:
        4
    resources:
        time_min = lambda wildcards, attempt: attempt * 6 * 60
    conda:
        "../envs/environment.yaml"
    # only count reads with mapping quality >= 30
    shell:
        "samtools view -q 30 -b {input} | samtools flagstat -@ {threads} -  > {output}"

rule multiQC_flagstat:
    input:
        expand("metrics/flagstat/{LANE}/{ID}.flagstat", zip, LANE=lanes, ID=ids)
    output:
        "metrics/flagstat/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 60,
        mem = 8
    conda:
        "../envs/environment.yaml"
    shell:
        "multiqc -f metrics/flagstat -o metrics/flagstat/multiqc --dirs --ignore TEST/"

rule multiQC_picard:
    input:
        expand("metrics/picard/{LANE}/{ID}.metrics.txt", zip, LANE=lanes, ID=ids)
    output:
        "metrics/picard/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 60,
        mem = 8
    conda:
        "../envs/environment.yaml"
    shell:
        "multiqc -f metrics/picard -o metrics/picard/multiqc --dirs --ignore TEST/"
