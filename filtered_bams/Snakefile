from collections import defaultdict
import os

#workdir: path_hilo + "filtered_bams/" # some rules need an absolute path to create a 'shadow' copy of files outside of current working directory


# list of lanes for sample IDs
with open("data/HILO_raw_reads/" + prefix_bams + "_lanes.list") as f:
    lanes = f.read().splitlines()

# dictionary mapping lanes to library names
libraries_dict = {"April2020_1": "April2020",
		"April2020_2": "April2020",
		"March2018": "March2018",
		"Jan2019_6": "March2018",
		"Jan2019_7": "March2018",
		"Jan2019_8": "March2018",
        "Maize55": "Maize55",
		"TEST": "TEST_LIBRARY",
        "TEST2": "TEST2_LIBRARY"
}
# make dictionary with a list of lanes for each id
merge_dict = defaultdict(list)
for i, id in enumerate(ids):
    merge_dict[id].append(lanes[i])

# reads 1 and 2 in paired sets, e.g. _1.fq.gz
reads = ["1", "2"]

rule index_ref:
    input:
        ref
    output:
        fai
    params:
        p = "med2"
    conda:
        "envs/environment.yaml"
    shell:
        "bwa index {input}"

## trimmomatic: trims out adapter contamination
rule trimmomatic:
    input:
        r1 = path_hilo + "data/HILO_raw_reads/{LANE}/{ID}_1.fq.gz", # fastq1
        r2 = path_hilo + "data/HILO_raw_reads/{LANE}/{ID}_2.fq.gz" # fastq2
    output:
        r1 = temp(path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_1.fq.gz"),
        r2 = temp(path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_2.fq.gz"),
        # reads where trimming entirely removed the mate
        r1_unpaired = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_unpaired_1.fq.gz",
        r2_unpaired = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_unpaired_2.fq.gz"
    params:
        # list of trimmers (see manual)
        trimmer=["ILLUMINACLIP:/share/apps/Trimmomatic-0.36/adapters/NexteraPE-PE.fa:2:30:10:2:TRUE", "MINLEN:30"],
        # optional parameters
        extra="",
        compression_level="-9",
        p = "bigmemm"
    shadow:
        "minimal"
    resources:
        time_min = lambda wildcards, attempt: attempt * 24 * 60,
        mem = lambda wildcards, attempt: attempt * 24
    threads:
        16
    wrapper:
        "0.59.1/bio/trimmomatic/pe"

## bwa_map: maps (trimmed) reads to reference genome
rule bwa_map:
    input:
        ref = ref,
        fai = fai,
        fq1 = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_1.fq.gz", # fastq1
        fq2 = path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_2.fq.gz" # fastq2
    output:
        temp("results/{LANE}/{ID}.bam")
    params:
        lib = lambda wildcards: libraries_dict[wildcards.LANE],
        p = "bigmemm"
    conda:
        "envs/environment.yaml"
    shadow:
        "minimal"
    resources:
        time_min = lambda wildcards, attempt: attempt * 24 * 60,
        mem = lambda wildcards, attempt: attempt * 24
    threads:
        16
    shell:
        "(bwa mem -R '@RG\\tID:{wildcards.LANE}\\tSM:{wildcards.ID}\\tPL:ILLUMINA\\tLB:{params.lib}\\tPU:{wildcards.ID}.{wildcards.LANE}' "
        "-t {threads} -v 3 {input.ref} {input.fq1} {input.fq2} | "
        "samtools view -Shu -) > {output}"

## samtools_sort: sorts bams by position
rule samtools_sort:
    input:
        "filtered_bams/results/{LANE}/{ID}.bam"
    output:
        temp("filtered_bams/results/{LANE}/{ID}.sort.bam")
    params:
        p = "bigmemm"
    conda:
        "envs/environment.yaml"
    shadow:
        "minimal"
    threads:
        4
    resources:
        time_min = lambda wildcards, attempt: attempt * 12 * 60,
        mem = 32
    shell:
        "mkdir -p tmp_sorting_reads/{wildcards.ID}_{wildcards.LANE} &&"
        "samtools sort -m 6G -@ {threads} "
        "-T tmp_sorting_reads/{wildcards.ID}_{wildcards.LANE} "
        "-O bam {input} > {output};"
        "rm -r tmp_sorting_reads/{wildcards.ID}_{wildcards.LANE}"

## mark_duplicates: marks and removes duplicate reads using PICARD
rule mark_duplicates:
    input:
        ref = ref,
        fai = fai,
        bam = "filtered_bams/results/{LANE}/{ID}.sort.bam"
    output:
        bam = "filtered_bams/results/{LANE}/{ID}.sort.dedup.bam",
        metrics = "filtered_bams/metrics/picard/{LANE}/{ID}.metrics.txt"
    params:
        p = "med2"
    conda:
        "envs/environment.yaml"
    shadow:
        "minimal"
    # shadow minimal should make the temporary directory on the --shadow-prefix location, i.e. set to local node storage
    resources:
        time_min = lambda wildcards, attempt: attempt * 8 * 60,
        mem = 8
    # java 1.8 and picard-tools 2.7.1 loaded from cluster
    # note: I only filter mapQ > 1 at this step, though in downstream analyses often use a more stringent threshold mapQ > 30
    shell:
        "module load java && "
        "module load picardtools/2.7.1 && "
        "mkdir -p tmp_dedup_reads/{wildcards.ID}_{wildcards.LANE} && "
        "java -Xmx6g -jar ${{PICARD}}/picard.jar MarkDuplicates "
        "INPUT= {input.bam} OUTPUT=/dev/stdout QUIET=true "
        "REMOVE_DUPLICATES=true "
        "TMP_DIR=tmp_dedup_reads/{wildcards.ID}_{wildcards.LANE} "
        "METRICS_FILE={output.metrics} | "
        "samtools view -bS -q 1 - > {output.bam} && "
        "rm -r tmp_dedup_reads/{wildcards.ID}_{wildcards.LANE}"
        # remove temporary directory at the end

## samtools_index: makes index for bam file
rule samtools_index:
    input:
        "filtered_bams/results/{LANE}/{ID}.sort.dedup.bam"
    output:
        "filtered_bams/results/{LANE}/{ID}.sort.dedup.bam.bai"
    params:
        p = "med2"
    conda:
        "envs/environment.yaml"
    shell:
        "samtools index {input}"

# merge multiple bams for the same HILO sample (only a subset of samples were sequenced on multiple lanes and therefore need to be merged)
def bams2merge(wildcards):
    return expand("filtered_bams/results/{LANE}/{ID}.sort.dedup.bam", LANE = merge_dict[wildcards.ID], ID = wildcards.ID)
def bais2merge(wildcards):
    return expand("filtered_bams/results/{LANE}/{ID}.sort.dedup.bam.bai", LANE = merge_dict[wildcards.ID], ID = wildcards.ID)
def count_bams(wildcards):
    return len(merge_dict[wildcards.ID])

## merge_bams: merges bams for the same sample library sequenced on multiple lanes
rule merge_bams:
    input:
        # zip in expand is like :::+ in gnu parallel (doesn't create all combinations of inputs)
        bams = bams2merge, # this is a function that produces the variable-length list of input files using the ID wildcard
        bais = bais2merge
    output:
        bam = "filtered_bams/merged_bams/{ID}.sort.dedup.bam", #note: not results/merged/ID.sort.dedup.bam because then 'merged' gets confused for a LANE wildcard value
        bai = "filtered_bams/merged_bams/{ID}.sort.dedup.bam.bai"
    params:
        p = "med2",
        n_input_bams = count_bams,
        path = path_hilo
    conda:
        "envs/environment.yaml"
    shell: # merge if multiple bams, else create a symlink for single bam
        """
        if [ {params.n_input_bams} -gt 1 ]; then
                echo 'merging files n={params.n_input_bams}'
                samtools merge {output.bam} {input.bams}
                sleep 5s
                samtools index {output.bam}
        else
                echo 'no merge required -- creating symlink to file'
                ln -s {params.path}{input.bams} {output.bam}
                sleep 5s
                ln -s {params.path}{input.bais} {output.bai}
        fi
        """

## fastQC: runs fastQC quality checks and metrics on raw reads (fastq files)
rule fastQC:
    input:
        "data/HILO_raw_reads/{LANE}/{ID}_{READ}.fq.gz"
    output:
        "filtered_bams/metrics/fastQC/{LANE}/{ID}_{READ}_fastqc.html"
    params:
        p = "med2"
    threads:
        4
    conda:
        "envs/environment.yaml"
    shell:
        "fastqc -o metrics/fastQC/{wildcards.LANE} -t {threads} {input}"

## fastQC_trimmed: runs fastQC quality checks and metrics on reads after trimming (trimmed fastq files)
rule fastQC_trimmed:
    input:
        path_hilo + "data/HILO_trimmed_reads/{LANE}/{ID}_{READ}.fq.gz"
    output:
        "filtered_bams/metrics/fastQC_trimmed/{LANE}/{ID}_{READ}_fastqc.html"
    params:
        p = "med2"
    threads:
        4
    conda:
        "envs/environment.yaml"
    shell:
        "fastqc -o metrics/fastQC_trimmed/{wildcards.LANE} -t {threads} {input}"

## multiQC_fastQC: creates summary report of fastQC
rule multiQC_fastQC:
    input:
        expand(["filtered_bams/metrics/fastQC/{LANE}/{ID}_1_fastqc.html",
                "filtered_bams/metrics/fastQC/{LANE}/{ID}_2_fastqc.html"], zip, LANE=lanes, ID=ids)
    output:
        "filtered_bams/metrics/fastQC/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 2*60,
        mem = 8
    conda:
        "envs/environment.yaml"
    shell:
        "multiqc -f metrics/fastQC -o metrics/fastQC/multiqc --dirs --ignore TEST/"

## multiQC_fastQC_trimmed: creates summary report of fastQC_trimmed
rule multiQC_fastQC_trimmed:
    input:
        expand(["filtered_bams/metrics/fastQC_trimmed/{LANE}/{ID}_1_fastqc.html",
                "filtered_bams/metrics/fastQC_trimmed/{LANE}/{ID}_2_fastqc.html"], zip, LANE=lanes, ID=ids)
    output:
        "filtered_bams/metrics/fastQC_trimmed/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 2*60,
        mem = 8
    conda:
        "envs/environment.yaml"
    shell:
        "multiqc -f metrics/fastQC_trimmed -o metrics/fastQC_trimmed/multiqc --dirs --ignore TEST/"

## samtools_flagstat: counts reads passing mapping quality > 30
rule samtools_flagstat:
    input:
        "filtered_bams/results/{LANE}/{ID}.sort.dedup.bam"
    output:
        "filtered_bams/metrics/flagstat/{LANE}/{ID}.flagstat"
    params:
        p = "med2"
    threads:
        4
    resources:
        time_min = lambda wildcards, attempt: attempt * 6 * 60
    conda:
        "envs/environment.yaml"
    # only count reads with mapping quality >= 30
    shell:
        "samtools view -q 30 -b {input} | samtools flagstat -@ {threads} -  > {output}"

## multiQC_flagstat: creates summary report of samtools_flagstat
rule multiQC_flagstat:
    input:
        expand("filtered_bams/metrics/flagstat/{LANE}/{ID}.flagstat", zip, LANE=lanes, ID=ids)
    output:
        "filtered_bams/metrics/flagstat/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 60,
        mem = 8
    conda:
        "envs/environment.yaml"
    shell:
        "multiqc -f metrics/flagstat -o metrics/flagstat/multiqc --dirs --ignore TEST/"

## multiQC_fastQC: creates summary report of PICARD mark duplicates
rule multiQC_picard:
    input:
        expand("filtered_bams/metrics/picard/{LANE}/{ID}.metrics.txt", zip, LANE=lanes, ID=ids)
    output:
        "filtered_bams/metrics/picard/multiqc/multiqc_report.html"
    params:
        p = "med2"
    resources:
        time_min = 60,
        mem = 8
    conda:
        "envs/environment.yaml"
    shell:
        "multiqc -f metrics/picard -o metrics/picard/multiqc --dirs --ignore TEST/"

## add_metadata: link sample IDs with associated metadata (e.g. population, elevation)
# rule add_metadata:
# note: did not run this with snakemake (ran interactively b/c would have re-run a bunch of files.)
#    input:
#        "filtered_bams/metrics/flagstat/multiqc/multiqc_report.html",
#         "data/HILO_raw_reads/merged_all.list",
#         "data/HILO_raw_reads/April2020_all.list",
#         "samples/april2020_meta_from_Taylor.txt",
#         "samples/get_tissue_5.28.19.txt",
#         "data/riplasm/riplasm.csv",
#         fai,
#         "colors.R"
#    output:
#        "samples/gps_and_elevation_for_sample_sites.txt",
#        "samples/HILO_meta.txt",
#        "samples/HILO_ids.list",
#        "samples/HILO_bams.list",
#        "samples/MAIZE55_ids.list",
#        "samples/MAIZE55_bams.list",
#        "samples/HILO_MAIZE55_ids.list",
#        "samples/HILO_MAIZE55_bams.list",
#        "samples/HILO_MAIZE55_meta.txt",
#        "samples/HILO_MAIZE55_meta.RData",
#        "samples/HILO_fastq_metadata.txt"
#    params:
#        p = "med2"
#    shadow:
#        "minimal"
#    resources:
#        time_min = lambda wildcards, attempt: attempt * 15,
#        mem = lambda wildcards, attempt: attempt * 2
#    conda:
#        "../envs/environment.yaml"
#    script:
#        "addMetadata2HiloIDs.R"

## list_total_samples: makes lists of included samples (IDs & bams) by population at 0.05x threshold ("samples/ALL_byPop") and 0.5x threshold ("samples/Over0.5x_byPop")
rule list_total_samples:
    input:
        meta = "samples/HILO_MAIZE55_meta.RData"
    output:
        expand("samples/{SUBSET}_byPop/{GROUP}_{LIST_TYPE}.list", GROUP=groups, LIST_TYPE=["ids", "bams"], SUBSET=["ALL", "Over0.5x"]),
        expand("samples/{SUBSET}_byPop/{POP}_{LIST_TYPE}.list", POP=sympatric_pops, LIST_TYPE=["ids", "bams"], SUBSET=["ALL", "Over0.5x"])
    params:
        p = "med2"
    shadow:
        "minimal"
    resources:
        time_min = 15,
        mem = 2
    conda:
        "../envs/environment.yaml"
    script:
        "list_total_samples_pass.R"

## plot_total_samples: plots the number of samples successfully sequenced per population
rule plot_total_samples:
    input:
        meta = "samples/HILO_MAIZE55_meta.RData",
        colors = "colors.R"
    output:
        png_local = "filtered_bams/plots/p_seq_counts.png",
        png_manuscript = "../hilo_manuscript/figures/p_seq_counts.png" # note: no shadow
    params:
        p = "med2"
    resources:
        time_min = 15,
        mem = 2
    conda:
        "../envs/environment.yaml"
    script:
        "plot_total_samples_pass.R"
