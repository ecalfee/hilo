# log for reproducing HILO data analysis

# Tue Oct 24 11:58:17 PDT 2017
# Alignment of first set of data
First download v4 of maize genome AGPv4 in fasta format from
https://www.ebi.ac.uk/ena/data/view/GCA_000005005.6

Use bwa for index, then alignment
cd data
~/Software/bwa.kit/bwa index AGPv4/AGPv4.fasta

Change the chromosome labels to something simpler, chr1, chr2 etc.
~/Software/bioawk/bioawk -c fastx '{print ">"$comment"\n"$seq}' AGPv4/AGPv4.fasta | sed 's/Zea mays cultivar B73 chromosome /chr/' | sed 's/, whole genome shotgun sequence.//' > AGPv4/AGPv4_simple.fasta
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta

Align one set of paired-end raw reads:
~/Software/bwa.kit/bwa mem AGPv4/AGPv4.fasta
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_1.fq.gz
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_2.fq.gz
 | gzip -3 > HILO_01.sam.gz

Aborted and made batch run in parallel instead piping directly into BAM format using samtools
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta; parallel --noswap --joblog hilo_alignment.log --jobs 6 '~/Software/bwa.kit/bwa mem -R "@RG\tID:batch_1\tSM:hilo_{1}" AGPv4/AGPv4_simple.fasta HILO_data/raw_data/HILO{1}/HILO{1}_*_1.fq.gz HILO_data/raw_data/HILO{1}/HILO{1}_*_2.fq.gz | ~/Software/bwa.kit/samtools view -b - > HILO_data/bam_files/HILO_{1}.bam' ::: {1..40}


# Starting from reads aligned by Dan using bwa
ecalfee@bigmem8:/group/jrigrp6/DanAlignments/HILO105$ ls
aln.bam						  HILO105_USPD16085642-AK6871_hky2mccxy_L6_2.fq.gz
HILO105_USPD16085642-AK6871_hky2mccxy_L6_1.fq.gz  MD5.txt

interactive session run:
srun -p bigmemm -t 2:00:00 --mem 2000 --pty bash

ran test batch (finished fine)
hilo/scripts$ sbatch --array=1-2 filterBam.sh
completed: 19908139_1  19908138_2

ran filtering on all of the as of yet aligned individuals from HILO
hilo/scripts$ sbatch --array=3-40,100-179 filterBam.sh

I can't really tell why some jobs failed and at what stage. I am running just 40 tasks now with more allocated time (4hrs not 1hr)
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# basically nothing ran properly, even files stating 'complete'
# I have requested to avoid bigmemm2 (permissions error) and only use nodes with at least 30G available temporary disk space for scratch
# and in addition I put in exit 3 and exit 4 flags to stop execution if it fails to make temp folders
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# got permissions on bigmem2 and changed script to exclude bigmem1 (lack of memory)
# re-running just a short sample set before the complete aligment set is ready
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh
Submitted batch job 19970756
# this had an end of script/file error, that I fixed and re-ran
# deleting intermediate scratch files first
ecalfee@farm:~/hilo/scripts$ ssh bigmem10 'rm -r /scratch/ecalfee'
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh
Submitted batch job 19970894

# running 11-30; only change to script is when the scratch folder is deleted
ecalfee@farm:~/hilo/scripts$ sbatch --array=11-40 filterBam.sh
Submitted batch job 19971006

# running the rest of the files up to ind. 200 except for the following exclusions:
# 41, 42, 43, 80 excluded because haven't finished alignment (no aln.bam file exists yet)
# 34, 109, 113 excluded because alignment needs to be re-run (timed-out -> truncated bam)
ecalfee@farm:~/hilo/scripts$ sbatch --array=44-79,81-108,110-112,114-200 filterBam.sh
Submitted batch job 20011145

# many failed due to time-out; re-started those jobs that failed plus 34, 109, 113 because they have been re-run
ecalfee@farm:~/hilo/scripts$ sbatch --array=34,101-105,109-114,117,119,121,125,127,130,131,133-135,137,139,141,142,161-163,165,169,170,172,174-177 filterBam.sh
Submitted batch job 20102438

# final failed jobs due to lack of space; re-running those jobs
ecalfee@farm:~/hilo/scripts$ sbatch --array=121,127,133,135,141,162,176,177 filterBam.sh
Submitted batch job 20107398

# indexing reference
ecalfee@farm:~/hilo/scripts$ sbatch indexRef.sh
Submitted batch job 20107852

# Tue May 29 15:26:50 PDT 2018
# all jobs except those excluded finished (ie. 41-43, 80).
# run angsd to find variable sites within all individuals > 0.1 MAF
# short test run (in srun):
ecalfee@bigmem10:~/hilo/data$ angsd -GL 1 -ref /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa \
-baq 1 -minMapQ 30 -minQ 20 -doMaf 2 -doGlf 2 -minMaf 0.1 -doMajorMinor 4 -bam TEST/testBAM.list \
-r 1:10000-20000 -out testANGSD

# indexed BAMS so they could be used
ecalfee@bigmem10:~/hilo/data/filtered_bam$ module load samtools
Module samtools/1.3.1 loaded
ecalfee@bigmem10:~/hilo/data/filtered_bam$ for i in {3..40} {44..79} {81..200}; do samtools index hilo_$i.sort.dedup.bam; done

# now pre-calculating BAQ to speed up GL in angsd and to use again when counting reads
# script caps base quality at the lower of raw base quality and BAQ from samtools
# using extended BAQ calculation -E option; recommended for improved sensitivity (will be new default)
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-40,44-79,80-200 addBAQBam.sh
Submitted batch job 20147251
ecalfee@farm:~/hilo/scripts$ date
Tue May 29 19:29:51 PDT 2018

# making list of BAMS for fist pass analysis:
data$ for i in {1..40} {44..79} {81..200}; do echo filtered_bam/hilo_$i.sort.dedup.baq.bam >> pass1_bam.all.list; done

# counting # reads that mapped & passed mapping quality and non-duplicate read quality filtering for each pass1 indiviudal:
# (this is an estimate of coverage/data quality for each sequenced individual)
~/hilo/data/filtered_bam$ for i in {1..200}; do samtools flagstat hilo_$i.sort.dedup.baq.bam | \
awk -v i="$i" '{print "hilo"i" "$0}' | grep 'total' >> pass1.all.metrics.Nreads; done

# finding variable sites with ANGSD, each of 10 chrom at a time (4 nodes requested per chrom):
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20182952
Wed May 30 09:16:56 PDT 2018
# -CANCELLED (problems = low memory and wrong individuals .. doubled)
# fixed and re-running 15:53 pm:
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20203873
# -CANCELLED. Talked to Jeff and decided to only look for SNPs > 0.5 freq using a non-HW based method
# I will do something different to find SNPs for local ancestry inference, but this doesn't use the reference
# and only counts reads supporting the two most common bases, after quality filters
# NOTE: this wouldn't find a fixed difference between maize 282 and the current sample
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20245129
# Problem was a file-dumping error because I had in a check from SNP_pval; resolved by removing that part
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20265142
# Took < 2hrs to run. Decided I wanted more SNPs, so I reduced the threshold to 50 individuals with information (> half).
# put old files in old_pass1/
# it's recommended in the NGSadmix paper to avoid sites with > 80% missing data, so 50+ ind with info is slightly conservative
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20280102
ecalfee@farm:~/hilo/scripts$ date
Thu May 31 10:38:49 PDT 2018
# only chr1 failed (largest). I reduced requested memory to get it back on queue
ecalfee@farm:~/hilo/scripts$ sbatch --array=1 callVarAngsd.sh
Submitted batch job 20419175
# chr1 failed again for too much memory, so I split it into 10 chunks:
hilo/scripts$ sbatch --array=0-9 callVarAngsd_split_chr1.sh
Submitted batch job 20591724
Mon Jun  4 14:38:49 PDT 2018
# where now 0-9 above indicate the indeces of each chunk ~30000 kbp long
# I will need to concatenate chunks before pruning (below)
# in srun concatenating (without intermediate headers)
data/var_sites/pass1$ zcat chr1_chunk0.mafs.gz | gzip > chr1.mafs.gz; \
for i in {1..9}; do zcat chr1_chunk$i.mafs.gz | tail -n +2 | gzip >> chr1.mafs.gz; done
# then moving all chunk files to their own subfolder:
data/var_sites/pass1$ mv chr1_chunk* chr1_chunks/

# made a python script to prune sites to approximately unlinked (no more than 1 SNP per 10kb).
# which I wrapped in a bash script pruneFixed.sh
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-9 pruneFixed.sh
Submitted batch job 20420257
~/hilo/scripts$ sbatch --array=1,10 pruneFixed.sh
Submitted batch job 20603410
# After pruning, I combine across all chromosomes and remove all header (and unzip/decompress);
# this leaves ~170k SNPs.
~hilo/data/var_sites/pass1$ mkdir pruned_positions
~/hilo/data/var_sites/pass1$ for i in {1..10}; do zcat chr$i.pruned.mafs.gz | tail -n +2 | cut -f 1-4 >> pruned_positions/all.positions; done
# then I divide into chunks, each containing 250 SNPs, for parallel analysis in ANGSD
~/hilo/data/var_sites/pass1/pruned_positions$ split all.positions -d -a 3 -l 250 positions_chunk

# next find samtools genotype likelihood for those sites across all individuals.
# made (very!) small test file first - 2 positions
~/hilo/data/var_sites/pass1/pruned_positions$ head positions_chunk000 -n 2 > positions_chunk999
# ran test file on slurm
~/hilo/scripts$ sbatch --array=999 -t 1:00:00 calcGL4Pruned.sh
Submitted batch job 20652316

# now running all 677 chunks
~/hilo/scripts$ sbatch --array=0-677 calcGL4Pruned.sh
Submitted batch job 20652343

# and then combine output (in srun)
~/hilo/data/geno_lik/pass1/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..677}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# and running NGSadmix
~/hilo/scripts$ sbatch runNGSadmix.sh
Submitted batch job 20781908
Mon Jun 11 22:36:54 PDT 2018

# modified NGSadmix to use array task ID number as # subpops (K)
# running with same SNPs for K = 3 & 4
~/hilo/scripts$ sbatch --array=3,4 runNGSadmix.sh
Submitted batch job 20803690
Submitted batch job 21671940 -- accidentally resubmitted job and re-ran files (completed & should be same output)

# I divide the genome into manageable 50Mb chunks based on .fa.fai reference file [comp. Barbara; scp results to Farm]
hilo/scripts$ mkdir -p ../data/refMaize/divide_50Mb
hilo/scripts$ python divRefRegions.py 50000000 ../data/refMaize/Zea_mays.AGPv4.dna.chr.fa.fai ../data/refMaize/divide_50Mb
# save a file with a list of regions: chr, start, end, region #, regions file name
~/hilo/data/refMaize/divide_50Mb$ for i in {0..46}; do awk -v i=$i 'gsub(":|-", "\t", $0){print $0"\t"i"\t""region_"i".txt"}' region_$i.txt >> ALL_regions.list; done

# new bam lists:
# data/HILO_IDs_cov_pass1.csv is created in plot_global_ancestry_NGSadmix & saves the # reads that pass filtering + estimated coverage = (# reads * 150bp/read)/2.3Gb as well as meta data on pops and HILO_id as well as IndN for pass1 & GL_beagle_file_ID (skips unaligned ind's)
# sympatric populations only, and individuals must have bam & est. coverage > 0.05
hilo/data$ awk '$1 != "n" && $3 != 20 && $3 != 22 && $3 != 33 && $5 != "NA" && $6 > 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlySympatric.list

# allopatric mexicana only, no minimum coverage
hilo/data$ awk '$1 != "n" && ($3 == 20 || $3 == 22 || $3 == 33) && $5 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlyAllopatricMex.list

# excluded ind's (fit into neither category above)
hilo/data$ awk '$1 != "n" && ($5 == "NA" || ($3 != 20 && $3 != 22 && $3 != 33 && $6 <= 0.05)) {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.excluded.list

# now finding new set of variant sites using ANGSD for only sympatric individuals (all)
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdSymp.sh
Submitted batch job 20811827
# everything ran but region_0 doesn't appear to be properly gipped and can't be opened
# so I am re-running just that region (problem was I wrote over just this file accidentally with test run of callVarAngsdAllo.sh)
~/hilo/scripts$ sbatch --array=0 callVarAngsdSymp.sh
Submitted batch job 20816879

# when I filter for low LD I can try different filters for nInd and MAF (more stringent)
# to maximize information across individuals for the LD

# now need to find all variant sites between reference and allopatric teosinte at >=30% freq.
# Then find all variant sites between reference and allopatric maize at >=30% freq .. put them together and exclude sites with less than a cutoff # individuals and allele frequency difference between mexicana & maize of 40%
# not quite working -- look at slurm log
~/hilo/scripts$ sbatch --array=0 callVarAngsdAllo.sh
# fixed and after another small typo false start now running all regions
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdAllo.sh
Submitted batch job 20817741

# added script to prune based on fixed cM width and Ogut_2015 map, rather than fixed bp (0.01 cM ~ 10kb on avg but varies across genome)
pruneFixedcM.py
# running with two diff. thresholds for minimum # individuals, 75 and 100: (loads module anaconda3 for use of pandas)
~/hilo/scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20930024
# did not work -- fixed error and added a way to set minInd as an evironmental variable and run each chromosome separately
# (can't break up further b/c of min cM across regions w/in chromosomes)
# running with default MIN_IND=75
scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20950468
# running again with MIN_IND=100
~/hilo/scripts$ sbatch --export=MIN_IND=100 pruneFixedcM.sh
Submitted batch job 20950469

# Thu Jun 21 23:44:06 PDT 2018
# out-of-order markers in the Ogut 2015 map need to be solved -- started process with clean_ogut_2015_map.R
# from Jeff: "Map and/or assembly error; Snp is in correct spot physically but stuck with mapping position
# assigned using bad genome" so I will drop all bad SNPs -- note they are clustered (e.g. on chr7 and affect ~3-5% markers)
# for now will use fixed BP pruning of sites until map issues are solved:

# updated pruneFixedBP.py from pruneFixed.py to take in multiple files and filter for MAF and number of ind's with data
# running w/ default min. ind. = 75 and separately w/ min. ind. = 100:
~/hilo/scripts$ sbatch pruneFixedBP.sh
Submitted batch job 21645706
~/hilo/scripts$ sbatch --export=MIN_IND=100 pruneFixedBP.sh
Submitted batch job 21645898
# both completed within a few minutes, pruning down to ~115k and ~46k
# I will start analysis with the 46K SNPs here at least 100 individuals have some data/reads

# breaking down 46k snps into chunks for beagle likelihood in angsd
~/hilo/data/var_sites/pass1/sympatric/pruned/minN100$ mkdir position_chunks
# combine and unzip; extract relevant columns (there are no headers .. so no need to take tail)
$ for i in {1..10}; do zcat thin10kb_chr$i.mafs.gz; done | cut -f 1-4 > all.positions
# split into chunks
$ split all.positions -d -a 3 -l 250 position_chunks/chunk
# run ANGSD - modified calcGL4Pruned.sh to take in variables as needed
# note: must set ALL variables separated by commas to overrule one variable
scripts$ sbatch --export=OUT_DIR=geno_lik/pass1/sympatricVar/pruned_chunks,IN_DIR=var_sites/pass1/sympatric/pruned/minN100/position_chunks/,BAM_LIST=pass1_bam.all.list \
--array=000-184 calcGL4Pruned.sh
Submitted batch job 21659878

# put it together into one GL file for NGSadmix
data/geno_lik/pass1/sympatricVar/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..184}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# run new SNPs in NGSadmix for K=2,3,4 (modified runNGSadmix.sh to take in variables for IN and OUT files):
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/sympatricVar/pruned_all.beagle.gz,OUT_DIR=NGSadmix/pass1/sympatricVar/allInd/ runNGSadmix.sh

Submitted batch job 21672755

# pulling out genotype likelihood data for NGSadmix for just allopatric teosinte and sympatric maize
# to see how well it differentiates these groups
# first I get the GL header for all individuals in pass1: ~/hilo/data/pass1_bam.all.GLheader
# then I use R to extract the column # for individuals I am interested in: findGLColN.R
# saved in file data/pass1_bam.SympMaizeAlloMex05.GLcolN
~/hilo/data/geno_lik/pass1/sympatricVar$ zcat pruned_all.beagle.gz | cut -f$(cat ../../../pass1_bam.SympMaizeAlloMex05.GLcolN) | gzip >> pruned_SympMaizeAlloMex05.beagle.gz
# and re-running in NGSadmix:
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/sympatricVar/pruned_SympMaizeAlloMex05.beagle.gz,OUT_DIR=NGSadmix/pass1/sympatricVar/SympMaizeAlloMex05/ runNGSadmix.sh
Submitted batch job 21673267

# and same for first set of SNPs (globally ascertained including low cov. ind's and allopatric teosinte)
~/hilo/data/geno_lik/pass1$ zcat pruned_all.beagle.gz | cut -f$(cat ../../pass1_bam.SympMaizeAlloMex05.GLcolN) | gzip >> pruned_SympMaizeAlloMex05.beagle.gz
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/pruned_SympMaizeAlloMex05.beagle.gz,OUT_DIR=NGSadmix/pass1/SympMaizeAlloMex05/ runNGSadmix.sh
Submitted batch job 21673279

# getting a full set of SNPs with MAF > 0.05 freq in all sequenced ind's
# modified allVarAngsdGL.sh to get a count-based MAF and also output SAMtools style GL file
scripts$ sbatch allVarAngsdGL.sh
Submitted batch job 21690039
# some failed due to time-out at 36 hours (re-doing):
scripts$ sbatch --array=0,21,26,31,37 --mem=20G -t 108:00:00 allVarAngsdGL.sh
Submitted batch job 22351904
# region 0 failed due to insufficient memory at t = 12hrs; all others completed (not sure why they now took < 36hrs)
scripts$ sbatch --array=0 --mem=40G -t 96:00:00 allVarAngsdGL.sh
Submitted batch job 22502000
# now completed in 9 hrs
# concatenating all beagle gz files together for one genome-wide GL file:
# takes too long to do interactively:
scripts$ sbatch catBeagleGL.sh
Submitted batch job 22757237
# taking just every 100th position/line: _pruned_by100
scripts$ sbatch catBeagleGL_nth.sh
Submitted batch job 22757239
-ISSUE IS THAT THIS OMITS THE FIRST HEADER!


# I need to understanding if mapping bias is a problem -- what does mapping look like across groups (e.g. % mapped)? Is there biased mapping across my SNP sets?
# first I see if there are large differences in % reads that do not map
# I create a script to count the reads that align (map at all), do not map, pass mapQ >=30 and pass de-duplication
scripts$ sbatch countReadsFiltered.sh
Submitted batch job 22758110
# saves output in text file, e.g. countReads/hilo_40.counts
# re-running files that TIMEOUT -- more time and less memory allotment
scripts$ sbatch --array=12,17-24,27-29,37,39,40,44-46,48-51,56-58,60-68,71,82,86-98,101,103-105,107,109-114,116-125,127-129,133,135-137,139,141,142,154,155,161,165,168-172,174-178,182,188,200 countReadsFiltered.sh
Submitted batch job 22765848
# still had many jobs cancel due to time out at 4 hours (!) - bumped up to 12 hours
scripts$ sbatch -t 12:00:00 --array=12,17-24,27-29,39,40,45-46,49-51,58,60-61,71,87-98,101,103-105,107,109-114,116-125,127,129,133,135-137,139,141,142,154,155,161,165,168-172,174-178,182,188,200 countReadsFiltered.sh
Submitted batch job 22799632
# still many jobs cancelled -- make new script that uses samtools built in view -c for 'counts' and trying again - countReadsFiltered2.sh
scripts$ sbatch countReadsFiltered2.sh
Submitted batch job 22937206
# not running due to priority -- submitted other job
scripts$ sbatch -p med -t 24:00:00 countReadsFiltered2.sh
Submitted batch job 22967048

# I can also zoom in and look at individuals' depth across specific regions:
~/hilo/data/filtered_bam$ samtools depth -r 1:400-500 hilo_18.sort.dedup.bam hilo_19.sort.dedup.bam


# Now I am working on getting a PCA using PCAngsd. This is better than NGStools PCA option because NGStools puts a prior on one population allele frequency -- so low coverage individuals end up near the center, whereas PCAngsd uses individually estimated allele frequencies (somehow without assuming they are indep.).
# to install python2.7 dependencies, I set up a python virtual environment for python 2.7.14 on my user on farm:
~/hilo$ module load python
~/hilo$ virtualenv --no-site-packages venv
# the --no-site-packages option is necessary to avoid conflicts between global packages and the ones in my venv
# activate virtual environment (need to add to top of any of my code now using this venv)
~/hilo$ source venv/bin/activate
~/bin/pcangsd$ pip install numpy Cython enum
~/bin/pcangsd$ pip install -r python_packages.txt
# problem installing numba due to llvm:
RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config
# install llvm:
wget http://releases.llvm.org/6.0.0/clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
~/software$ tar xf clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
(venv) ecalfee@bigmem2:~/hilo$ export LLVM_CONFIG=/home/ecalfee/software/clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04/bin/llvm-config
# based on online forum
~/hilo$ pip install funcsigs
# NONE OF THIS WORKED -- likely clang vs. gcc compiler issues (!)
# alternative route -- I've installed anaconda2
# to use, first set path to anaconda2 files:
export PATH=/home/ecalfee/software/anaconda2/bin:$PATH
# this time only, I create a new conda environment:
~/hilo$ conda create -y -n condaEnv
# every time, I need to activate the condaEnv:
~/hilo$ source activate condaEnv
# to this environment, I install llvmlite
(condaEnv) ecalfee@c8-62:~/hilo$ conda install -y llvmlite
# when I open python2, I can import all the pcangsd dependencies: numpy, scipy, pandas, numba
# so they appear to be pre-installed
# this example file now runs -- great!
(condaEnv) ecalfee@c8-62:~/hilo/data/TEST$ python2 ~/bin/pcangsd/pcangsd.py -beagle testANGSD.beagle.gz -threads 1 -iter 100 -o TESTpcangsd

# running PCA on set of pruned SNPs
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/pruned_all runPCAngsd.sh
Submitted batch job 22757232
# added -admix option and re-did:
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/pruned_all --mem=1G -t 3:00:00 runPCAngsd.sh
Submitted batch job 22758865
# and running PCA on set of pruned SNPs that vary within sympatric populations maize/teosinte
~/hilo/scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/sympatricVar/pruned_all runPCAngsd.sh
Submitted batch job 22757408
# added -admix option and re-did:
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/sympatricVar/pruned_all --mem=1G -t 3:00:00 runPCAngsd.sh
Submitted batch job 22758855
WORKED
# will also run PCA on data for all hilo individuals, at all SNPs in 'allVar'
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome --mem=2G -t 96:00:00 runPCAngsd.sh
Submitted batch job 22762726
CANCELLED-EXCEEDED MEMORY LIMIT. RE-RUNNING:
sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome --mem=12G -t 96:00:00 runPCAngsd.sh
Submitted batch job 22766166
# subset of all SNPs in 'allVar'
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome_pruned_by100 --mem=1G -t 36:00:00 runPCAngsd.sh
Submitted batch job 22762723
ERROR-DID NOT ACCEPT REDUCED FILE BY 100 .. NOT SURE WHY .. PROBLEM IS THAT IT'S MISSING A HEADER - I CAN FIX THAT

# I will also estimate pairwise Fst between each group (allopatric/sympatric/maize/mex) and between populations within each larger group.
# to do Fst, I first need to calculate allele freq. spectrum for each population -- so I separate them into population bam files
# each meta population (no filtering for coverage except skipping files with no alignment in pass1)
~/hilo/data$ awk '$5 == "maize" && $6 == "sympatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/maize.symp.list
~/hilo/data$ awk '$5 == "mexicana" && $6 == "sympatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/mexicana.symp.list
~/hilo/data$ awk '$5 == "mexicana" && $6 == "allopatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/mexicana.allo.list
# give these numerical copies for ease:
~/hilo/data/pass1_bam_pops$ cp maize.symp.list pop1000.list
~/hilo/data/pass1_bam_pops$ cp mexicana.symp.list pop2000.list
~/hilo/data/pass1_bam_pops$ cp mexicana.allo.list pop3000.list
# each individual population
~/hilo/data$ for i in $(awk '$1 != "n" && $7 != "NA" {print $3}' HILO_IDs_cov_pass1.csv | sort | uniq); do awk -v i=$i '$3 == i && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/pop$i.list; done

# all steps to SFS are listed here: http://www.popgen.dk/angsd/index.php/Fst
# should use many cores for step 2 and need to download realSFS software (actually it's part of ANGSD version already on farm)

# step 1: calculate individual site allele frequencies for each pop
scripts$ sbatch calcSAFAngsd.sh
Submitted batch job 21693057
# re-doing the 2 that didn't finish (time out cancellation):
scripts$ sbatch --mem=20G --array=1000,2000 -t 108:00:00 calcSAFAngsd.sh
Submitted batch job 22356436 -- will not finish (way too long)
# also an error with angsd -- too low a version, so I updated (see error & update log below)
# re-doing SAF split by chromosome:
# first for groups of pops, needs lots of memory and time:
~/hilo/scripts$ for i in 1000 2000 3000; do sbatch --export=POP=$i calcSAFAngsd_byChr.sh; done
Submitted batch job 22625518
Submitted batch job 22625519
Submitted batch job 22625520
# then for all individual pops, requires less memory. First put popN in a list:
~/hilo/scripts$ for i in {18..31} {33..35} {360..363} {365..374}; do echo $i >> ../data/pass1_bam_pops/all_pops.numbers; done
~/hilo/scripts$ for i in $(cat ../data/pass1_bam_pops/all_pops.numbers); do sbatch --mem=15G -t 24:00:00 --export=POP=$i calcSAFAngsd_byChr.sh; done
Submitted batch job 22628540-22628570
# re-submitting 11 jobs that ran out of time (modified script for more threads; less memory per thread):
sbatch --export=POP=22 --array=1 calcSAFAngsd_byChr.sh
Submitted batch job 22764908
sbatch --export=POP=20 --array=1-10 calcSAFAngsd_byChr.sh
Submitted batch job 22764942

# concatenate results by chromosome back together (skipping pops 20, 22 and larger groups for now):
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.sh
Submitted batch job 22765120
# fixed typo with --outname vs -outname & reran:
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.sh
Submitted batch job 22800872 -- DIDN'T WORK. fixed true typo -outnames vs. -outnames & order & reran:
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.s
Submitted batch job 22801018

# step2: for each pair of populations, calculate the 2D SFS prior: (this is a lot of Fst calculations!)
s.t. like hilo$ for i in {18..31} {33..35} {360..363} {365..374}; do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; then echo $i $j; fi; done; done
# problem: getting the following error:
realSFS print TEST/pop31.pop22.old.fst.gz
	-> Version of fname:TEST/pop31.pop22.old.fst.gz is:0
	-> Looks like you are trying to use a version of realSFS that is incompatible with the old binary output from ANGSD
	-> Please use realSFS.old instead (or consider redoing the saf files )
	-> Will exit
  # updating ANGSD on farm:
  ecalfee@bigmem2:~/bin$ module load zlib
  Module zlib/1.2.8 loaded
  ecalfee@bigmem2:~/bin$ module load htslib
  Module htslib/1.3.2 loaded
  # download newest angsd official release and accompanying htslib:
  ~/bin$ wget http://popgen.dk/software/download/angsd/angsd0.920.tar.gz
  # unzip
  ~/bin$ tar xf angsd0.920.tar.gz
  # install htslib
  ~/software/htslib$ make install prefix=../bin
  # install angsd using link to my local htslib directory
  ~/software/angsd$ make HTSSRC=../htslib/
  # moved exec. file software/angsd/angsd to bin/ and software/angsd/misc/realSFS to bin/ too and v1.6 of tabix and htsfile too
# to calculate Fst between population pairs:
# running now for all pop pairs except 20 & 22
scripts$ for i in 18 19 21 {23..31} {33..35} {360..363} {365..374}; do for j in 18 19 21 {23..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; then sbatch --export=POP1=$i,POP2=$j calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 22800219-22800624
# NOTE : There has to be a better way to organize these jobs into one 'super job'

# This whole Fst estimation should be done with a set of regions. For SAF calculations, I can provide a set of sites or a region
TEST$ angsd sites index sites_5.txt
TEST$ realSFS print pop18.saf.idx -sites sites_5.txt
TEST$ realSFS print pop18.saf.idx -r 1:500-600
# but a regions file doesn't work
TEST$ realSFS print pop18.saf.idx -rf regions_5.txt - error
# I may be able to either loop over a regions file in a short script, and cat together the SAF's in the end
# or to supply an -rf file to angsd when making the SAF file
# the following works, where regions.txt has regions e.g. 1:100-200 listed
*regions must be sorted!!
data$ angsd -out TEST/SAFbyRegion_test_pop18 -anc /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa -fold 1 -rf TEST/regions.txt -bam pass1_bam_pops/pop18.list -remove_bads 1 -minMapQ 30 -GL 1 -dosaf 1

# Getting a set of random regions from the maize reference genome:
# bedtools random function will work, but I need to make a 'genome' file with chromosome names & lengths first:
hilo/data$ cut -f1,2 /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa.fai | grep -v 't' > refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths
# now generating 1000 random regions of 100bp each:
scripts$ sbatch getRandomRegions.sh
Submitted batch job 22934334
# and calculating SAF off of these regions using an updated calcSAFAngsd.sh script:
scripts$ sbatch --array=18-31,33-35,360-363,365-374 --mem=4G calcSAFAngsd.sh
Submitted batch job 22934857
scripts$ sbatch --array=1000,2000,3000 --mem=12G -t 24:00:00 calcSAFAngsd.sh
Submitted batch job 22934957
