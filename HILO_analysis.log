# log for reproducing HILO data analysis

# Tue Oct 24 11:58:17 PDT 2017
# Alignment of first set of data
First download v4 of maize genome AGPv4 in fasta format from
https://www.ebi.ac.uk/ena/data/view/GCA_000005005.6

Use bwa for index, then alignment
cd data
~/Software/bwa.kit/bwa index AGPv4/AGPv4.fasta

Change the chromosome labels to something simpler, chr1, chr2 etc.
~/Software/bioawk/bioawk -c fastx '{print ">"$comment"\n"$seq}' AGPv4/AGPv4.fasta | sed 's/Zea mays cultivar B73 chromosome /chr/' | sed 's/, whole genome shotgun sequence.//' > AGPv4/AGPv4_simple.fasta
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta

Align one set of paired-end raw reads:
~/Software/bwa.kit/bwa mem AGPv4/AGPv4.fasta
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_1.fq.gz
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_2.fq.gz
 | gzip -3 > HILO_01.sam.gz

Aborted and made batch run in parallel instead piping directly into BAM format using samtools
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta; parallel --noswap --joblog hilo_alignment.log --jobs 6 '~/Software/bwa.kit/bwa mem -R "@RG\tID:batch_1\tSM:hilo_{1}" AGPv4/AGPv4_simple.fasta HILO_data/raw_data/HILO{1}/HILO{1}_*_1.fq.gz HILO_data/raw_data/HILO{1}/HILO{1}_*_2.fq.gz | ~/Software/bwa.kit/samtools view -b - > HILO_data/bam_files/HILO_{1}.bam' ::: {1..40}


# Starting from reads aligned by Dan using bwa
ecalfee@bigmem8:/group/jrigrp6/DanAlignments/HILO105$ ls
aln.bam						  HILO105_USPD16085642-AK6871_hky2mccxy_L6_2.fq.gz
HILO105_USPD16085642-AK6871_hky2mccxy_L6_1.fq.gz  MD5.txt

interactive session run:
srun -p bigmemm -t 2:00:00 --mem 2000 --pty bash

ran test batch (finished fine)
hilo/scripts$ sbatch --array=1-2 filterBam.sh
completed: 19908139_1  19908138_2

ran filtering on all of the as of yet aligned individuals from HILO
hilo/scripts$ sbatch --array=3-40,100-179 filterBam.sh

I can't really tell why some jobs failed and at what stage. I am running just 40 tasks now with more allocated time (4hrs not 1hr)
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# basically nothing ran properly, even files stating 'complete'
# I have requested to avoid bigmemm2 (permissions error) and only use nodes with at least 30G available temporary disk space for scratch
# and in addition I put in exit 3 and exit 4 flags to stop execution if it fails to make temp folders
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# got permissions on bigmem2 and changed script to exclude bigmem1 (lack of memory)
# re-running just a short sample set before the complete aligment set is ready
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh
Submitted batch job 19970756
# this had an end of script/file error, that I fixed and re-ran
# deleting intermediate scratch files first
ecalfee@farm:~/hilo/scripts$ ssh bigmem10 'rm -r /scratch/ecalfee'
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh
Submitted batch job 19970894

# running 11-30; only change to script is when the scratch folder is deleted
ecalfee@farm:~/hilo/scripts$ sbatch --array=11-40 filterBam.sh
Submitted batch job 19971006

# running the rest of the files up to ind. 200 except for the following exclusions:
# 41, 42, 43, 80 excluded because haven't finished alignment (no aln.bam file exists yet)
# 34, 109, 113 excluded because alignment needs to be re-run (timed-out -> truncated bam)
ecalfee@farm:~/hilo/scripts$ sbatch --array=44-79,81-108,110-112,114-200 filterBam.sh
Submitted batch job 20011145

# many failed due to time-out; re-started those jobs that failed plus 34, 109, 113 because they have been re-run
ecalfee@farm:~/hilo/scripts$ sbatch --array=34,101-105,109-114,117,119,121,125,127,130,131,133-135,137,139,141,142,161-163,165,169,170,172,174-177 filterBam.sh
Submitted batch job 20102438

# final failed jobs due to lack of space; re-running those jobs
ecalfee@farm:~/hilo/scripts$ sbatch --array=121,127,133,135,141,162,176,177 filterBam.sh
Submitted batch job 20107398

# indexing reference
ecalfee@farm:~/hilo/scripts$ sbatch indexRef.sh
Submitted batch job 20107852

# Tue May 29 15:26:50 PDT 2018
# all jobs except those excluded finished (ie. 41-43, 80).
# run angsd to find variable sites within all individuals > 0.1 MAF
# short test run (in srun):
ecalfee@bigmem10:~/hilo/data$ angsd -GL 1 -ref /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa \
-baq 1 -minMapQ 30 -minQ 20 -doMaf 2 -doGlf 2 -minMaf 0.1 -doMajorMinor 4 -bam TEST/testBAM.list \
-r 1:10000-20000 -out testANGSD

# indexed BAMS so they could be used
ecalfee@bigmem10:~/hilo/data/filtered_bam$ module load samtools
Module samtools/1.3.1 loaded
ecalfee@bigmem10:~/hilo/data/filtered_bam$ for i in {3..40} {44..79} {81..200}; do samtools index hilo_$i.sort.dedup.bam; done

# now pre-calculating BAQ to speed up GL in angsd and to use again when counting reads
# script caps base quality at the lower of raw base quality and BAQ from samtools
# using extended BAQ calculation -E option; recommended for improved sensitivity (will be new default)
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-40,44-79,80-200 addBAQBam.sh
Submitted batch job 20147251
ecalfee@farm:~/hilo/scripts$ date
Tue May 29 19:29:51 PDT 2018

# making list of BAMS for first pass analysis:
data$ for i in {1..40} {44..79} {81..200}; do echo filtered_bam/hilo_$i.sort.dedup.baq.bam >> pass1_bam.all.list; done

# counting # reads that mapped & passed mapping quality and non-duplicate read quality filtering for each pass1 indiviudal:
# (this is an estimate of coverage/data quality for each sequenced individual)
~/hilo/data/filtered_bam$ for i in {1..200}; do samtools flagstat hilo_$i.sort.dedup.baq.bam | \
awk -v i="$i" '{print "hilo"i" "$0}' | grep 'total' >> pass1.all.metrics.Nreads; done
# counting # reads pre-filtering, and at de-duplication using PICARD metrics.txt output files
# (to compare effects of filtering across groups/individuals):
~/hilo/data/filtered_bam$ grep 'LIBRARY' hilo_1.metrics.txt | awk '{print "StudyID\t"$0}' > pass1.all.metrics.raw.Nreads; \
for i in {1..200}; do grep 'Unknown Library' hilo_$i.metrics.txt | \
awk -v i="$i" '{print "hilo"i" "$0}' >> pass1.all.metrics.raw.Nreads; done

# finding variable sites with ANGSD, each of 10 chrom at a time (4 nodes requested per chrom):
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20182952
Wed May 30 09:16:56 PDT 2018
# -CANCELLED (problems = low memory and wrong individuals .. doubled)
# fixed and re-running 15:53 pm:
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20203873
# -CANCELLED. Talked to Jeff and decided to only look for SNPs > 0.05 freq using a non-HW based method
# I will do something different to find SNPs for local ancestry inference, but this doesn't use the reference
# and only counts reads supporting the two most common bases, after quality filters
# NOTE: this wouldn't find a fixed difference between maize 282 and the current sample
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20245129
# Problem was a file-dumping error because I had in a check from SNP_pval; resolved by removing that part
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20265142
# Took < 2hrs to run. Decided I wanted more SNPs, so I reduced the threshold to 50 individuals with information (> half).
# put old files in old_pass1/
# it's recommended in the NGSadmix paper to avoid sites with > 80% missing data, so 50+ ind with info is slightly conservative
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20280102
ecalfee@farm:~/hilo/scripts$ date
Thu May 31 10:38:49 PDT 2018
# only chr1 failed (largest). I reduced requested memory to get it back on queue
ecalfee@farm:~/hilo/scripts$ sbatch --array=1 callVarAngsd.sh
Submitted batch job 20419175
# chr1 failed again for too much memory, so I split it into 10 chunks:
hilo/scripts$ sbatch --array=0-9 callVarAngsd_split_chr1.sh
Submitted batch job 20591724
Mon Jun  4 14:38:49 PDT 2018
# where now 0-9 above indicate the indeces of each chunk ~30000 kbp long
# I will need to concatenate chunks before pruning (below)
# in srun concatenating (without intermediate headers)
data/var_sites/pass1$ zcat chr1_chunk0.mafs.gz | gzip > chr1.mafs.gz; \
for i in {1..9}; do zcat chr1_chunk$i.mafs.gz | tail -n +2 | gzip >> chr1.mafs.gz; done
# then moving all chunk files to their own subfolder:
data/var_sites/pass1$ mv chr1_chunk* chr1_chunks/

# made a python script to prune sites to approximately unlinked (no more than 1 SNP per 10kb).
# which I wrapped in a bash script pruneFixed.sh
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-9 pruneFixed.sh
Submitted batch job 20420257
~/hilo/scripts$ sbatch --array=1,10 pruneFixed.sh
Submitted batch job 20603410
# After pruning, I combine across all chromosomes and remove all header (and unzip/decompress);
# this leaves ~170k SNPs.
~hilo/data/var_sites/pass1$ mkdir pruned_positions
~/hilo/data/var_sites/pass1$ for i in {1..10}; do zcat chr$i.pruned.mafs.gz | tail -n +2 | cut -f 1-4 >> pruned_positions/all.positions; done
# then I divide into chunks, each containing 250 SNPs, for parallel analysis in ANGSD
~/hilo/data/var_sites/pass1/pruned_positions$ split all.positions -d -a 3 -l 250 positions_chunk

# next find samtools genotype likelihood for those sites across all individuals.
# made (very!) small test file first - 2 positions
~/hilo/data/var_sites/pass1/pruned_positions$ head positions_chunk000 -n 2 > positions_chunk999
# ran test file on slurm
~/hilo/scripts$ sbatch --array=999 -t 1:00:00 calcGL4Pruned.sh
Submitted batch job 20652316

# now running all 677 chunks
~/hilo/scripts$ sbatch --array=0-677 calcGL4Pruned.sh
Submitted batch job 20652343

# and then combine output (in srun)
~/hilo/data/geno_lik/pass1/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..677}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# and running NGSadmix
~/hilo/scripts$ sbatch runNGSadmix.sh
Submitted batch job 20781908
Mon Jun 11 22:36:54 PDT 2018

# modified NGSadmix to use array task ID number as # subpops (K)
# running with same SNPs for K = 3 & 4
~/hilo/scripts$ sbatch --array=3,4 runNGSadmix.sh
Submitted batch job 20803690
Submitted batch job 21671940 -- accidentally resubmitted job and re-ran files (completed & should be same output)

# I divide the genome into manageable 50Mb chunks based on .fa.fai reference file [comp. Barbara; scp results to Farm]
hilo/scripts$ mkdir -p ../data/refMaize/divide_50Mb
hilo/scripts$ python divRefRegions.py 50000000 ../data/refMaize/Zea_mays.AGPv4.dna.chr.fa.fai ../data/refMaize/divide_50Mb
# save a file with a list of regions: chr, start, end, region #, regions file name
~/hilo/data/refMaize/divide_50Mb$ for i in {0..46}; do awk -v i=$i 'gsub(":|-", "\t", $0){print $0"\t"i"\t""region_"i".txt"}' region_$i.txt >> ALL_regions.list; done

# repeat for even smaller regions -- 5Mb (created region _0 to 425)
ecalfee@bigmem3:~/hilo/scripts$ mkdir -p ../data/refMaize/divide_5Mb
ecalfee@bigmem3:~/hilo/scripts$ python3 divRefRegions.py 5000000 ../data/refMaize/AGPv4.fa.fai ../data/refMaize/divide_5Mb
scripts$ cd ../data/refMaize/divide_5Mb; for i in {0..425}; do awk -v i=$i 'gsub(":|-", "\t", $0){print $0"\t"i"\t""region_"i".txt"}' region_$i.txt >> ALL_regions.list; done


# new bam lists:
# data/HILO_IDs_cov_pass1.csv is created in plot_global_ancestry_NGSadmix & saves the # reads that pass filtering + estimated coverage = (# reads * 150bp/read)/2.3Gb as well as meta data on pops and HILO_id as well as IndN for pass1 & GL_beagle_file_ID (skips unaligned ind's)
# sympatric populations only, and individuals must have bam & est. coverage > 0.05
hilo/data$ awk '$1 != "n" && $3 != 20 && $3 != 22 && $3 != 33 && $5 != "NA" && $6 > 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlySympatric.list

# allopatric mexicana only, no minimum coverage
hilo/data$ awk '$1 != "n" && ($3 == 20 || $3 == 22 || $3 == 33) && $5 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlyAllopatricMex.list

# excluded ind's (fit into neither category above)
hilo/data$ awk '$1 != "n" && ($5 == "NA" || ($3 != 20 && $3 != 22 && $3 != 33 && $6 <= 0.05)) {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.excluded.list

# now finding new set of variant sites using ANGSD for only sympatric individuals (all)
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdSymp.sh
Submitted batch job 20811827
# everything ran but region_0 doesn't appear to be properly gzipped and can't be opened
# so I am re-running just that region (problem was I wrote over just this file accidentally with test run of callVarAngsdAllo.sh)
~/hilo/scripts$ sbatch --array=0 callVarAngsdSymp.sh
Submitted batch job 20816879

# when I filter for low LD I can try different filters for nInd and MAF (more stringent)
# to maximize information across individuals for the LD

# now need to find all variant sites between reference and allopatric teosinte at >=30% freq.
# Then find all variant sites between reference and allopatric maize at >=30% freq .. put them together and exclude sites with less than a cutoff # individuals and allele frequency difference between mexicana & maize of 40%
# not quite working -- look at slurm log
~/hilo/scripts$ sbatch --array=0 callVarAngsdAllo.sh
# fixed and after another small typo false start now running all regions
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdAllo.sh
Submitted batch job 20817741

# added script to prune based on fixed cM width and Ogut_2015 map, rather than fixed bp (0.01 cM ~ 10kb on avg but varies across genome)
pruneFixedcM.py
# running with two diff. thresholds for minimum # individuals, 75 and 100: (loads module anaconda3 for use of pandas)
~/hilo/scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20930024
# did not work -- fixed error and added a way to set minInd as an evironmental variable and run each chromosome separately
# (can't break up further b/c of min cM across regions w/in chromosomes)
# running with default MIN_IND=75
scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20950468
# running again with MIN_IND=100
~/hilo/scripts$ sbatch --export=MIN_IND=100 pruneFixedcM.sh
Submitted batch job 20950469

# Thu Jun 21 23:44:06 PDT 2018
# out-of-order markers in the Ogut 2015 map need to be solved -- started process with clean_ogut_2015_map.R
# from Jeff: "Map and/or assembly error; Snp is in correct spot physically but stuck with mapping position
# assigned using bad genome" so I will drop all bad SNPs -- note they are clustered (e.g. on chr7 and affect ~3-5% markers)
# for now will use fixed BP pruning of sites until map issues are solved:

# updated pruneFixedBP.py from pruneFixed.py to take in multiple files and filter for MAF and number of ind's with data
# running w/ default min. ind. = 75 and separately w/ min. ind. = 100:
~/hilo/scripts$ sbatch pruneFixedBP.sh
Submitted batch job 21645706
~/hilo/scripts$ sbatch --export=MIN_IND=100 pruneFixedBP.sh
Submitted batch job 21645898
# both completed within a few minutes, pruning down to ~115k and ~46k
# I will start analysis with the 46K SNPs here at least 100 individuals have some data/reads

# breaking down 46k snps into chunks for beagle likelihood in angsd
~/hilo/data/var_sites/pass1/sympatric/pruned/minN100$ mkdir position_chunks
# combine and unzip; extract relevant columns (there are no headers .. so no need to take tail)
$ for i in {1..10}; do zcat thin10kb_chr$i.mafs.gz; done | cut -f 1-4 > all.positions
# split into chunks
$ split all.positions -d -a 3 -l 250 position_chunks/chunk
# run ANGSD - modified calcGL4Pruned.sh to take in variables as needed
# note: must set ALL variables separated by commas to overrule one variable
scripts$ sbatch --export=OUT_DIR=geno_lik/pass1/sympatricVar/pruned_chunks,IN_DIR=var_sites/pass1/sympatric/pruned/minN100/position_chunks/,BAM_LIST=pass1_bam.all.list \
--array=000-184 calcGL4Pruned.sh
Submitted batch job 21659878

# put it together into one GL file for NGSadmix
data/geno_lik/pass1/sympatricVar/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..184}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# run new SNPs in NGSadmix for K=2,3,4 (modified runNGSadmix.sh to take in variables for IN and OUT files):
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/sympatricVar/pruned_all.beagle.gz,OUT_DIR=NGSadmix/pass1/sympatricVar/allInd/ runNGSadmix.sh

Submitted batch job 21672755

# pulling out genotype likelihood data for NGSadmix for just allopatric teosinte and sympatric maize
# to see how well it differentiates these groups
# first I get the GL header for all individuals in pass1: ~/hilo/data/pass1_bam.all.GLheader
# then I use R to extract the column # for individuals I am interested in: findGLColN.R
# saved in file data/pass1_bam.SympMaizeAlloMex05.GLcolN
~/hilo/data/geno_lik/pass1/sympatricVar$ zcat pruned_all.beagle.gz | cut -f$(cat ../../../pass1_bam.SympMaizeAlloMex05.GLcolN) | gzip >> pruned_SympMaizeAlloMex05.beagle.gz
# and re-running in NGSadmix:
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/sympatricVar/pruned_SympMaizeAlloMex05.beagle.gz,OUT_DIR=NGSadmix/pass1/sympatricVar/SympMaizeAlloMex05/ runNGSadmix.sh
Submitted batch job 21673267

# and same for first set of SNPs (globally ascertained including low cov. ind's and allopatric teosinte)
~/hilo/data/geno_lik/pass1$ zcat pruned_all.beagle.gz | cut -f$(cat ../../pass1_bam.SympMaizeAlloMex05.GLcolN) | gzip >> pruned_SympMaizeAlloMex05.beagle.gz
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/pruned_SympMaizeAlloMex05.beagle.gz,OUT_DIR=NGSadmix/pass1/SympMaizeAlloMex05/ runNGSadmix.sh
Submitted batch job 21673279

# getting a full set of SNPs with MAF > 0.05 freq in all sequenced ind's
# modified allVarAngsdGL.sh to get a count-based MAF and also output SAMtools style GL file
scripts$ sbatch allVarAngsdGL.sh
Submitted batch job 21690039
# some failed due to time-out at 36 hours (re-doing):
scripts$ sbatch --array=0,21,26,31,37 --mem=20G -t 108:00:00 allVarAngsdGL.sh
Submitted batch job 22351904
# region 0 failed due to insufficient memory at t = 12hrs; all others completed (not sure why they now took < 36hrs)
scripts$ sbatch --array=0 --mem=40G -t 96:00:00 allVarAngsdGL.sh
Submitted batch job 22502000
# now completed in 9 hrs
# concatenating all beagle gz files together for one genome-wide GL file:
# takes too long to do interactively:
scripts$ sbatch catBeagleGL.sh
Submitted batch job 22757237
# taking just every 100th position/line: _pruned_by100
scripts$ sbatch catBeagleGL_nth.sh
Submitted batch job 22757239
# -ISSUE IS THAT THIS OMITS THE FIRST HEADER! - FIXED. Creating file by1000.
scripts$ sbatch catBeagleGL_nth.sh
Submitted batch job 23015476
# Due to typo had to fix header here too:
(re-add header due to misslabelling issue)
hilo/data/geno_lik/pass1/allVar$ zcat whole_Genome_pruned_by1000.beagle.gz | gzip > pruned_by1000.beagle.gz; zcat whole_genome_pruned_by1000.beagle.gz | gzip >> pruned_by1000.beagle.gz


# I need to understanding if mapping bias is a problem -- what does mapping look like across groups (e.g. % mapped)? Is there biased mapping across my SNP sets?
# first I see if there are large differences in % reads that do not map
# I create a script to count the reads that align (map at all), do not map, pass mapQ >=30 and pass de-duplication
# SKIP THIS PART -- BETTER/FASTER TO USE data/filtered_bam/pass1.all.metrics.Nreads and pass1.all.metrics.raw.Nreads
scripts$ sbatch countReadsFiltered.sh
Submitted batch job 22758110
# saves output in text file, e.g. countReads/hilo_40.counts
# re-running files that TIMEOUT -- more time and less memory allotment
scripts$ sbatch --array=12,17-24,27-29,37,39,40,44-46,48-51,56-58,60-68,71,82,86-98,101,103-105,107,109-114,116-125,127-129,133,135-137,139,141,142,154,155,161,165,168-172,174-178,182,188,200 countReadsFiltered.sh
Submitted batch job 22765848
# still had many jobs cancel due to time out at 4 hours (!) - bumped up to 12 hours
scripts$ sbatch -t 12:00:00 --array=12,17-24,27-29,39,40,45-46,49-51,58,60-61,71,87-98,101,103-105,107,109-114,116-125,127,129,133,135-137,139,141,142,154,155,161,165,168-172,174-178,182,188,200 countReadsFiltered.sh
Submitted batch job 22799632
# still many jobs cancelled -- make new script that uses samtools built in view -c for 'counts' and trying again - countReadsFiltered2.sh
scripts$ sbatch countReadsFiltered2.sh
Submitted batch job 22937206  - I CANCELLED
# not running due to priority -- submitted other job
scripts$ sbatch -p med -t 24:00:00 countReadsFiltered2.sh
Submitted batch job 22967048 - I CANCELLED (SEE ABOVE FOR ALTERNATIVE FILES GENERATED IN filtered_bam/

# I can also zoom in and look at individuals' depth across specific regions:
~/hilo/data/filtered_bam$ samtools depth -r 1:400-500 hilo_18.sort.dedup.bam hilo_19.sort.dedup.bam


# Now I am working on getting a PCA using PCAngsd. This is better than NGStools PCA option because NGStools puts a prior on one population allele frequency -- so low coverage individuals end up near the center, whereas PCAngsd uses individually estimated allele frequencies (somehow without assuming they are indep.).
# to install python2.7 dependencies, I set up a python virtual environment for python 2.7.14 on my user on farm:
~/hilo$ module load python
~/hilo$ virtualenv --no-site-packages venv
# the --no-site-packages option is necessary to avoid conflicts between global packages and the ones in my venv
# activate virtual environment (need to add to top of any of my code now using this venv)
~/hilo$ source venv/bin/activate
~/bin/pcangsd$ pip install numpy Cython enum
~/bin/pcangsd$ pip install -r python_packages.txt
# problem installing numba due to llvm:
RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config
# install llvm:
wget http://releases.llvm.org/6.0.0/clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
~/software$ tar xf clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
(venv) ecalfee@bigmem2:~/hilo$ export LLVM_CONFIG=/home/ecalfee/software/clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04/bin/llvm-config
# based on online forum
~/hilo$ pip install funcsigs
# NONE OF THIS WORKED -- likely clang vs. gcc compiler issues (!)
# alternative route -- I've installed anaconda2
# to use, first set path to anaconda2 files:
export PATH=/home/ecalfee/software/anaconda2/bin:$PATH
# this time only, I create a new conda environment:
~/hilo$ conda create -y -n condaEnv
# every time, I need to activate the condaEnv:
~/hilo$ source activate condaEnv
# to this environment, I install llvmlite
(condaEnv) ecalfee@c8-62:~/hilo$ conda install -y llvmlite
# when I open python2, I can import all the pcangsd dependencies: numpy, scipy, pandas, numba
# so they appear to be pre-installed
# this example file now runs -- great!
(condaEnv) ecalfee@c8-62:~/hilo/data/TEST$ python2 ~/bin/pcangsd/pcangsd.py -beagle testANGSD.beagle.gz -threads 1 -iter 100 -o TESTpcangsd

# running PCA on set of pruned SNPs
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/pruned_all runPCAngsd.sh
Submitted batch job 22757232
# added -admix option and re-did:
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/pruned_all --mem=1G -t 3:00:00 runPCAngsd.sh
Submitted batch job 22758865
# and running PCA on set of pruned SNPs that vary within sympatric populations maize/teosinte
~/hilo/scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/sympatricVar/pruned_all runPCAngsd.sh
Submitted batch job 22757408
# added -admix option and re-did:
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/sympatricVar/pruned_all --mem=1G -t 3:00:00 runPCAngsd.sh
Submitted batch job 22758855
WORKED
# will also run PCA on data for all hilo individuals, at all SNPs in 'allVar'
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome --mem=2G -t 96:00:00 runPCAngsd.sh
Submitted batch job 22762726
CANCELLED-EXCEEDED MEMORY LIMIT. RE-RUNNING:
sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome --mem=12G -t 96:00:00 runPCAngsd.sh
Submitted batch job 22766166
# subset of all SNPs in 'allVar'
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome_pruned_by100 --mem=1G -t 36:00:00 runPCAngsd.sh
Submitted batch job 22762723
ERROR-DID NOT ACCEPT REDUCED FILE BY 100 .. NOT SURE WHY .. PROBLEM IS THAT IT'S MISSING A HEADER - I CAN FIX THAT:
hilo/data/geno_lik/pass1/allVar$ zcat whole_genome.beagle.gz | head -n 1 | gzip > whole_genome_every_100th_pos.beagle.gz; zcat whole_genome_pruned_by100.beagle.gz | gzip >> whole_genome_every_100th_pos.beagle.gz
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome_pruned_by100 --mem=12G -t 36:00:00 runPCAngsd.sh
# the above file also won't run; trying again with ~100K SNPs from
# create SNPs using catBeagleGL_nth.sh -- takes every 1000th site
# run PCAngsd
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/pruned_by1000.beagle.gz --mem=4G -t 16:00:00 runPCAngsd.sh
Submitted batch job 23021131 - CANCELLED (error in file name)
Submitted batch job 23029600 - CANCELLED
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/pruned_by1000 --mem=2G -p med -t 16:00:00 runPCAngsd.sh
Submitted batch job 23029537 - RAN, TOOK VERY LITTLE TIME (<10min and <1G)

# I will also estimate pairwise Fst between each group (allopatric/sympatric/maize/mex) and between populations within each larger group.
# to do Fst, I first need to calculate allele freq. spectrum for each population -- so I separate them into population bam files
# each meta population (no filtering for coverage except skipping files with no alignment in pass1)
~/hilo/data$ awk '$5 == "maize" && $6 == "sympatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/maize.symp.list ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ awk '$5 == "maize" && $6 == "sympatric" && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/maize.symp.list
~/hilo/data$ awk '$5 == "mexicana" && $6 == "sympatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/mexicana.symp.list ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ awk '$5 == "mexicana" && $6 == "sympatric" && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/mexicana.symp.list
~/hilo/data$ awk '$5 == "mexicana" && $6 == "allopatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/mexicana.allo.list ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ awk '$5 == "mexicana" && $6 == "allopatric" && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/mexicana.allo.list
# give these numerical copies for ease: ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
~/hilo/data/pass1_bam_pops$ cp maize.symp.list pop1000.list
~/hilo/data/pass1_bam_pops$ cp mexicana.symp.list pop2000.list
~/hilo/data/pass1_bam_pops$ cp mexicana.allo.list pop3000.list
# each individual population
~/hilo/data$ for i in $(awk '$1 != "n" && $7 != "NA" {print $3}' HILO_IDs_cov_pass1.csv | sort | uniq); do awk -v i=$i '$3 == i && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/pop$i.list; done ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ for i in $(awk '$1 != "n" && $11 >= 0.05 {print $3}' pass1_ids.txt | sort | uniq); do awk -v i=$i '$3 == i && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/pop$i.list; done

# all steps to SFS are listed here: http://www.popgen.dk/angsd/index.php/Fst
# should use many cores for step 2 and need to download realSFS software (actually it's part of ANGSD version already on farm)

# step 1: calculate individual site allele frequencies for each pop
scripts$ sbatch calcSAFAngsd.sh
Submitted batch job 21693057
# re-doing the 2 that didn't finish (time out cancellation):
scripts$ sbatch --mem=20G --array=1000,2000 -t 108:00:00 calcSAFAngsd.sh
Submitted batch job 22356436 -- will not finish (way too long)
# also an error with angsd -- too low a version, so I updated (see error & update log below)
# re-doing SAF split by chromosome:
# first for groups of pops, needs lots of memory and time:
~/hilo/scripts$ for i in 1000 2000 3000; do sbatch --export=POP=$i calcSAFAngsd_byChr.sh; done
Submitted batch job 22625518
Submitted batch job 22625519
Submitted batch job 22625520
# then for all individual pops, requires less memory. First put popN in a list:
~/hilo/scripts$ for i in {18..31} {33..35} {360..363} {365..374}; do echo $i >> ../data/pass1_bam_pops/all_pops.numbers; done
~/hilo/scripts$ for i in $(cat ../data/pass1_bam_pops/all_pops.numbers); do sbatch --mem=15G -t 24:00:00 --export=POP=$i calcSAFAngsd_byChr.sh; done
Submitted batch job 22628540-22628570
# re-submitting 11 jobs that ran out of time (modified script for more threads; less memory per thread):
sbatch --export=POP=22 --array=1 calcSAFAngsd_byChr.sh
Submitted batch job 22764908
sbatch --export=POP=20 --array=1-10 calcSAFAngsd_byChr.sh
Submitted batch job 22764942

# concatenate results by chromosome back together (skipping pops 20, 22 and larger groups for now):
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.sh
Submitted batch job 22765120
# fixed typo with --outname vs -outname & reran:
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.sh
Submitted batch job 22800872 -- DIDN'T WORK. fixed true typo -outnames vs. -outnames & order & reran:
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.s
Submitted batch job 22801018

# step2: for each pair of populations, calculate the 2D SFS prior: (this is a lot of Fst calculations!)
s.t. like hilo$ for i in {18..31} {33..35} {360..363} {365..374}; do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; then echo $i $j; fi; done; done
# problem: getting the following error:
realSFS print TEST/pop31.pop22.old.fst.gz
	-> Version of fname:TEST/pop31.pop22.old.fst.gz is:0
	-> Looks like you are trying to use a version of realSFS that is incompatible with the old binary output from ANGSD
	-> Please use realSFS.old instead (or consider redoing the saf files )
	-> Will exit
  # updating ANGSD on farm:
  ecalfee@bigmem2:~/bin$ module load zlib
  Module zlib/1.2.8 loaded
  ecalfee@bigmem2:~/bin$ module load htslib
  Module htslib/1.3.2 loaded
  # download newest angsd official release and accompanying htslib:
  ~/bin$ wget http://popgen.dk/software/download/angsd/angsd0.920.tar.gz
  # unzip
  ~/bin$ tar xf angsd0.920.tar.gz
  # install htslib
  ~/software/htslib$ make install prefix=../bin
  # install angsd using link to my local htslib directory
  ~/software/angsd$ make HTSSRC=../htslib/
  # moved exec. file software/angsd/angsd to bin/ and software/angsd/misc/realSFS to bin/ too and v1.6 of tabix and htsfile too
# to calculate Fst between population pairs:
# running now for all pop pairs except 20 & 22
scripts$ for i in 18 19 21 {23..31} {33..35} {360..363} {365..374}; do for j in 18 19 21 {23..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; then sbatch --export=POP1=$i,POP2=$j calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 22800219-22800624
# NOTE : There has to be a better way to organize these jobs into one 'super job'
scripts$ for i in {18..31} {33..35} {360..363} {365..374}; \
do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; \
then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 23021289-23021760
4 jobs not running -- requeue-ing 23021318-21 from c10-92:
scripts$ sbatch --export=POP1=18,POP2=274,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 calc2DSFSAngsd.sh
Submitted batch job 23026718
scripts$ for j in {20..22}; \
do sbatch --export=POP1=19,POP2=$j,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 calc2DSFSAngsd.sh; done
Submitted batch job 23026842-4

# now calculating Fst pairwise:
scripts$ for i in {18..31} {33..35} {360..363} {365..374}; \
do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; \
then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 -x c10-92 calcFSTAngsd.sh; fi; done; done
Submitted batch job 23026944-23027416 - ALL DONE - rerunning ones missing fst output files:
scripts$ sbatch --export=POP1=18,POP2=19,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031530 - hung up
Submitted batch job 23031925
scripts$ sbatch --export=POP1=18,POP2=30,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031543
scripts$ sbatch --export=POP1=20,POP2=370,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031546
scripts$ sbatch --export=POP1=22,POP2=26,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031554
scripts$ sbatch --export=POP1=27,POP2=362,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031559
scripts$ sbatch --export=POP1=29,POP2=360,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031568
scripts$ sbatch --export=POP1=361,POP2=362,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031574
scripts$ sbatch --export=POP1=362,POP2=363,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031581

# combine Fst results into one file:
hilo/data/SAF/pass1/N1000.L100.regions$ echo -e "pop1 \t pop2 \t Fst_Hudson \t Fst_avg_of_ratios " \
> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.fst.stats
hilo/data/SAF/pass1/N1000.L100.regions$ for i in {18..31} {33..35} {360..363} {365..374}; \
do for j in {18..31} {33..35} {360..363} {365..374}; \
do if [ $j -gt $i ]; then awk -v i=$i -v j=$j '{print i "\t" j "\t" $1 "\t" $2}' \
pop$i.pop$j.fst.stats >> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.fst.stats; fi; done; done

# just submitting pairs for 2D-SFS between larger groups
scripts$ for i in 1000 2000 3000; do for j in 1000 2000 3000; do if [ $j -gt $i ]; \
then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 23013831-3 - COMPLETED

# and then calculating pairwise Fst using Hudson/Bhatia2013 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3759727/) estimator = 1 (vs. Reynolds estimator = 0). 3rd/4th column in the .fst.idx file are the numerator & denominator of the statistic at each locus (see github discussion/documentation: https://github.com/ANGSD/angsd/issues/61).
scripts$ for i in 1000 2000 3000; do for j in 1000 2000 3000; do if [ $j -gt $i ]; then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; fi; done; done
Submitted batch job 23014153-5
# I will use 'weighted' Fst (ratio of averages ... vs unweighted = average of ratios across loci).
# creating a summary file of weighted Fst between groups
hilo/data/SAF/pass1/N1000.L100.regions$ echo -e "pop1 \t pop2 \t Fst_Hudson \t Fst_avg_of_ratios " > ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.group.fst.stats
hilo/data/SAF/pass1/N1000.L100.regions$ for i in 1000 2000 3000; do for j in 1000 2000 3000; \
do if [ $j -gt $i ]; then awk -v i=$i -v j=$j '{print i "\t" j "\t" $1 "\t" $2}' \
pop$i.pop$j.fst.stats >> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.group.fst.stats; fi; done; done


# This whole Fst estimation should be done with a set of regions. For SAF calculations, I can provide a set of sites or a region
TEST$ angsd sites index sites_5.txt
TEST$ realSFS print pop18.saf.idx -sites sites_5.txt
TEST$ realSFS print pop18.saf.idx -r 1:500-600
# but a regions file doesn't work
TEST$ realSFS print pop18.saf.idx -rf regions_5.txt - error
# I may be able to either loop over a regions file in a short script, and cat together the SAF's in the end
# or to supply an -rf file to angsd when making the SAF file
# the following works, where regions.txt has regions e.g. 1:100-200 listed
*regions must be sorted!!
data$ angsd -out TEST/SAFbyRegion_test_pop18 -anc /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa -fold 1 -rf TEST/regions.txt -bam pass1_bam_pops/pop18.list -remove_bads 1 -minMapQ 30 -GL 1 -dosaf 1

# Getting a set of random regions from the maize reference genome:
# bedtools random function will work, but I need to make a 'genome' file with chromosome names & lengths first:
hilo/data$ cut -f1,2 /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa.fai | grep -v 't' > refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths
# now generating 1000 random regions of 100bp each: data/refMaize/random_regions/N1000.L100.regions
scripts$ sbatch getRandomRegions.sh
Submitted batch job 22934334
# and calculating SAF off of these regions using an updated calcSAFAngsd.sh script:
scripts$ sbatch --array=18-31,33-35,360-363,365-374 --mem=4G calcSAFAngsd.sh
Submitted batch job 22934857
scripts$ sbatch --array=1000,2000,3000 --mem=12G -t 24:00:00 calcSAFAngsd.sh
Submitted batch job 22934957

# made new R script to visualize population structure and admixture in my sample - PCA, Fst and mapping to maize for allopatric & sympatric teosinte & sympatric maize
# scripts/eval_PCA_Fst.R
# need to add in allopatric maize. One potential option is downloading HapMap3 bams, a subset of which are 282, but I'm having trouble identifying tropical/subtropical TS lines from the large # in hapmap3 https://cbsusrv04.tc.cornell.edu/users/panzea/filegateway.aspx?category=Genotypes
# to access iplant from farm:
module load icommands
iinit (then follow suggestions by: https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands)
other icommands (e.g. ils, icd): https://pods.iplantcollaborative.org/wiki/display/DS/Using+iCommands
# then use crossmap http://crossmap.sourceforge.net/ and a chain to lift over from v3 to APGv4 reference genome:
ftp://ftp.ensemblgenomes.org/pub/plants/release-32/assembly_chain/zea_mays/ (chain)
Another option is to get original reads from NCBI <<but this is a maze of nonexistent plant samples https://www.ncbi.nlm.nih.gov/biosample/7209658 >>
# the 282 vcf v4 is on farm: /group/jrigrp/Share/genotypes/282_7X/c${i}_282_corrected_onHmp321.vcf.gz
# these have GL's for SNPs that vary within this sample, GT:AD:GL e.g.
# 0/1:7,2:28,0,167 means 0/1 is the called genotype; 7 ref allele & 2 alt allele counts; 28,0,167 are the genotype likelihoods
##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
##FORMAT=<ID=AD,Number=.,Type=Integer,Description="Allelic depths for the reference and alternate alleles in the order listed">
##FORMAT=<ID=GL,Number=.,Type=Integer,Description="Genotype likelihoods for 0/0, 0/1, 1/1, or  0/0, 0/1, 0/2, 1/1, 1/2, 2/2 if 2 alt alleles">

# making new script to calculate coverage per individual for variant sites in HILO pass1:
scripts$ sbatch calcDepthCovSites.sh
Submitted batch job 23079251 - fixed typo & reran:
Submitted batch job 23079342 - CANCELLED - OUT OF MEMORY > 20g/3nodes at chr2
hilo/scripts$ sbatch calcDepthCovSites.sh - new edits for more nodes, and arrayed by chromosome
Submitted batch job 23505206 - COMPLETED; results files split by CHROM. CHR1 did not finish.
hilo/scripts$ sbatch --mem=30G --array=1 calcDepthCovSites.sh
Submitted batch job 23778351
scripts$ sbatch calcDepthCovRegions.sh
Submitted batch job 23079256 - COMPLETED

# 282 panel doesn't have easy aligned bams -- GL for SNPs is preserved in vcf file. The issue is whether there is significant bias between 282 SNPs and SNPs from admix panel, e.g. fixed differences. If there is, I will have to re-align fastq from 282. Checking SNP overlap:
scripts$ sbatch checkSNPOverlap282.sh

# I am adding in bams for allopatric Maize from Anne: 4 landrace populations, 10 ind's each (same pop = sequentially together)
# first I make a list of the bam file locations on farm
ecalfee@farm:/group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK$ for i in $(ls *removedup_realigned.bam); do echo /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/$i >> ~/hilo/data/bam.maize4popLowland.list; done
# and a combined list for calling new SNPs
ecalfee@farm:~/hilo/data$ cat pass1_bam.all.list bam.maize4popLowland.list > merged_bam.pass1_all.maize4popLowland.list
# Then I get GL at variant sites across the combined sample, now including lowland maize:
scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list allVarAngsdGL.sh
Submitted batch job 23526847 -- Did not run: Mt and Pt chromosomes not in bams from Anne (allopatric maize);
# changed allVarAngsdGL.sh to have a new variable to not check bam headers for same chromosomes;
# ok because I'm restricting analysis to the shared autosomes 1-10 -- THIS WAS A BAD IDEA -- IT'S NOT FINDING THE CORRECT CHROMOSOMES (!) -- MANY SNPs @ 50% freq.
scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23551911 -- ugh, typo (now fixed)
scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23552475 - ERROR IN FILE NAMES (1 instead of 01..)
# creating symlinks for all bams from anne lorant 4 pop lowland maize so that they are local and named consistently
# and I can re-index
# creating symbolic link for #1
~/hilo/data/alloMaize4pop_symlink_bam$ ln -s /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/JRIAL11-1_removedup_realigned.bam maizeLow_01.bam
# and for the rest:
for i in {02..40}; do ln -s /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/JRIAL11-"$i"_removedup_realigned.bam maizeLow_"$i".bam; done
# remaking files with list of bams for lowland
data$ for i in {01..07} {09..40}; do echo alloMaize4pop_symlink_bam/maizeLow_$i.bam >> bam.maize4popLowland.list; done
data$ cat pass1_bam.all.list bam.maize4popLowland.list > merged_bam.pass1_all.maize4popLowland.list
ecalfee@bigmem1:~/hilo/scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23553309 - did not run. remade symlinks wiht proper "":
Submitted batch job 23553370 - took out 08 from alloMaize4popLow:
Submitted batch job 23553429 - RUNNING; I cancelled (needs index files to run)
# indexing allopatric maize symlinks
~/hilo/data/alloMaize4pop_symlink_bam$ (!) I put this into a script samtools index maizeLow_"$i".bam
scripts$ sbatch indexAllo4PopMaize.sh
Submitted batch job 23553496 - Fixed typo below:
Submitted batch job 23554317 - COMPLETED
# now running GL again:
~/hilo/scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23563805 -- TOO MEMORY INTENSE -- SOME ARE COMPLETE BUT OTHERS NOTE EVEN CLOSE -- WILL NEVER FINISH.
ALTERNATIVES:
# (1) I find variable sites first (and then will calculate GL for a subset)
# (2) I split the genome into a large number of regions across multiple files (approximately retrieving 1/1000th of the maize total genome)
~/hilo/scripts$ sbatch getRandomRegions_andSplit.sh
Submitted batch job 23570489 - FIXED TYPO:
Submitted batch job 23571407
# after I will use these regions files to create GL files for each set of regions.

# I ALSO NEED TO ADDRESS THIS ERROR -- THERE ARE MANY MANY SNPs at > 50% freq; I THINK THE BAM HEADERS ARE INDEED A PROBLEM, OFFSETTING BY 1 CHRM.
# error in file: Trying to access fasta efter end of chromsome+200:9/9 pos=181122463 ref_len=159769782
"geno_lik/merged_pass1_maize4popLowland/allVar/region_42.arg"
# and again in region 38: Trying to access fasta efter end of chromsome+200:8/8 pos=182381402 ref_len=181122637
# I am re-ordering bams using picard tools -- reorderBam4popAlloMaizeLow.sh (starting with just 16/40 individuals, 4 per pop)
~/hilo/scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23571569 - FIXED DIRECTORY TYPO and made scratch directory in my home folder instead of local to each node (lack of space)
Submitted batch job 23576593 -Failed due to lack of 'sequence dictionary'. will make one. and will make a symlink for reference genome too in makeSeqDictRef.sh script:
scripts$ sbatch makeSeqDictRef.sh
Submitted batch job 23576612 - fix typo:
Submitted batch job 23576616 - COMPLETED
# edited reorderBam4popAlloMaizeLow.sh to refer to local reference AGPv4.fa and .dict
~/hilo/scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23576622 -- produced an error and truncated bams

# making a small test file to avoid future errors in the bams/reordering etc.
data/alloMaize4pop_symlink_bam$ samtools view -b maizeLow_01.bam "10:10-20" > maizeLow_99.bam # subset of reads
~/hilo/scripts$ sbatch --array=99 -t 10:00 --mem=1G -p med reorderBam4popAlloMaizeLow.sh
Submitted batch job 23706200 #e.g. run, tried many; produces no error ONLY IF I don't provide a maizeLow_99.bam.bai index file (or maizeLow_99.bai) from samtools
# making slightly larger test region:
data/alloMaize4pop_symlink_bam$ samtools view -b maizeLow_01.bam "10:1000-2000" > maizeLow_98.bam
scripts$ sbatch --array=98 -t 10:00 --mem=1G -p med reorderBam4popAlloMaizeLow.sh
Submitted batch job 23706846


# also making a small test file for a hilo individual
data/TEST$ samtools view -b ../filtered_bam/hilo_23.sort.dedup.baq.bam "10:1000-2000" > hilo_23.subset10.1-2k.bam
data/TEST$ samtools index hilo_23.subset10.1-2k.bam
TEST$ echo hilo_23.subset10.1-2k.bam > test_veryshort_bams.list
TEST$ echo maizeLow_98.reordered.bam >> test_veryshort_bams.list
# ANGSD is able to run with the reordered pair:
~/hilo/data/TEST$ angsd -out test_small_maizeLow_hilo -ref ../refMaize/AGPv4.fa \
-bam test_veryshort_bams.list -remove_bads 1 \
-minMapQ 30 -minQ 20 \
-doMajorMinor 2 \
-doCounts 1 -minMaf 0.05 -doMaf 8 \
-GL 1 -doGlf 2

# REORDERING MAIZE_LOW BAMS:
# moved all .bai files to /outdated/ to temporarily save
scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23707049
-- ALL BUT IND01 CANCELLED BECAUSE IT FOUND THE .BAI FILE FOLLOWNG THE SYMLINK
# 01 may be wrong too because it
# I am trying another solution: just giving a new header to Anne's bams addint mt and pt chromosomes (chr10 will still come out of order, first)
~/hilo/data$ mkdir -p alloMaize/maizeLow/re_head # make new directory
~/hilo/data/alloMaize/maizeLow$ samtools view -H /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/JRIAL11-02_removedup_realigned.bam > new_header_with_mt.sam
# edit header with nano, adding these 2 lines:
@SQ	SN:Pt	LN:140384
@SQ	SN:Mt	LN:569630

# modified header of the short bam file
~/hilo/data/alloMaize/maizeLow$ samtools reheader new_header_with_mt.sam ../../alloMaize4pop_symlink_bam/maizeLow_98.bam > re_head/maizeLow_TEST.bam
~/hilo/data/alloMaize/maizeLow/re_head$ samtools index maizeLow_TEST.bam
~/hilo/data/alloMaize/maizeLow/re_head$ mv maizeLow_TEST.bam* ../../../TEST/.
# test ANGSD for full mini files
~/hilo/data/TEST$ echo maizeLow_TEST.bam > TEST_ANGSD_GL_rehead_bams.list
~/hilo/data/TEST$ echo hilo_23.subset10.1-2k.bam >> TEST_ANGSD_GL_rehead_bams.list
~/hilo/data/TEST$ angsd -out TEST_ANGSD_GL_rehead -ref ../refMaize/AGPv4.fa -minMapQ 30 -minQ 20 -remove_bads 1 -doMajorMinor 2 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -bam TEST_ANGSD_GL_rehead_bams.list
-- THE ABOVE WILL NOT WORK -- with or without a region -r listed
# test with the test re-ordered bam
ecalfee@bigmem7:~/hilo/data/TEST$ echo maizeLow_98.reordered.bam > TEST_ANGSD_GL_reorder_bams.list
ecalfee@bigmem7:~/hilo/data/TEST$ echo hilo_23.subset10.1-2k.bam >> TEST_ANGSD_GL_reorder_bams.list
~/hilo/data/TEST$ angsd -out TEST_ANGSD_GL_reorder -ref ../refMaize/AGPv4.fa -minMapQ 30 -minQ 20 -remove_bads 1 -doMajorMinor 2 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -bam TEST_ANGSD_GL_reorder_bams.list
-- THE ABOVE WORKED ON TEST FILES --
# So I am trying reordering again, but where I copy the bams first so they have
# no bai files (and I delete intermediate copied pre-reordering bams at the close of the script)
ecalfee@bigmem7:~/hilo/scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23752487 -- ONLY individual 01 ran without issue. Changed output names & scratch directory (issue was couldn't write to scratch) & running all but ind 01
~/hilo/scripts$ sbatch --array=2-4,11-14,21-24,31-34 reorderBam4popAlloMaizeLow.sh
Submitted batch job 23776475
# copying over file for individual 1 (with index)
~/hilo/data$ cp alloMaize/maizeLow/reordered_01.bam alloMaize4Low/.
~/hilo/data$ cp alloMaize/maizeLow/reordered_01.bai alloMaize4Low/.
~/hilo/data$ rm -r alloMaize/
# made list of new bams
data$ for i in {01..04} {11..14} {21..24} {31..34}; do echo alloMaize4Low/reordered_$i.bam >> alloMaize4Low_16_bam.list; done
# and combined list
data$ cat pass1_bam.all.list alloMaize4Low_16_bam.list merged_bam.pass1_all.alloMaize4Low_16.list
# re-running; delayed to start when the last reorder job finishes
scripts$ sbatch --dependency=afterok:23776475_12 --export=DIR_OUT=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,BAM_IN=merged_bam.pass1_all.alloMaize4Low_16.list allVarAngsdGL.sh
Submitted batch job 23807271 -- long time to make priority : CANCELLED (see below, running on smaller regions)
# To reduce output time and computations I made a new script to just get variant sites by region:
scripts$ sbatch --export=OUT_DIR=var_sites/pass1_alloMaize4Low,BAM_LIST=merged_bam.pass1_all.alloMaize4Low_16.list callVarAngsd_Regions.sh
Submitted batch job 23905396 -- CANCELLED. WILL REWRITE FOR SMALLER TASKS AND RUN ON LOW PRIORITY.
# Got permission to run as 'high priority' on bigmemh. Made 0-425 new regions (each 5Mb) and running with just 1 core
# (no threading) ... and letting 50 tasks run at one time: (!) mistake -- only use 8 cpu's at a time on bigmemh!
ecalfee@bigmem3:~/hilo/scripts$ sbatch -p bigmemh allVarAngsdGL1.sh --Jeff cancelled & restarted
Submitted batch job 24226853
# attempting to rerun those that cancelled due to memory surplus as short jobs on low priority
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=0,1,25,39,75,81,92,93,97,103 --mem=12G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24263414
ecalfee@bigmem3:~/hilo/scripts$ sbatch -p bigmeml --array=0 --mem=24G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24264289 (WHY DOES REGION 0 NEED SO MUCH MEMORY?) -- DOUBLING MEMORY AGAIN:
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=0 --mem=48G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24268274
# and re-running 39. All other 'low priority' tasks finished:
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=39 --mem=24G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24269347 - EXCEEDED MEMORY
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=39 --mem=48G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24271818 - COMPLETED
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=127 --mem=24G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24271599 - COMPLETED
# and cancelled then re-submitted other jobs not yet queued with higher (max-per-node memory = 8G) but lower walltime (3 hrs max)
# will start after this set of 8 completes
ecalfee@bigmem2:~/hilo/scripts$ sbatch --dependency=afterany:24230156 --array=148-425%8 -t 3:00:00 --mem=8G allVarAngsdGL1.sh
Submitted batch job 24263714
# because I accidentally set it to medium priority, I'll get rid of the task limit and put the last 100 on high priority:
scontrol update ArrayTaskThrottle=0 JobId=24263714
scancel 24263714_[325-425] - RAN WITH SOME CANCELLED (Memory/timeout)
ecalfee@bigmem2:~/hilo/scripts$ sbatch --array=325-425%6 -t 3:00:00 --mem=8G -p bigmemh allVarAngsdGL1.sh
Submitted batch job 24272202 - RAN WITH SOME CANCELLED (memory/timeout)
Re-running ones that timed out on bigmemh with longer times and 2x the memory:
~/hilo/scripts$ sbatch --array=324,178,179,181,216,257,262,281%4 -t 6:00:00 --mem=16G -p bigmemh allVarAngsdGL1.sh
Submitted batch job 24378121 - COMPLETED
Re-running ones that were memory short on bigmeml with 2 hours and 3x the memory:
~/hilo/scripts$ sbatch --array=361,358,252,256,298,320 -t 2:00:00 --mem=24G -p bigmeml allVarAngsdGL1.sh
Submitted batch job 24378629 - COMPLETED
# Concatenating results into a file for every 1000th SNP:
scripts$ sbatch -p bigmemh --export=startR=0,endR=425,DIR_GL=data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,N=1000 catBeagleGL_nth.sh
Submitted batch job 24402884 -- ERROR FOR LOOP
Submitted batch job 24402416 -- ERROR BASH VARIABLES
Submitted batch job 24402989 -- ERROR WORKING DIRECTORY
scripts$ sbatch -p bigmemh --export=startR=0,endR=425,DIR_GL=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,N=1000 catBeagleGL_nth.sh
Submitted batch job 24403123
# and finally calculating PCA for this subset of SNPs
scripts$ sbatch -p bigmemh --dependency=afterok:24403123 --export=GL_PREFIX=data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/whole_genome_pruned_every_1000 runPCAngsd.sh
Submitted batch job 24403224 - DONE - RENAMED ALL FILES with .partial. so I can rerun below
# ran but I should re-do PCA etc. because it's missing a 2 segments that didn't complete GL properly:
~/hilo/scripts$ sbatch --array=261,415 -t 6:00:00 --mem=16G -p bigmemh allVarAngsdGL1.sh
Submitted batch job 24414246
scripts$ sbatch -p bigmemh --dependency=afterok:24414246 --export=startR=0,endR=425,DIR_GL=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,N=1000 catBeagleGL_nth.sh
Submitted batch job 24414522
~/hilo/scripts$ sbatch -p bigmemh --dependency=afterok:24414522 --export=GL_PREFIX=data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/whole_genome_pruned_every_1000 runPCAngsd.sh
Submitted batch job 24414553




# FST

# I save the allopatric maize list of bams also as pop 4000:
ecalfee@farm:~/hilo/data$ cp bam.maize4popLowland.list pass1_bam_pops/pop4000.list
# then I calculate SAF to get Fst between this allopatric maize landraces and the sympatric maize/mex and allopatric mex
# calcualte SAF for regions:
scripts$ sbatch --array=4000 -t 10:00:00 --mem=4G calcSAFAngsd.sh
Submitted batch job 23526611 - on hold: (AssocGrpJobsLimit) - did not finish because of lack of index files (fixed see above)
Submitted batch job 23563858 - NODE FAIL (will rerun excluding bigmem5:)
scripts$ sbatch --array=4000 -t 10:00:00 --mem=4G -x bigmem5 calcSAFAngsd.sh
Submitted batch job 23564165 - COMPLETED.
# and pairwise SFS between 4000 and 1000, 2000, 3000:
scripts$ for i in 1000 2000 3000; do sbatch --export=POP1=$i,POP2=4000,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; done
Submitted batch job 23571618-20 - COMPLETED
# then Fst pairwise with 4000 and other population groups:
scripts$ for i in 1000 2000 3000; do sbatch --export=POP1=$i,POP2=4000,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; done
Submitted batch job 23903001-3 - COMPLETED (and not very different from below..it worked fine despite not re-ordered)

# RE-DOING FST after REORDERING BAMS -- there could be mistakes in original above due to comparing
# SAF across individuals with same chromosome names but diff. order (I'm not sure of effect):
# made new bam list 5000 for 16 bams the that I re-ordered (4 per pop from allopatric lowland Maize):
~/hilo/data$ cp alloMaize4Low_16_bam.list pass1_bam_pops/pop5000.list
# calculate SAF for regions:
scripts$ sbatch --array=5000 -t 12:00:00 --mem=4G calcSAFAngsd.sh
Submitted batch job 23902868 - ERROR FROM IND 01. Recopied over reordered bam for this ind & reran:
Submitted batch job 23903106 - I CANCELLED immediately (arguments after script.sh were being ignored)
Submitted batch job 23903114 - COMPLETED
# calculate pairwise SFS between 5000 and 1000, 2000, 3000
scripts$ for i in 1000 2000 3000; do sbatch --dependency=afterok:23903114 --export=POP1=$i,POP2=5000,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; done
Submitted batch job 23903227-9 -COMPLETED
# then calculate pairwise Fst between all groups
scripts$ for i in 1000 2000 3000; do sbatch --dependency=afterok:23903227,23903228,23903229 --export=POP1=$i,POP2=5000,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; done
Submitted batch job 23903329-31 - COMPLETED
# add results to summary file
~/hilo/data/SAF/pass1/N1000.L100.regions$ for i in 1000 2000 3000; do awk -v i=$i '{print i "\t" 5000 "\t" $1 "\t" $2}' pop$i.pop5000.fst.stats >> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.group.fst.stats; done

# MAPPING AND COVERAGE METRICS -- I moved the first plots on mapping out of eval_PCA_Fst.R to
# their own script: plot_mapping_metrics.R . Then I added othere coverage analyses
# for (1) Fst regions
# and (2) a subset of variant sites called in hilo;
# var_sites/pass1/pruned_positions/all.positions (~170k SNPs separated by >= 10kb; used in initial admixture analysis; with over 100 individuals with some data (MORE than cutoff for other SNPs)).

# Adding 282 reference maize data
# Getting 282 SRAs for download from NCBI - ACTUALLY michelle will align 282 'ts' tropical/subtropical bams for me
hilo/data/maize282$ wget -O maize282_SRA_SRP108889 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&db=sra&rettype=runinfo&term="SRP108889"[All Fields]'
# made symlinks for vcf files (variants re-possitioned onto genome AGPv4):
~/hilo/data/maize282/vcf_AGPv4$ for i in {1..10}; do ln -s /group/jrigrp/Share/genotypes/282_7X/c${i}_282_corrected_onHmp321.vcf.gz chr$i.vcf.gz; done
# new script to make smaller vcf's with just thinned SNPs for PCA from 282 panel
scripts$ sbatch -p bigmemh --array=1-10%6 subset_282var_nth.sh
Submitted batch job 24423018 -- ERROR (FIXED & RERUNNING BELOW)
Submitted batch job 24484991
# now pulling out a sites file to use in angsd which has chr, pos, major, minor alleles
~/hilo/data/maize282/vcf_AGPv4$ for i in {1..10}; do grep -v "#" subset_chr$i.recode.vcf | awk '{print $1"\t"$2"\t"$4"\t"$5}' > subset_chr$i.sites; done
# and making a matching regions file that just has chr:pos for every site
~/hilo/data/maize282/vcf_AGPv4$ for i in {1..10}; do grep -v "#" subset_chr$i.recode.vcf | awk '{print $1":"$2}' > subset_chr$i.regions; done
# now getting genotype likelihoods for all hilo individuals and lowland maize 16 individuals for these sites
# using samtools GL method:
~/hilo/scripts$ sbatch --array=1-10%4 -p bigmemh calcGL4maize282sites.sh
Submitted batch job 24490030 - FIXED QUOTES AROUND VARIABLES:
Submitted batch job 24490494 - MORE QUOTE FIXES:
Submitted batch job 24490702 - NEEDED DOUBLE BACKSLASH \\
Submitted batch job 24490876 - MORE BACKSLASH
Submitted batch job 24491053 - TO FORWARD SLASH
Submitted batch job 24491358
# Now I'll reformat the GL's from the vcf file into ones matching BEAGLE expectations using a python script


# getting new metadata - cloning from JRI github 'riplasm':
hilo/data$ git clone https://github.com/rossibarra/riplasm.git

# PI / diversity statistics (theta estimates):
# To get within-pop diversity genomewide, I use a small set of regions and calculate a SFS per pop (on bigmeml)
scripts$ sbatch -p bigmeml calc1DSFSAngsd.sh
Submitted batch job 24302961 - ERROR in calling SFS program (removed misc/)
scripts$ sbatch -p bigmeml calc1DSFSAngsd.sh
Submitted batch job 24403505 - RAN & COMPLETED WITHIN SECONDS


# GOT NEW LABELS / POPULATION IDs for HILO INDIVIDUALS HILO161-HILO200 + FIXED LABEL SWAP ERROR BETWEEN HILO61 and HILO66 (swap hand-written in lab notes):

# running NGSadmix with addition of 16 lowland 4 pop allopatric maize individuals and SNPs pruned to 1/1000:
~/hilo/scripts$ sbatch -p bigmemh --array=2-4 --export=GL_FILE=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/whole_genome_pruned_every_1000.beagle.gz,OUT_DIR=NGSadmix/merged_pass1_all_alloMaize4Low_16/ runNGSadmix.sh
Submitted batch job 24980206 - COMPLETED

# made new script that links metadata to hilo ID using updated ID-population link from Anne 8.8.18:
addMetadata2HiloIDs.R creates new files hilo_ids.txt and pass1_ids.txt and pass1_allo4Low_ids.txt
# To avoid confusion I put all txt and csv files with the old labels (pre fixing the label swap)
# in a separate directory e.g. "../data/pre_label_fix/HILO_samples.csv"
# This resolved the label switch error (yay!). The only thing left is to switch HILO61 and HILO66 that have a documented mix up in field notes - DONE 8.15.18
# And to remake files listing bams by population
# Some but not all plots have been re-made with the new labels
# Also still working on how to drop out-of-order SNPs from recombination map

# RE-DOING FST WITH NEW LABELS AND EXCLUDING INDIVIDUALS WITH LESS THAN 0.05x COVERAGE
# calcualte SAF for regions:
# edited SAF script to only use 1 core for computing (should be light computing with just using regions)
hilo/scripts$ sbatch --array=1000,2000,3000,5000 -p bigmemh -t 10:00:00 --mem=4G calcSAFAngsd.sh
Submitted batch job 25459455
# then on med calc 2D SFS
scripts$ for i in 1000 2000 3000 5000; do for j in 1000 2000 3000 5000; do if [ $j -gt $i ]; then sbatch --dependency=afterok:25459455 --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 25459940-5
# then calculate pairwise Fst between all groups
scripts$ for i in 1000 2000 3000 5000; do for j in 1000 2000 3000 5000; do if [ $j -gt $i ]; then sbatch --dependency=afterok:25459940,25459941,25459942,25459943,25459944,25459945 --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; fi; done; done
Submitted batch job 25459976-81 -- FIXED TYPO w/ always 2nd pop = 5000 & reran:
Submitted batch job 25461663-8 - COMPLETED
# add results to summary file
~/hilo/data/SAF/pass1/N1000.L100.regions$ for i in 1000 2000 3000 5000; do for j in 1000 2000 3000 5000; do if [ $j -gt $i ]; then awk -v i=$i -v j=$j '{print i "\t" j "\t" $1 "\t" $2}' pop$i.pop$j.fst.stats >> pairwise.group.fst.stats; fi; done; done
RE-CALCULATING PI WITHIN GROUPS
hilo/scripts$ sbatch -p bigmeml --array=1000,2000,3000,5000 -t 30:00 calc1DSFSAngsd.sh
Submitted batch job 25462433 - COMPLETED (THEN CALCULATE PI USING R SCRIPT calcPifromSFS.R)

DOING ABBA-BABA TEST
First allo_mex symp_mex symp_maize allo_maize (underpowered to detect sympatric gene flow) doAbbaBaba4pop_maize.sh
here, allopatric maize is the stand in 'outgroup'
# helper files:
# bam list in population order
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list pass1_bam_pops/maize.symp.list alloMaize4Low_16_bam.list; do cat $i >> 4pop_maize_bam.list; done
# size of populations file
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list pass1_bam_pops/maize.symp.list alloMaize4Low_16_bam.list; do wc -l $i | awk '{print $1}' >> 4pop_maize_bam.sizeFile; done
# try to run:
hilo/scripts$ sbatch -p bigmemh doAbbaBaba4pop_maize.sh
Submitted batch job 25463604
# 3 argument      100 is unknown will exit -- I'll try not specifying a block size
Submitted batch job 25463679 - typo
Submitted batch job 25463718 - still won't recognize arguments (?) may need to download latest ANGSD version from git
# Fixed problem with script and rerunning:
hilo/scripts$ sbatch -p bigmemh --mem=8G doAbbaBaba4pop_maize.sh
Submitted batch job 25496804 - CANCELLED, RENAMED FILES & RESTARTED:
Submitted batch job 25497360
# Summarise D-statistic results: first make a population names
hilo/data$ for i in mexicana.allo mexicana.symp maize.symp maize.allo; do echo $i >> AbbaBaba/4pop_maize_bam.popNames; done
# run Rscript with ANGSD
~/hilo/data/AbbaBaba$ Rscript ~/software/angsd/R/estAvgError.R angsdFile="4pop_maize_bam.regions.abbababa" out="4pop_maize_bam.summary" sizeFile=4pop_maize_bam.sizeFile nameFile=4pop_maize_bam.popNames > 4pop_maize_bam.summary.readable
# also created 2 other summary files (important one is 4pop_maize_bam.summary.Observed.txt)
# basically it's not significant but there are low #'s of independent blocks and total Abba/Baba observations
# and I know I'm underpowered setting it up this way, so yeah.

# NEXT: use Aanc as outgroup (Arbitrary A)
Need to make helper files:
# first test (admixture into symp. maize):
# make pop order: allopatric maize, sympatric maize, allopatric teosinte, out
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.allo.list; do cat $i >> AbbaBaba/3pop_alloMaize_sympMaize_alloMex_bam.list; done
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.allo.list; do wc -l $i | awk '{print $1}' >> AbbaBaba/3pop_alloMaize_sympMaize_alloMex_bam.sizeFile; done
scripts$ sbatch --export=PREFIX=3pop_alloMaize_sympMaize_alloMex_bam -p bigmemh doAbbaBaba3pop_Aanc.sh
Submitted batch job 25497465
# second test (admixture into symp. teosinte):
# and make pop order: allopatric teosinte, sympatric teosinte, allopatric maize, out
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list alloMaize4Low_16_bam.list; do cat $i >> AbbaBaba/3pop_alloMex_sympMex_alloMaize_bam.list; done
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list alloMaize4Low_16_bam.list; do wc -l $i | awk '{print $1}' >> AbbaBaba/3pop_alloMex_sympMex_alloMaize_bam.sizeFile; done
scripts$ sbatch --export=PREFIX=3pop_alloMex_sympMex_alloMaize_bam -p bigmemh doAbbaBaba3pop_Aanc.sh
Submitted batch job 25497534
# third test (admixture into symp. maize using all teosinte's as admixing population):
# lower admixture expected due to admixture into sympatric teosintes but also higher power to
# detect admixture because of a much larger panel of individuals/more coverage
# pop order: allopatric maize, sympatric maize, all teosinte, out
# first make a list of all mexicana individuals, combining sympatric and allopatric bams
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list; do cat $i >> pass1_bam_pops/mexicana.all.list; done
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.all.list; do cat $i >> AbbaBaba/3pop_alloMaize_sympMaize_allMex_bam.list; done
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.all.list; do wc -l $i | awk '{print $1}' >> AbbaBaba/3pop_alloMaize_sympMaize_allMex_bam.sizeFile; done
scripts$ sbatch --export=PREFIX=3pop_alloMaize_sympMaize_allMex_bam -p bigmemh doAbbaBaba3pop_Aanc.sh
Submitted batch job 25497879
### NOTE: only the 4population and not the 3population script appear to output anything

# next use tripsacum or sorghum as outgroup and possibly extend to a much larger portion of the genome
# can I actually do an f3 or 3 population test?

# New landraces : 4 lowland mexico, 6 lowland South America & 5 Andes
# ind's listed in ~/hilo/data/landraces_fromLi/alloMaizeInclude.list
# (will check andean ind's to confirm no admixture later)
# Getting new landraces aligned properly:
# downloading bams from li
ssh -p 2022 ecalfee@farm.cse.ucdavis.edu # connect to port 2022 for file downloads on farm
cd hilo/data/landraces_fromLi/original
# use icommands to get files (sign in for iplant use, -K checks checksums)
module load icommands
iinit # login w/ password
# run my script to download data from iplant with nohup in background, hopefully this works (?)
ecalfee@c11-42:~/hilo/data/landraces_fromLi/original$ nohup ../../../scripts/getLiDataiplant.sh &
[1] 18482
# did not work :/ . So first I am using irsync to get files from Li's iplant directory
# onto mine (to free up her space). Then I'll delete files I don't need (DONE) and use irsync
# again to get the files onto farm
my_computer$ irsync -r -V i:/iplant/home/lilepisorus/landraces_AGPv4 i:/iplant/home/ecalfee/landraces_fromLi
# now attempting to sync directly to farm
my_computer$ ssh -p 2022 ecalfee@farm.cse.ucdavis.edu
$ module load icommands
$ iinit
$ irsync -r -V i:/iplant/home/ecalfee/landraces_fromLi ~/hilo/data/landraces_fromLi/original
# first ran above with -s option (faster, only checks size), then ran without -s so it uses checksums
# using script to re-align to my version of v4 maize genome (no extra contigs)
scripts$ sbatch remapAlloMaizeV4.sh
Submitted batch job 25847933 - # needed to index reference, but did get all files to .fq output
scripts$ sbatch indexRefBWA.sh
Submitted batch job 26008240
scripts$ sbatch --dependency=afterok:26008240 remapAlloMaizeV4.sh
Submitted batch job 26008241 -- DONE, BUT BAM/SAM FILES EMPTY (no sites). .fq look ok.
# all slurm-log files have something like ""[mem_sam_pe] paired reads have different names: "FCC1WDLACXX:4:2116:2328:81442#", "FCC1WDLACXX:4:2108:10115:86292#""

##### how to make better slurm dependencies scripts/pipelines:
https://hpc.nih.gov/docs/job_dependencies.html

# added Xochimilco to a new list of allopatric mexicana
# (because appears unadmixed and we need more until allopatric mex get sequenced more):
~/hilo/data/pass1_bam_pops$ cat mexicana.allo.list pop35.list >> mexicana.allo.withXochi35.list
# I get a list of sites first with major/minor alleles using mafsToSitesFile.sh script:
scripts$ sbatch mafsToSitesFile.sh
Submitted batch job 25543209 - FIXED TYPO
Submitted batch job 25543731 - ANOTHER TYPO
Submitted batch job 25544229 - FINISHED, BUT NEW FILE NAMING & ADDED INDEXING:
Submitted batch job 255534

# the script alleleFreqPipeline.sh basically just stores all the population names so it can run
# calcAlleleFreqPop.sh on each one to calculate allele frequencies for every variable site for each pop
scripts$ sbatch alleleFreqPipeline.sh
Submitted batch job 25544710 - ALL errors; fixed typos, indexed sites file etc.
Submitted batch job 25554149 - RUNNING - COMPLETED
# 8.21.18 now to check which subprocesses didn't finish:
hilo$ for i in $(grep 'Submitted' slurm-log/*25554149*.out | awk '{print $4}'); do grep 'CANCELLED' slurm-log/*$i*.out; done
# these are: (identified by hilo$ grep 25554201 -B 2 slurm-log/*25554149*.out)
maize.allo.4Low16 (#1) - 0, 39, 252, 256
~/hilo/scripts$ sbatch --array=1,39,252,256 --mem=16G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 25772687 -- COMPLETED, OOPS redid region 1 instead of zero:
scripts$ sbatch --array=0 --mem=16G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 25773575
pop29 (#13) - 85
~/hilo/scripts$ sbatch --array=85 --mem=16G -t 24:00:00 --export=POP=pop29,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 25772727 - COMPLETED
# the following freq files also don't exist and need to be re-done:
sbatch --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,\
DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26026327 - cancelled reading old sites file
Submitted batch job 26026424 - didn't quite fix. Now fixed to have correct sites directory (NOTE -- THESE NEW FILES DON'T HAVE DUPLICATED SITES):
scripts$ sbatch --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,\
DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 26026522 - CANCELLED - still a problem with sites file looking the same age or older than the index.
# added sleep 2s to script that removes duplicates (can mimic in other scripts) and removing and remaking sites files for problem files
data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar$ for i in {111..114} 159 257 {289..291} {326..327} {363..365} 395; do echo $i; rm region_$i.var.sites.*; angsd sites index region_$i.var.sites; echo "done indexing "$i; done
data/var_sites/merged_pass1_all_alloMaize4Low_16$ same thing as above -- COMPLETED
# running the above script for calcAlleleFreqPop again:
Submitted batch job 26027501
# and running the same allele freq script above but with POP=mexicana.allo.withXochi35
scripts$ sbatch --dependency=afterok:26027501 --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=mexicana.allo.withXochi35,DIR_POPS=pass1_bam_pops,\
DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 26027624
# re-doing from old variant sites (with doubles, just to avoid inconsistency) -- output in geno_lik dir
scripts$ sbatch --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=mexicana.allo.withXochi35,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26029518 - ERROR TYPO in sites dir
Submitted batch job 26029702
scripts$ sbatch --dependency=afterok:26029702 --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26029559 - ALSO HAD TYPO in sites dir
Submitted batch job 26029746 - COMPLETE, but looks like region_0 file is truncated; fix below
scripts$ sbatch --array=0 -p bigmemh --mem=8G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,\
DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26030193 - EXCEEDED Memory, trying again with many cores on partition med
scripts$ sbatch --array=0 -p med --mem=24G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26031000 -COMPLETED

# now that I have allele frequencies, I can do an F4 test. The only issue is that
# I need to match alleles by position because ANGSD will omit a frequency if no individuals have
# any data. I *may* also want to make more stringent criteria for # ind's with data, e.g. in maize & allo mex, to include a site, but don't need to for a first pass analysis. I think this should only increase variance in f4, but not bias it to include all sites while a strict site filter for depth may actually increase bias due to mapping bias.

# After making allele freq files I found that my position/sites files had every SNP duplicated because I had specified -rf and -f in calling variant sites and calculating MAF scripts
# this affects all files eg. region_9.mafs.gz region_9.var.sites for regions covering the whole chromosome, including within population frequencies.
# BUT the NGSadmix & PCA results should be largely unchanged because there are no duplicated sites in my
# genome-wide variant sites filtering by every 1000th SNP (they just aren't 100% 10kb apart due to the duplicates). I will need to re-call all variant sites and will want to use new thinned
# SNPs anyways, filtered by cM position rather than bp's. Temporarily I will remove duplicates in a post-processing R-script (which won't be necessary once upstream is corrected & files are re-run).
# this doesn't affect FST and SFS stats that used a small set of regions (not duplicated with -r -rf). After re-running I should confirm that the issue is the -r -rf and the duplication problem goes away.
# I can temporarily fix the doubled sites problem by using split
split -l 73165 region_287.var.sites (then be sure to re-index the new sites file _A and the # lines = half of wc -l region_287.var.sites)
# implemented in removeDuplSites.sh making new sites files in var_sites/merged_pass1_all_alloMaize4Low_16/allVar/region_*.var.sites
scripts$ sbatch removeDuplSites.sh
Submitted batch job 25968083 -- I cancelled & reran after removing # jobs at a time limit:
Submitted batch job 25968187 -- mostly finished except some TIMEOUT run on node 7 and 1 (will rerun & exclude these nodes)
scripts$ sbatch -x bigmem1,bigmem7 --array=32-53,84-95,136-199 -t 8:00:00 removeDuplSites.sh
Submitted batch job 25971992 - COMPLETED -- but turns out a few others didn't say CANCELLED but were cancelled for timeout, rerunning below
scripts$ sbatch -p med --array=54-60,62-83 -t 8:00:00 removeDuplSites.sh
Submitted batch job 26030055 -COMPLETED

##### currently working on doGenoAngsd2Plink.sh (may be done once I have new sites files) and doHaploAngsd.sh
# doGeno worked but doHaploCall did not
# new plan than doHaploAngsd is to get read counts for Maj/Min allele for all individuals
# and then I randomly sample 1 allele from each individual with data. Maybe I filter sites first...
# in the end I need sites filtered by XcM
# with counts for each admixed individual

# first I filter sites for MAF difference maize-mex and min # individuals with reads/coverage in both allopatric pass1_bam_pops
# for now I am including pop35 as allopatric mex to have more individuals (Xochimilco)
scripts$ sbatch filterAlloMAFnInd.sh
Submitted batch job 26022854 - TYPO DIDN'T run
Submitted batch job 26023401 - ERRORS maybe in working directory. trying again with just 1 tester region_
scripts$ sbatch --array=9 filterAlloMAFnInd.sh
Submitted batch job 26024275 - fixed typo and reran
Submitted batch job 26024388
scripts$ sbatch -p bigmemh --array=0-425%7 filterAlloMAFnInd.sh
Submitted batch job 26024492 -- all worked except those with missing freq files for allopatric maize. Fixing that (above see job 26026522) & re-running
scripts$ sbatch -p low --array=111-114,159,257,289-291,326-327,363-365,395 --dependency=afterok:26026522 filterAlloMAFnInd.sh
Submitted batch job 26026785 - CANCELLED, fixed indexing issue and re-trying
scripts$ sbatch -p low --array=111-114,159,257,289-291,326-327,363-365,395 --dependency=afterok:26027624 filterAlloMAFnInd.sh
Submitted batch job 26027652 - STILL FILES MISSING. - reran those files then re-running this script below:
scripts$ sbatch -p low --array=111-114,159,257,289-291,326-327,363-365,395 --dependency=afterok:26029746 filterAlloMAFnInd.sh
Submitted batch job 26029827
scripts$ sbatch -p low --array=0,54-60,62-83 --dependency=afterok:26030055,26030193 filterAlloMAFnInd.sh
Submitted batch job 26030212 - COMPLETED except 0 (why did this run when 26030193 was cancelled for memory?)
scripts$ sbatch -p low --array=0 --dependency=afterok:26031000 filterAlloMAFnInd.sh
Submitted batch job 26031038 - COMPLETED, but DIDN't WAIT -- not suer why, will rerun
scripts$ sbatch -p low --array=0 filterAlloMAFnInd.sh
Submitted batch job 26031985 - COMPLETED

# then I use a second script to thin SNPs to minimize within-population LD
# I choose a fixed cM width of .0001 (~.1Kb) rather than trying to calculate LD within low coverage pops
# a reasonable alternative to consider later is to calculate LD within my maize reference sample only
# and use that to filter (or at least to choose a cM cutoff..but I'm not sure that's necessary)
scripts$ sbatch -p low --array=2 -t 30:00 pruneFixedcM.sh # testing - working
scripts$ sbatch -p bigmemh --dependency=afterok:26027652 --array=1-10%7 pruneFixedcM.sh
Submitted batch job 26028060 - FIRST SCRIPT FOR FILTERING WASN'T ACTUALLY DONE, so re-running
scripts$ sbatch -p bigmemh --dependency=afterok:26030212 --array=1-10%7 pruneFixedcM.sh
Submitted batch job 26030285
scripts$ sbatch -p bigmemh --dependency=afterok:26031038 --array=1 pruneFixedcM.sh
Submitted batch job 26031064 - DIDN'T WORK
Submitted batch job 26032015 (ran again now without dependency because I reran that to completion)

# before running ASEReadCounter I need to add ReadGroups so GATK will process my bams (takes a few mins) and re-index:
scripts$ sbatch addReadGroup2Bam.sh
Submitted batch job 26007563 -- many did not run (retry all:)
Submitted batch job 26007874 -- COMPLETED

# update: I can get a VCF output from angsd for allopatric maize for any positions I desire:
scripts$ sbatch -p bigmemh --dependency=afterok:26032015 --array=1-10%4 getVCFforAllo.sh
Submitted batch job 26035120 # by 4 because it's 16G (or 2 cpu's per task) - exited out due to lack of angsd index, rerunning
scripts$ sbatch -p bigmemh --array=1-10%4 getVCFforAllo.sh
Submitted batch job 26118782 - typo fixed & rerunning:
Submitted batch job 26119622

## problem with ANGSD vcf being used by GATK but plink is able to parse the vcf and get allele counts
scripts$ sbatch -p bigmemh --dependency=afterok:26035120 --array=1-10%8 alloVCF2AlleleCounts.sh
Submitted batch job 26035124 - resubmitting
scripts$ sbatch -p bigmemh --dependency=afterok:26119622 --array=1-10%8 alloVCF2AlleleCounts.sh
Submitted batch job 26119734

# I can get allele counts for major and minor allele using GATK ASEReadCounter
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:26035124_$i countReadsRefAlt.sh; done
Submitted batch job 26035127-36 - resubmitting
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:26119734_$i countReadsRefAlt.sh; done
Submitted batch job 26119815-25 # skips 21

# then in R I can randomly sample 1 read from each allopatric mexicana individual to make an 'allopatric allele counts' file
# and put individuals and chromosome positions into population input files for reading by ancestry_hmm
