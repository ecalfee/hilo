# log for reproducing HILO data analysis

# Tue Oct 24 11:58:17 PDT 2017
# Alignment of first set of data
First download v4 of maize genome AGPv4 in fasta format from
https://www.ebi.ac.uk/ena/data/view/GCA_000005005.6

Use bwa for index, then alignment
cd data
~/Software/bwa.kit/bwa index AGPv4/AGPv4.fasta

Change the chromosome labels to something simpler, chr1, chr2 etc.
~/Software/bioawk/bioawk -c fastx '{print ">"$comment"\n"$seq}' AGPv4/AGPv4.fasta | sed 's/Zea mays cultivar B73 chromosome /chr/' | sed 's/, whole genome shotgun sequence.//' > AGPv4/AGPv4_simple.fasta
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta

Align one set of paired-end raw reads:
~/Software/bwa.kit/bwa mem AGPv4/AGPv4.fasta 
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_1.fq.gz 
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_2.fq.gz
 | gzip -3 > HILO_01.sam.gz

Aborted and made batch run in parallel instead piping directly into BAM format using samtools
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta; parallel --noswap --joblog hilo_alignment.log --jobs 6 '~/Software/bwa.kit/bwa mem -R "@RG\tID:batch_1\tSM:hilo_{1}" AGPv4/AGPv4_simple.fasta HILO_data/raw_data/HILO{1}/HILO{1}_*_1.fq.gz HILO_data/raw_data/HILO{1}/HILO{1}_*_2.fq.gz | ~/Software/bwa.kit/samtools view -b - > HILO_data/bam_files/HILO_{1}.bam' ::: {1..40}


# Starting from reads aligned by Dan using bwa
ecalfee@bigmem8:/group/jrigrp6/DanAlignments/HILO105$ ls
aln.bam						  HILO105_USPD16085642-AK6871_hky2mccxy_L6_2.fq.gz
HILO105_USPD16085642-AK6871_hky2mccxy_L6_1.fq.gz  MD5.txt

interactive session run:
srun -p bigmemm -t 2:00:00 --mem 2000 --pty bash

ran test batch (finished fine)
hilo/scripts$ sbatch --array=1-2 filterBam.sh
completed: 19908139_1  19908138_2

ran filtering on all of the as of yet aligned individuals from HILO
hilo/scripts$ sbatch --array=3-40,100-179 filterBam.sh 

I can't really tell why some jobs failed and at what stage. I am running just 40 tasks now with more allocated time (4hrs not 1hr)
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# basically nothing ran properly, even files stating 'complete'
# I have requested to avoid bigmemm2 (permissions error) and only use nodes with at least 30G available temporary disk space for scratch 
# and in addition I put in exit 3 and exit 4 flags to stop execution if it fails to make temp folders
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# got permissions on bigmem2 and changed script to exclude bigmem1 (lack of memory)
# re-running just a short sample set before the complete aligment set is ready
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh 
Submitted batch job 19970756
# this had an end of script/file error, that I fixed and re-ran
# deleting intermediate scratch files first
ecalfee@farm:~/hilo/scripts$ ssh bigmem10 'rm -r /scratch/ecalfee'
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh 
Submitted batch job 19970894

# running 11-30; only change to script is when the scratch folder is deleted
ecalfee@farm:~/hilo/scripts$ sbatch --array=11-40 filterBam.sh 
Submitted batch job 19971006

# running the rest of the files up to ind. 200 except for the following exclusions:
# 41, 42, 43, 80 excluded because haven't finished alignment (no aln.bam file exists yet)
# 34, 109, 113 excluded because alignment needs to be re-run (timed-out -> truncated bam)
ecalfee@farm:~/hilo/scripts$ sbatch --array=44-79,81-108,110-112,114-200 filterBam.sh
Submitted batch job 20011145

# many failed due to time-out; re-started those jobs that failed plus 34, 109, 113 because they have been re-run
ecalfee@farm:~/hilo/scripts$ sbatch --array=34,101-105,109-114,117,119,121,125,127,130,131,133-135,137,139,141,142,161-163,165,169,170,172,174-177 filterBam.sh
Submitted batch job 20102438

# final failed jobs due to lack of space; re-running those jobs
ecalfee@farm:~/hilo/scripts$ sbatch --array=121,127,133,135,141,162,176,177 filterBam.sh
Submitted batch job 20107398

# indexing reference
ecalfee@farm:~/hilo/scripts$ sbatch indexRef.sh
Submitted batch job 20107852

# Tue May 29 15:26:50 PDT 2018
# all jobs except those excluded finished (ie. 41-43, 80).
# run angsd to find variable sites within all individuals > 0.1 MAF 
# short test run (in srun):
ecalfee@bigmem10:~/hilo/data$ angsd -GL 1 -ref /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa \
-baq 1 -minMapQ 30 -minQ 20 -doMaf 2 -doGlf 2 -minMaf 0.1 -doMajorMinor 4 -bam TEST/testBAM.list \
-r 1:10000-20000 -out testANGSD

# indexed BAMS so they could be used
ecalfee@bigmem10:~/hilo/data/filtered_bam$ module load samtools
Module samtools/1.3.1 loaded 
ecalfee@bigmem10:~/hilo/data/filtered_bam$ for i in {3..40} {44..79} {81..200}; do samtools index hilo_$i.sort.dedup.bam; done

# now pre-calculating BAQ to speed up GL in angsd and to use again when counting reads
# script caps base quality at the lower of raw base quality and BAQ from samtools
# using extended BAQ calculation -E option; recommended for improved sensitivity (will be new default)
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-40,44-79,80-200 addBAQBam.sh
Submitted batch job 20147251
ecalfee@farm:~/hilo/scripts$ date
Tue May 29 19:29:51 PDT 2018

# making list of BAMS for fist pass analysis:
data$ for i in {1..40} {44..79} {81..200}; do echo filtered_bam/hilo_$i.sort.dedup.baq.bam >> pass1_bam.all.list; done

# counting # reads that mapped & passed mapping quality and non-duplicate read quality filtering for each pass1 indiviudal:
# (this is an estimate of coverage/data quality for each sequenced individual)
~/hilo/data/filtered_bam$ for i in {1..200}; do samtools flagstat hilo_$i.sort.dedup.baq.bam | \
awk -v i="$i" '{print "hilo"i" "$0}' | grep 'total' >> pass1.all.metrics.Nreads; done

# finding variable sites with ANGSD, each of 10 chrom at a time (4 nodes requested per chrom):
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh 
Submitted batch job 20182952
Wed May 30 09:16:56 PDT 2018
# -CANCELLED (problems = low memory and wrong individuals .. doubled)
# fixed and re-running 15:53 pm:
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh 
Submitted batch job 20203873
# -CANCELLED. Talked to Jeff and decided to only look for SNPs > 0.5 freq using a non-HW based method
# I will do something different to find SNPs for local ancestry inference, but this doesn't use the reference
# and only counts reads supporting the two most common bases, after quality filters
# NOTE: this wouldn't find a fixed difference between maize 282 and the current sample
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh 
Submitted batch job 20245129
# Problem was a file-dumping error because I had in a check from SNP_pval; resolved by removing that part
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20265142
# Took < 2hrs to run. Decided I wanted more SNPs, so I reduced the threshold to 50 individuals with information (> half).
# put old files in old_pass1/
# it's recommended in the NGSadmix paper to avoid sites with > 80% missing data, so 50+ ind with info is slightly conservative
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20280102
ecalfee@farm:~/hilo/scripts$ date
Thu May 31 10:38:49 PDT 2018
# only chr1 failed (largest). I reduced requested memory to get it back on queue
ecalfee@farm:~/hilo/scripts$ sbatch --array=1 callVarAngsd.sh 
Submitted batch job 20419175
# chr1 failed again for too much memory, so I split it into 10 chunks:
hilo/scripts$ sbatch --array=0-9 callVarAngsd_split_chr1.sh 
Submitted batch job 20591724
Mon Jun  4 14:38:49 PDT 2018
# where now 0-9 above indicate the indeces of each chunk ~30000 kbp long
# I will need to concatenate chunks before pruning (below)
# in srun concatenating (without intermediate headers)
data/var_sites/pass1$ zcat chr1_chunk0.mafs.gz | gzip > chr1.mafs.gz; \
for i in {1..9}; do zcat chr1_chunk$i.mafs.gz | tail -n +2 | gzip >> chr1.mafs.gz; done
# then moving all chunk files to their own subfolder:
data/var_sites/pass1$ mv chr1_chunk* chr1_chunks/

# made a python script to prune sites to approximately unlinked (no more than 1 SNP per 10kb).
# which I wrapped in a bash script pruneFixed.sh
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-9 pruneFixed.sh
Submitted batch job 20420257
~/hilo/scripts$ sbatch --array=1,10 pruneFixed.sh
Submitted batch job 20603410
# After pruning, I combine across all chromosomes and remove all header (and unzip/decompress); 
# this leaves ~170k SNPs. 
~hilo/data/var_sites/pass1$ mkdir pruned_positions
~/hilo/data/var_sites/pass1$ for i in {1..10}; do zcat chr$i.pruned.mafs.gz | tail -n +2 | cut -f 1-4 >> pruned_positions/all.positions; done
# then I divide into chunks, each containing 250 SNPs, for parallel analysis in ANGSD
~/hilo/data/var_sites/pass1/pruned_positions$ split all.positions -d -a 3 -l 250 positions_chunk

# next find samtools genotype likelihood for those sites across all individuals. 
# made (very!) small test file first - 2 positions 
~/hilo/data/var_sites/pass1/pruned_positions$ head positions_chunk000 -n 2 > positions_chunk999
# ran test file on slurm
~/hilo/scripts$ sbatch --array=999 -t 1:00:00 calcGL4Pruned.sh 
Submitted batch job 20652316

# now running all 677 chunks
~/hilo/scripts$ sbatch --array=0-677 calcGL4Pruned.sh
Submitted batch job 20652343

# and then combine output (in srun)
~/hilo/data/geno_lik/pass1/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..677}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# and running NGSadmix
~/hilo/scripts$ sbatch runNGSadmix.sh
Submitted batch job 20781908
Mon Jun 11 22:36:54 PDT 2018

# modified NGSadmix to use array task ID number as # subpops (K)
# running with same SNPs for K = 3 & 4
~/hilo/scripts$ sbatch --array=3,4 runNGSadmix.sh
Submitted batch job 20803690

# I divide the genome into manageable 50Mb chunks based on .fa.fai reference file [comp. Barbara; scp results to Farm]
hilo/scripts$ mkdir -p ../data/refMaize/divide_50Mb
hilo/scripts$ python divRefRegions.py 50000000 ../data/refMaize/Zea_mays.AGPv4.dna.chr.fa.fai ../data/refMaize/divide_50Mb
# save a file with a list of regions: chr, start, end, region #, regions file name
~/hilo/data/refMaize/divide_50Mb$ for i in {0..46}; do awk -v i=$i 'gsub(":|-", "\t", $0){print $0"\t"i"\t""region_"i".txt"}' region_$i.txt >> ALL_regions.list; done

# new bam lists:
# sympatric populations only, and individuals must have bam & est. coverage > 0.05
hilo/data$ awk '$1 != "n" && $3 != 20 && $3 != 22 && $3 != 33 && $5 != "NA" && $6 > 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlySympatric.list

# allopatric mexicana only, no minimum coverage
hilo/data$ awk '$1 != "n" && ($3 == 20 || $3 == 22 || $3 == 33) && $5 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlyAllopatricMex.list

# excluded ind's (fit into neither category above)
hilo/data$ awk '$1 != "n" && ($5 == "NA" || ($3 != 20 && $3 != 22 && $3 != 33 && $6 <= 0.05)) {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.excluded.list

# now finding new set of variant sites using ANGSD for only sympatric individuals (all)
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdSymp.sh 
Submitted batch job 20811827
# everything ran but region_0 doesn't appear to be properly gipped and can't be opened
# so I am re-running just that region (problem was I wrote over just this file accidentally with test run of callVarAngsdAllo.sh)
~/hilo/scripts$ sbatch --array=0 callVarAngsdSymp.sh
Submitted batch job 20816879

# when I filter for low LD I can try different filters for nInd and MAF (more stringent)
# to maximize information across individuals for the LD

# now need to find all variant sites between reference and allopatric teosinte at >=30% freq. 
# Then find all variant sites between reference and allopatric maize at >=30% freq .. put them together and exclude sites with less than a cutoff # individuals and allele frequency difference between mexicana & maize of 40%
# not quite working -- look at slurm log
~/hilo/scripts$ sbatch --array=0 callVarAngsdAllo.sh
# fixed and after another small typo false start now running all regions
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdAllo.sh
Submitted batch job 20817741

# added script to prune based on fixed cM width and Ogut_2015 map, rather than fixed bp (0.01 cM ~ 10kb on avg but varies across genome)
pruneFixedcM.py
# running with two diff. thresholds for minimum # individuals, 75 and 100: (loads module anaconda3 for use of pandas)
~/hilo/scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20930024
# did not work -- fixed error and added a way to set minInd as an evironmental variable and run each chromosome separately 
# (can't break up further b/c of min cM across regions w/in chromosomes)
# running with default MIN_IND=75
scripts$ sbatch pruneFixedcM.sh
# runnign again with MIN_IND=100
scripts$ sbatch --export=MIN_IND=100 pruneFixedcM.sh




