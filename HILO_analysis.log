# log for reproducing HILO data analysis

# Tue Oct 24 11:58:17 PDT 2017
# Alignment of first set of data
First download v4 of maize genome AGPv4 in fasta format from
https://www.ebi.ac.uk/ena/data/view/GCA_000005005.6

Use bwa for index, then alignment
cd data
~/Software/bwa.kit/bwa index AGPv4/AGPv4.fasta

Change the chromosome labels to something simpler, chr1, chr2 etc.
~/Software/bioawk/bioawk -c fastx '{print ">"$comment"\n"$seq}' AGPv4/AGPv4.fasta | sed 's/Zea mays cultivar B73 chromosome /chr/' | sed 's/, whole genome shotgun sequence.//' > AGPv4/AGPv4_simple.fasta
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta

Align one set of paired-end raw reads:
~/Software/bwa.kit/bwa mem AGPv4/AGPv4.fasta
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_1.fq.gz
HILO_data/raw_data/HILO1/HILO1_USPD16082576-K2560_H3HFWCCXY_L7_2.fq.gz
 | gzip -3 > HILO_01.sam.gz

Aborted and made batch run in parallel instead piping directly into BAM format using samtools
~/Software/bwa.kit/bwa index AGPv4/AGPv4_simple.fasta; parallel --noswap --joblog hilo_alignment.log --jobs 6 '~/Software/bwa.kit/bwa mem -R "@RG\tID:batch_1\tSM:hilo_{1}" AGPv4/AGPv4_simple.fasta HILO_data/raw_data/HILO{1}/HILO{1}_*_1.fq.gz HILO_data/raw_data/HILO{1}/HILO{1}_*_2.fq.gz | ~/Software/bwa.kit/samtools view -b - > HILO_data/bam_files/HILO_{1}.bam' ::: {1..40}


# Starting from reads aligned by Dan using bwa
ecalfee@bigmem8:/group/jrigrp6/DanAlignments/HILO105$ ls
aln.bam						  HILO105_USPD16085642-AK6871_hky2mccxy_L6_2.fq.gz
HILO105_USPD16085642-AK6871_hky2mccxy_L6_1.fq.gz  MD5.txt

interactive session run:
srun -p bigmemm -t 2:00:00 --mem 2000 --pty bash

ran test batch (finished fine)
hilo/scripts$ sbatch --array=1-2 filterBam.sh
completed: 19908139_1  19908138_2

ran filtering on all of the as of yet aligned individuals from HILO
hilo/scripts$ sbatch --array=3-40,100-179 filterBam.sh

I can't really tell why some jobs failed and at what stage. I am running just 40 tasks now with more allocated time (4hrs not 1hr)
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# basically nothing ran properly, even files stating 'complete'
# I have requested to avoid bigmemm2 (permissions error) and only use nodes with at least 30G available temporary disk space for scratch
# and in addition I put in exit 3 and exit 4 flags to stop execution if it fails to make temp folders
hilo/scripts$ sbatch --array=1-40 filterBam.sh

# got permissions on bigmem2 and changed script to exclude bigmem1 (lack of memory)
# re-running just a short sample set before the complete aligment set is ready
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh
Submitted batch job 19970756
# this had an end of script/file error, that I fixed and re-ran
# deleting intermediate scratch files first
ecalfee@farm:~/hilo/scripts$ ssh bigmem10 'rm -r /scratch/ecalfee'
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 filterBam.sh
Submitted batch job 19970894

# running 11-30; only change to script is when the scratch folder is deleted
ecalfee@farm:~/hilo/scripts$ sbatch --array=11-40 filterBam.sh
Submitted batch job 19971006

# running the rest of the files up to ind. 200 except for the following exclusions:
# 41, 42, 43, 80 excluded because haven't finished alignment (no aln.bam file exists yet)
# 34, 109, 113 excluded because alignment needs to be re-run (timed-out -> truncated bam)
ecalfee@farm:~/hilo/scripts$ sbatch --array=44-79,81-108,110-112,114-200 filterBam.sh
Submitted batch job 20011145

# many failed due to time-out; re-started those jobs that failed plus 34, 109, 113 because they have been re-run
ecalfee@farm:~/hilo/scripts$ sbatch --array=34,101-105,109-114,117,119,121,125,127,130,131,133-135,137,139,141,142,161-163,165,169,170,172,174-177 filterBam.sh
Submitted batch job 20102438

# final failed jobs due to lack of space; re-running those jobs
ecalfee@farm:~/hilo/scripts$ sbatch --array=121,127,133,135,141,162,176,177 filterBam.sh
Submitted batch job 20107398

# indexing reference
ecalfee@farm:~/hilo/scripts$ sbatch indexRef.sh
Submitted batch job 20107852

# Tue May 29 15:26:50 PDT 2018
# all jobs except those excluded finished (ie. 41-43, 80).
# run angsd to find variable sites within all individuals > 0.1 MAF
# short test run (in srun):
ecalfee@bigmem10:~/hilo/data$ angsd -GL 1 -ref /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa \
-baq 1 -minMapQ 30 -minQ 20 -doMaf 2 -doGlf 2 -minMaf 0.1 -doMajorMinor 4 -bam TEST/testBAM.list \
-r 1:10000-20000 -out testANGSD

# indexed BAMS so they could be used
ecalfee@bigmem10:~/hilo/data/filtered_bam$ module load samtools
Module samtools/1.3.1 loaded
ecalfee@bigmem10:~/hilo/data/filtered_bam$ for i in {3..40} {44..79} {81..200}; do samtools index hilo_$i.sort.dedup.bam; done

# now pre-calculating BAQ to speed up GL in angsd and to use again when counting reads
# script caps base quality at the lower of raw base quality and BAQ from samtools
# using extended BAQ calculation -E option; recommended for improved sensitivity (will be new default)
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-40,44-79,80-200 addBAQBam.sh
Submitted batch job 20147251
ecalfee@farm:~/hilo/scripts$ date
Tue May 29 19:29:51 PDT 2018

# making list of BAMS for first pass analysis:
data$ for i in {1..40} {44..79} {81..200}; do echo filtered_bam/hilo_$i.sort.dedup.baq.bam >> pass1_bam.all.list; done

# counting # reads that mapped & passed mapping quality and non-duplicate read quality filtering for each pass1 indiviudal:
# (this is an estimate of coverage/data quality for each sequenced individual)
~/hilo/data/filtered_bam$ for i in {1..200}; do samtools flagstat hilo_$i.sort.dedup.baq.bam | \
awk -v i="$i" '{print "hilo"i" "$0}' | grep 'total' >> pass1.all.metrics.Nreads; done
# counting # reads pre-filtering, and at de-duplication using PICARD metrics.txt output files
# (to compare effects of filtering across groups/individuals):
~/hilo/data/filtered_bam$ grep 'LIBRARY' hilo_1.metrics.txt | awk '{print "StudyID\t"$0}' > pass1.all.metrics.raw.Nreads; \
for i in {1..200}; do grep 'Unknown Library' hilo_$i.metrics.txt | \
awk -v i="$i" '{print "hilo"i" "$0}' >> pass1.all.metrics.raw.Nreads; done

# finding variable sites with ANGSD, each of 10 chrom at a time (4 nodes requested per chrom):
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20182952
Wed May 30 09:16:56 PDT 2018
# -CANCELLED (problems = low memory and wrong individuals .. doubled)
# fixed and re-running 15:53 pm:
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20203873
# -CANCELLED. Talked to Jeff and decided to only look for SNPs > 0.05 freq using a non-HW based method
# I will do something different to find SNPs for local ancestry inference, but this doesn't use the reference
# and only counts reads supporting the two most common bases, after quality filters
# NOTE: this wouldn't find a fixed difference between maize 282 and the current sample
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20245129
# Problem was a file-dumping error because I had in a check from SNP_pval; resolved by removing that part
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20265142
# Took < 2hrs to run. Decided I wanted more SNPs, so I reduced the threshold to 50 individuals with information (> half).
# put old files in old_pass1/
# it's recommended in the NGSadmix paper to avoid sites with > 80% missing data, so 50+ ind with info is slightly conservative
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-10 callVarAngsd.sh
Submitted batch job 20280102
ecalfee@farm:~/hilo/scripts$ date
Thu May 31 10:38:49 PDT 2018
# only chr1 failed (largest). I reduced requested memory to get it back on queue
ecalfee@farm:~/hilo/scripts$ sbatch --array=1 callVarAngsd.sh
Submitted batch job 20419175
# chr1 failed again for too much memory, so I split it into 10 chunks:
hilo/scripts$ sbatch --array=0-9 callVarAngsd_split_chr1.sh
Submitted batch job 20591724
Mon Jun  4 14:38:49 PDT 2018
# where now 0-9 above indicate the indeces of each chunk ~30000 kbp long
# I will need to concatenate chunks before pruning (below)
# in srun concatenating (without intermediate headers)
data/var_sites/pass1$ zcat chr1_chunk0.mafs.gz | gzip > chr1.mafs.gz; \
for i in {1..9}; do zcat chr1_chunk$i.mafs.gz | tail -n +2 | gzip >> chr1.mafs.gz; done
# then moving all chunk files to their own subfolder:
data/var_sites/pass1$ mv chr1_chunk* chr1_chunks/

# made a python script to prune sites to approximately unlinked (no more than 1 SNP per 10kb).
# which I wrapped in a bash script pruneFixed.sh
ecalfee@farm:~/hilo/scripts$ sbatch --array=1-9 pruneFixed.sh
Submitted batch job 20420257
~/hilo/scripts$ sbatch --array=1,10 pruneFixed.sh
Submitted batch job 20603410
# After pruning, I combine across all chromosomes and remove all header (and unzip/decompress);
# this leaves ~170k SNPs.
~hilo/data/var_sites/pass1$ mkdir pruned_positions
~/hilo/data/var_sites/pass1$ for i in {1..10}; do zcat chr$i.pruned.mafs.gz | tail -n +2 | cut -f 1-4 >> pruned_positions/all.positions; done
# then I divide into chunks, each containing 250 SNPs, for parallel analysis in ANGSD
~/hilo/data/var_sites/pass1/pruned_positions$ split all.positions -d -a 3 -l 250 positions_chunk

# next find samtools genotype likelihood for those sites across all individuals.
# made (very!) small test file first - 2 positions
~/hilo/data/var_sites/pass1/pruned_positions$ head positions_chunk000 -n 2 > positions_chunk999
# ran test file on slurm
~/hilo/scripts$ sbatch --array=999 -t 1:00:00 calcGL4Pruned.sh
Submitted batch job 20652316

# now running all 677 chunks
~/hilo/scripts$ sbatch --array=0-677 calcGL4Pruned.sh
Submitted batch job 20652343

# and then combine output (in srun)
~/hilo/data/geno_lik/pass1/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..677}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# and running NGSadmix
~/hilo/scripts$ sbatch runNGSadmix.sh
Submitted batch job 20781908
Mon Jun 11 22:36:54 PDT 2018

# modified NGSadmix to use array task ID number as # subpops (K)
# running with same SNPs for K = 3 & 4
~/hilo/scripts$ sbatch --array=3,4 runNGSadmix.sh
Submitted batch job 20803690
Submitted batch job 21671940 -- accidentally resubmitted job and re-ran files (completed & should be same output)

# I divide the genome into manageable 50Mb chunks based on .fa.fai reference file [comp. Barbara; scp results to Farm]
hilo/scripts$ mkdir -p ../data/refMaize/divide_50Mb
hilo/scripts$ python divRefRegions.py 50000000 ../data/refMaize/Zea_mays.AGPv4.dna.chr.fa.fai ../data/refMaize/divide_50Mb
# save a file with a list of regions: chr, start, end, region #, regions file name
~/hilo/data/refMaize/divide_50Mb$ for i in {0..46}; do awk -v i=$i 'gsub(":|-", "\t", $0){print $0"\t"i"\t""region_"i".txt"}' region_$i.txt >> ALL_regions.list; done

# repeat for even smaller regions -- 5Mb (created region _0 to 425)
ecalfee@bigmem3:~/hilo/scripts$ mkdir -p ../data/refMaize/divide_5Mb
ecalfee@bigmem3:~/hilo/scripts$ python3 divRefRegions.py 5000000 ../data/refMaize/AGPv4.fa.fai ../data/refMaize/divide_5Mb
scripts$ cd ../data/refMaize/divide_5Mb; for i in {0..425}; do awk -v i=$i 'gsub(":|-", "\t", $0){print $0"\t"i"\t""region_"i".txt"}' region_$i.txt >> ALL_regions.list; done


# new bam lists:
# data/HILO_IDs_cov_pass1.csv is created in plot_global_ancestry_NGSadmix & saves the # reads that pass filtering + estimated coverage = (# reads * 150bp/read)/2.3Gb as well as meta data on pops and HILO_id as well as IndN for pass1 & GL_beagle_file_ID (skips unaligned ind's)
# sympatric populations only, and individuals must have bam & est. coverage > 0.05
hilo/data$ awk '$1 != "n" && $3 != 20 && $3 != 22 && $3 != 33 && $5 != "NA" && $6 > 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlySympatric.list

# allopatric mexicana only, no minimum coverage
hilo/data$ awk '$1 != "n" && ($3 == 20 || $3 == 22 || $3 == 33) && $5 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.onlyAllopatricMex.list

# excluded ind's (fit into neither category above)
hilo/data$ awk '$1 != "n" && ($5 == "NA" || ($3 != 20 && $3 != 22 && $3 != 33 && $6 <= 0.05)) {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam.excluded.list

# now finding new set of variant sites using ANGSD for only sympatric individuals (all)
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdSymp.sh
Submitted batch job 20811827
# everything ran but region_0 doesn't appear to be properly gzipped and can't be opened
# so I am re-running just that region (problem was I wrote over just this file accidentally with test run of callVarAngsdAllo.sh)
~/hilo/scripts$ sbatch --array=0 callVarAngsdSymp.sh
Submitted batch job 20816879

# when I filter for low LD I can try different filters for nInd and MAF (more stringent)
# to maximize information across individuals for the LD

# now need to find all variant sites between reference and allopatric teosinte at >=30% freq.
# Then find all variant sites between reference and allopatric maize at >=30% freq .. put them together and exclude sites with less than a cutoff # individuals and allele frequency difference between mexicana & maize of 40%
# not quite working -- look at slurm log
~/hilo/scripts$ sbatch --array=0 callVarAngsdAllo.sh
# fixed and after another small typo false start now running all regions
~/hilo/scripts$ sbatch --array=0-46 callVarAngsdAllo.sh
Submitted batch job 20817741

# added script to prune based on fixed cM width and Ogut_2015 map, rather than fixed bp (0.01 cM ~ 10kb on avg but varies across genome)
pruneFixedcM.py
# running with two diff. thresholds for minimum # individuals, 75 and 100: (loads module anaconda3 for use of pandas)
~/hilo/scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20930024
# did not work -- fixed error and added a way to set minInd as an evironmental variable and run each chromosome separately
# (can't break up further b/c of min cM across regions w/in chromosomes)
# running with default MIN_IND=75
scripts$ sbatch pruneFixedcM.sh
Submitted batch job 20950468
# running again with MIN_IND=100
~/hilo/scripts$ sbatch --export=MIN_IND=100 pruneFixedcM.sh
Submitted batch job 20950469

# Thu Jun 21 23:44:06 PDT 2018
# out-of-order markers in the Ogut 2015 map need to be solved -- started process with clean_ogut_2015_map.R
# from Jeff: "Map and/or assembly error; Snp is in correct spot physically but stuck with mapping position
# assigned using bad genome" so I will drop all bad SNPs -- note they are clustered (e.g. on chr7 and affect ~3-5% markers)
# for now will use fixed BP pruning of sites until map issues are solved:

# updated pruneFixedBP.py from pruneFixed.py to take in multiple files and filter for MAF and number of ind's with data
# running w/ default min. ind. = 75 and separately w/ min. ind. = 100:
~/hilo/scripts$ sbatch pruneFixedBP.sh
Submitted batch job 21645706
~/hilo/scripts$ sbatch --export=MIN_IND=100 pruneFixedBP.sh
Submitted batch job 21645898
# both completed within a few minutes, pruning down to ~115k and ~46k
# I will start analysis with the 46K SNPs here at least 100 individuals have some data/reads

# breaking down 46k snps into chunks for beagle likelihood in angsd
~/hilo/data/var_sites/pass1/sympatric/pruned/minN100$ mkdir position_chunks
# combine and unzip; extract relevant columns (there are no headers .. so no need to take tail)
$ for i in {1..10}; do zcat thin10kb_chr$i.mafs.gz; done | cut -f 1-4 > all.positions
# split into chunks
$ split all.positions -d -a 3 -l 250 position_chunks/chunk
# run ANGSD - modified calcGL4Pruned.sh to take in variables as needed
# note: must set ALL variables separated by commas to overrule one variable
scripts$ sbatch --export=OUT_DIR=geno_lik/pass1/sympatricVar/pruned_chunks,IN_DIR=var_sites/pass1/sympatric/pruned/minN100/position_chunks/,BAM_LIST=pass1_bam.all.list \
--array=000-184 calcGL4Pruned.sh
Submitted batch job 21659878

# put it together into one GL file for NGSadmix
data/geno_lik/pass1/sympatricVar/pruned_chunks$ zcat chunk_000.beagle.gz | gzip >> ../pruned_all.beagle.gz; \
for i in {001..184}; do zcat chunk_$i.beagle.gz | tail -n +2 | gzip >> ../pruned_all.beagle.gz; done

# run new SNPs in NGSadmix for K=2,3,4 (modified runNGSadmix.sh to take in variables for IN and OUT files):
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/sympatricVar/pruned_all.beagle.gz,OUT_DIR=NGSadmix/pass1/sympatricVar/allInd/ runNGSadmix.sh

Submitted batch job 21672755

# pulling out genotype likelihood data for NGSadmix for just allopatric teosinte and sympatric maize
# to see how well it differentiates these groups
# first I get the GL header for all individuals in pass1: ~/hilo/data/pass1_bam.all.GLheader
# then I use R to extract the column # for individuals I am interested in: findGLColN.R
# saved in file data/pass1_bam.SympMaizeAlloMex05.GLcolN
~/hilo/data/geno_lik/pass1/sympatricVar$ zcat pruned_all.beagle.gz | cut -f$(cat ../../../pass1_bam.SympMaizeAlloMex05.GLcolN) | gzip >> pruned_SympMaizeAlloMex05.beagle.gz
# and re-running in NGSadmix:
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/sympatricVar/pruned_SympMaizeAlloMex05.beagle.gz,OUT_DIR=NGSadmix/pass1/sympatricVar/SympMaizeAlloMex05/ runNGSadmix.sh
Submitted batch job 21673267

# and same for first set of SNPs (globally ascertained including low cov. ind's and allopatric teosinte)
~/hilo/data/geno_lik/pass1$ zcat pruned_all.beagle.gz | cut -f$(cat ../../pass1_bam.SympMaizeAlloMex05.GLcolN) | gzip >> pruned_SympMaizeAlloMex05.beagle.gz
scripts$ sbatch --array=2-4 --export=GL_FILE=geno_lik/pass1/pruned_SympMaizeAlloMex05.beagle.gz,OUT_DIR=NGSadmix/pass1/SympMaizeAlloMex05/ runNGSadmix.sh
Submitted batch job 21673279

# getting a full set of SNPs with MAF > 0.05 freq in all sequenced ind's
# modified allVarAngsdGL.sh to get a count-based MAF and also output SAMtools style GL file
scripts$ sbatch allVarAngsdGL.sh
Submitted batch job 21690039
# some failed due to time-out at 36 hours (re-doing):
scripts$ sbatch --array=0,21,26,31,37 --mem=20G -t 108:00:00 allVarAngsdGL.sh
Submitted batch job 22351904
# region 0 failed due to insufficient memory at t = 12hrs; all others completed (not sure why they now took < 36hrs)
scripts$ sbatch --array=0 --mem=40G -t 96:00:00 allVarAngsdGL.sh
Submitted batch job 22502000
# now completed in 9 hrs
# concatenating all beagle gz files together for one genome-wide GL file:
# takes too long to do interactively:
scripts$ sbatch catBeagleGL.sh
Submitted batch job 22757237
# taking just every 100th position/line: _pruned_by100
scripts$ sbatch catBeagleGL_nth.sh
Submitted batch job 22757239
# -ISSUE IS THAT THIS OMITS THE FIRST HEADER! - FIXED. Creating file by1000.
scripts$ sbatch catBeagleGL_nth.sh
Submitted batch job 23015476
# Due to typo had to fix header here too:
(re-add header due to misslabelling issue)
hilo/data/geno_lik/pass1/allVar$ zcat whole_Genome_pruned_by1000.beagle.gz | gzip > pruned_by1000.beagle.gz; zcat whole_genome_pruned_by1000.beagle.gz | gzip >> pruned_by1000.beagle.gz


# I need to understanding if mapping bias is a problem -- what does mapping look like across groups (e.g. % mapped)? Is there biased mapping across my SNP sets?
# first I see if there are large differences in % reads that do not map
# I create a script to count the reads that align (map at all), do not map, pass mapQ >=30 and pass de-duplication
# SKIP THIS PART -- BETTER/FASTER TO USE data/filtered_bam/pass1.all.metrics.Nreads and pass1.all.metrics.raw.Nreads
scripts$ sbatch countReadsFiltered.sh
Submitted batch job 22758110
# saves output in text file, e.g. countReads/hilo_40.counts
# re-running files that TIMEOUT -- more time and less memory allotment
scripts$ sbatch --array=12,17-24,27-29,37,39,40,44-46,48-51,56-58,60-68,71,82,86-98,101,103-105,107,109-114,116-125,127-129,133,135-137,139,141,142,154,155,161,165,168-172,174-178,182,188,200 countReadsFiltered.sh
Submitted batch job 22765848
# still had many jobs cancel due to time out at 4 hours (!) - bumped up to 12 hours
scripts$ sbatch -t 12:00:00 --array=12,17-24,27-29,39,40,45-46,49-51,58,60-61,71,87-98,101,103-105,107,109-114,116-125,127,129,133,135-137,139,141,142,154,155,161,165,168-172,174-178,182,188,200 countReadsFiltered.sh
Submitted batch job 22799632
# still many jobs cancelled -- make new script that uses samtools built in view -c for 'counts' and trying again - countReadsFiltered2.sh
scripts$ sbatch countReadsFiltered2.sh
Submitted batch job 22937206  - I CANCELLED
# not running due to priority -- submitted other job
scripts$ sbatch -p med -t 24:00:00 countReadsFiltered2.sh
Submitted batch job 22967048 - I CANCELLED (SEE ABOVE FOR ALTERNATIVE FILES GENERATED IN filtered_bam/

# I can also zoom in and look at individuals' depth across specific regions:
~/hilo/data/filtered_bam$ samtools depth -r 1:400-500 hilo_18.sort.dedup.bam hilo_19.sort.dedup.bam


# Now I am working on getting a PCA using PCAngsd. This is better than NGStools PCA option because NGStools puts a prior on one population allele frequency -- so low coverage individuals end up near the center, whereas PCAngsd uses individually estimated allele frequencies (somehow without assuming they are indep.).
# to install python2.7 dependencies, I set up a python virtual environment for python 2.7.14 on my user on farm:
~/hilo$ module load python
~/hilo$ virtualenv --no-site-packages venv
# the --no-site-packages option is necessary to avoid conflicts between global packages and the ones in my venv
# activate virtual environment (need to add to top of any of my code now using this venv)
~/hilo$ source venv/bin/activate
~/bin/pcangsd$ pip install numpy Cython enum
~/bin/pcangsd$ pip install -r python_packages.txt
# problem installing numba due to llvm:
RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config
# install llvm:
wget http://releases.llvm.org/6.0.0/clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
~/software$ tar xf clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz
(venv) ecalfee@bigmem2:~/hilo$ export LLVM_CONFIG=/home/ecalfee/software/clang+llvm-6.0.0-x86_64-linux-gnu-ubuntu-16.04/bin/llvm-config
# based on online forum
~/hilo$ pip install funcsigs
# NONE OF THIS WORKED -- likely clang vs. gcc compiler issues (!)
# alternative route -- I've installed anaconda2
# to use, first set path to anaconda2 files:
export PATH=/home/ecalfee/software/anaconda2/bin:$PATH
# this time only, I create a new conda environment:
~/hilo$ conda create -y -n condaEnv
# every time, I need to activate the condaEnv:
~/hilo$ source activate condaEnv
# to this environment, I install llvmlite
(condaEnv) ecalfee@c8-62:~/hilo$ conda install -y llvmlite
# when I open python2, I can import all the pcangsd dependencies: numpy, scipy, pandas, numba
# so they appear to be pre-installed
# this example file now runs -- great!
(condaEnv) ecalfee@c8-62:~/hilo/data/TEST$ python2 ~/bin/pcangsd/pcangsd.py -beagle testANGSD.beagle.gz -threads 1 -iter 100 -o TESTpcangsd

# running PCA on set of pruned SNPs
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/pruned_all runPCAngsd.sh
Submitted batch job 22757232
# added -admix option and re-did:
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/pruned_all --mem=1G -t 3:00:00 runPCAngsd.sh
Submitted batch job 22758865
# and running PCA on set of pruned SNPs that vary within sympatric populations maize/teosinte
~/hilo/scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/sympatricVar/pruned_all runPCAngsd.sh
Submitted batch job 22757408
# added -admix option and re-did:
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/sympatricVar/pruned_all --mem=1G -t 3:00:00 runPCAngsd.sh
Submitted batch job 22758855
WORKED
# will also run PCA on data for all hilo individuals, at all SNPs in 'allVar'
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome --mem=2G -t 96:00:00 runPCAngsd.sh
Submitted batch job 22762726
CANCELLED-EXCEEDED MEMORY LIMIT. RE-RUNNING:
sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome --mem=12G -t 96:00:00 runPCAngsd.sh
Submitted batch job 22766166
# subset of all SNPs in 'allVar'
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome_pruned_by100 --mem=1G -t 36:00:00 runPCAngsd.sh
Submitted batch job 22762723
ERROR-DID NOT ACCEPT REDUCED FILE BY 100 .. NOT SURE WHY .. PROBLEM IS THAT IT'S MISSING A HEADER - I CAN FIX THAT:
hilo/data/geno_lik/pass1/allVar$ zcat whole_genome.beagle.gz | head -n 1 | gzip > whole_genome_every_100th_pos.beagle.gz; zcat whole_genome_pruned_by100.beagle.gz | gzip >> whole_genome_every_100th_pos.beagle.gz
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/whole_genome_pruned_by100 --mem=12G -t 36:00:00 runPCAngsd.sh
# the above file also won't run; trying again with ~100K SNPs from
# create SNPs using catBeagleGL_nth.sh -- takes every 1000th site
# run PCAngsd
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/pruned_by1000.beagle.gz --mem=4G -t 16:00:00 runPCAngsd.sh
Submitted batch job 23021131 - CANCELLED (error in file name)
Submitted batch job 23029600 - CANCELLED
scripts$ sbatch --export=GL_PREFIX=data/geno_lik/pass1/allVar/pruned_by1000 --mem=2G -p med -t 16:00:00 runPCAngsd.sh
Submitted batch job 23029537 - RAN, TOOK VERY LITTLE TIME (<10min and <1G)

# I will also estimate pairwise Fst between each group (allopatric/sympatric/maize/mex) and between populations within each larger group.
# to do Fst, I first need to calculate allele freq. spectrum for each population -- so I separate them into population bam files
# each meta population (no filtering for coverage except skipping files with no alignment in pass1)
~/hilo/data$ awk '$5 == "maize" && $6 == "sympatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/maize.symp.list ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ awk '$5 == "maize" && $6 == "sympatric" && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/maize.symp.list
~/hilo/data$ awk '$5 == "mexicana" && $6 == "sympatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/mexicana.symp.list ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ awk '$5 == "mexicana" && $6 == "sympatric" && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/mexicana.symp.list
~/hilo/data$ awk '$5 == "mexicana" && $6 == "allopatric" && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/mexicana.allo.list ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ awk '$5 == "mexicana" && $6 == "allopatric" && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/mexicana.allo.list
# give these numerical copies for ease: ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
~/hilo/data/pass1_bam_pops$ cp maize.symp.list pop1000.list
~/hilo/data/pass1_bam_pops$ cp mexicana.symp.list pop2000.list
~/hilo/data/pass1_bam_pops$ cp mexicana.allo.list pop3000.list
# each individual population
~/hilo/data$ for i in $(awk '$1 != "n" && $7 != "NA" {print $3}' HILO_IDs_cov_pass1.csv | sort | uniq); do awk -v i=$i '$3 == i && $7 != "NA" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' HILO_IDs_cov_pass1.csv >> pass1_bam_pops/pop$i.list; done ---> REDO AFTER LABEL FIX 8.15.18 AND ONLY FILTERING OUT LOW COVERAGE IND'S:
hilo/data$ for i in $(awk '$1 != "n" && $11 >= 0.05 {print $3}' pass1_ids.txt | sort | uniq); do awk -v i=$i '$3 == i && $11 >= 0.05 {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/pop$i.list; done

# all steps to SFS are listed here: http://www.popgen.dk/angsd/index.php/Fst
# should use many cores for step 2 and need to download realSFS software (actually it's part of ANGSD version already on farm)

# step 1: calculate individual site allele frequencies for each pop
scripts$ sbatch calcSAFAngsd.sh
Submitted batch job 21693057
# re-doing the 2 that didn't finish (time out cancellation):
scripts$ sbatch --mem=20G --array=1000,2000 -t 108:00:00 calcSAFAngsd.sh
Submitted batch job 22356436 -- will not finish (way too long)
# also an error with angsd -- too low a version, so I updated (see error & update log below)
# re-doing SAF split by chromosome:
# first for groups of pops, needs lots of memory and time:
~/hilo/scripts$ for i in 1000 2000 3000; do sbatch --export=POP=$i calcSAFAngsd_byChr.sh; done
Submitted batch job 22625518
Submitted batch job 22625519
Submitted batch job 22625520
# then for all individual pops, requires less memory. First put popN in a list:
~/hilo/scripts$ for i in {18..31} {33..35} {360..363} {365..374}; do echo $i >> ../data/pass1_bam_pops/all_pops.numbers; done
~/hilo/scripts$ for i in $(cat ../data/pass1_bam_pops/all_pops.numbers); do sbatch --mem=15G -t 24:00:00 --export=POP=$i calcSAFAngsd_byChr.sh; done
Submitted batch job 22628540-22628570
# re-submitting 11 jobs that ran out of time (modified script for more threads; less memory per thread):
sbatch --export=POP=22 --array=1 calcSAFAngsd_byChr.sh
Submitted batch job 22764908
sbatch --export=POP=20 --array=1-10 calcSAFAngsd_byChr.sh
Submitted batch job 22764942

# concatenate results by chromosome back together (skipping pops 20, 22 and larger groups for now):
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.sh
Submitted batch job 22765120
# fixed typo with --outname vs -outname & reran:
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.sh
Submitted batch job 22800872 -- DIDN'T WORK. fixed true typo -outnames vs. -outnames & order & reran:
scripts$ sbatch --array=18-19,23-31,33-35,360-363,365-374 catSAFAngsd.s
Submitted batch job 22801018

# step2: for each pair of populations, calculate the 2D SFS prior: (this is a lot of Fst calculations!)
s.t. like hilo$ for i in {18..31} {33..35} {360..363} {365..374}; do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; then echo $i $j; fi; done; done
# problem: getting the following error:
realSFS print TEST/pop31.pop22.old.fst.gz
	-> Version of fname:TEST/pop31.pop22.old.fst.gz is:0
	-> Looks like you are trying to use a version of realSFS that is incompatible with the old binary output from ANGSD
	-> Please use realSFS.old instead (or consider redoing the saf files )
	-> Will exit
  # updating ANGSD on farm:
  ecalfee@bigmem2:~/bin$ module load zlib
  Module zlib/1.2.8 loaded
  ecalfee@bigmem2:~/bin$ module load htslib
  Module htslib/1.3.2 loaded
  # download newest angsd official release and accompanying htslib:
  ~/bin$ wget http://popgen.dk/software/download/angsd/angsd0.920.tar.gz
  # unzip
  ~/bin$ tar xf angsd0.920.tar.gz
  # install htslib
  ~/software/htslib$ make install prefix=../bin
  # install angsd using link to my local htslib directory
  ~/software/angsd$ make HTSSRC=../htslib/
  # moved exec. file software/angsd/angsd to bin/ and software/angsd/misc/realSFS to bin/ too and v1.6 of tabix and htsfile too
# to calculate Fst between population pairs:
# running now for all pop pairs except 20 & 22
scripts$ for i in 18 19 21 {23..31} {33..35} {360..363} {365..374}; do for j in 18 19 21 {23..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; then sbatch --export=POP1=$i,POP2=$j calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 22800219-22800624
# NOTE : There has to be a better way to organize these jobs into one 'super job'
scripts$ for i in {18..31} {33..35} {360..363} {365..374}; \
do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; \
then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 23021289-23021760
4 jobs not running -- requeue-ing 23021318-21 from c10-92:
scripts$ sbatch --export=POP1=18,POP2=274,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 calc2DSFSAngsd.sh
Submitted batch job 23026718
scripts$ for j in {20..22}; \
do sbatch --export=POP1=19,POP2=$j,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 calc2DSFSAngsd.sh; done
Submitted batch job 23026842-4

# now calculating Fst pairwise:
scripts$ for i in {18..31} {33..35} {360..363} {365..374}; \
do for j in {18..31} {33..35} {360..363} {365..374}; do if [ $j -gt $i ]; \
then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions \
-t 1:00:00 -x c10-92 calcFSTAngsd.sh; fi; done; done
Submitted batch job 23026944-23027416 - ALL DONE - rerunning ones missing fst output files:
scripts$ sbatch --export=POP1=18,POP2=19,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031530 - hung up
Submitted batch job 23031925
scripts$ sbatch --export=POP1=18,POP2=30,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031543
scripts$ sbatch --export=POP1=20,POP2=370,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031546
scripts$ sbatch --export=POP1=22,POP2=26,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031554
scripts$ sbatch --export=POP1=27,POP2=362,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031559
scripts$ sbatch --export=POP1=29,POP2=360,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031568
scripts$ sbatch --export=POP1=361,POP2=362,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031574
scripts$ sbatch --export=POP1=362,POP2=363,DIR=SAF/pass1/N1000.L100.regions -t 1:00:00 -x c10-92 calcFSTAngsd.sh
Submitted batch job 23031581

# combine Fst results into one file:
hilo/data/SAF/pass1/N1000.L100.regions$ echo -e "pop1 \t pop2 \t Fst_Hudson \t Fst_avg_of_ratios " \
> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.fst.stats
hilo/data/SAF/pass1/N1000.L100.regions$ for i in {18..31} {33..35} {360..363} {365..374}; \
do for j in {18..31} {33..35} {360..363} {365..374}; \
do if [ $j -gt $i ]; then awk -v i=$i -v j=$j '{print i "\t" j "\t" $1 "\t" $2}' \
pop$i.pop$j.fst.stats >> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.fst.stats; fi; done; done

# just submitting pairs for 2D-SFS between larger groups
scripts$ for i in 1000 2000 3000; do for j in 1000 2000 3000; do if [ $j -gt $i ]; \
then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 23013831-3 - COMPLETED

# and then calculating pairwise Fst using Hudson/Bhatia2013 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3759727/) estimator = 1 (vs. Reynolds estimator = 0). 3rd/4th column in the .fst.idx file are the numerator & denominator of the statistic at each locus (see github discussion/documentation: https://github.com/ANGSD/angsd/issues/61).
scripts$ for i in 1000 2000 3000; do for j in 1000 2000 3000; do if [ $j -gt $i ]; then sbatch --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; fi; done; done
Submitted batch job 23014153-5
# I will use 'weighted' Fst (ratio of averages ... vs unweighted = average of ratios across loci).
# creating a summary file of weighted Fst between groups
hilo/data/SAF/pass1/N1000.L100.regions$ echo -e "pop1 \t pop2 \t Fst_Hudson \t Fst_avg_of_ratios " > ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.group.fst.stats
hilo/data/SAF/pass1/N1000.L100.regions$ for i in 1000 2000 3000; do for j in 1000 2000 3000; \
do if [ $j -gt $i ]; then awk -v i=$i -v j=$j '{print i "\t" j "\t" $1 "\t" $2}' \
pop$i.pop$j.fst.stats >> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.group.fst.stats; fi; done; done


# This whole Fst estimation should be done with a set of regions. For SAF calculations, I can provide a set of sites or a region
TEST$ angsd sites index sites_5.txt
TEST$ realSFS print pop18.saf.idx -sites sites_5.txt
TEST$ realSFS print pop18.saf.idx -r 1:500-600
# but a regions file doesn't work
TEST$ realSFS print pop18.saf.idx -rf regions_5.txt - error
# I may be able to either loop over a regions file in a short script, and cat together the SAF's in the end
# or to supply an -rf file to angsd when making the SAF file
# the following works, where regions.txt has regions e.g. 1:100-200 listed
*regions must be sorted!!
data$ angsd -out TEST/SAFbyRegion_test_pop18 -anc /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa -fold 1 -rf TEST/regions.txt -bam pass1_bam_pops/pop18.list -remove_bads 1 -minMapQ 30 -GL 1 -dosaf 1

# Getting a set of random regions from the maize reference genome:
# bedtools random function will work, but I need to make a 'genome' file with chromosome names & lengths first:
hilo/data$ cut -f1,2 /group/jrigrp/Share/assemblies/Zea_mays.AGPv4.dna.chr.fa.fai | grep -v 't' > refMaize/Zea_mays.AFPv4.dna.chr.autosome.lengths
# now generating 1000 random regions of 100bp each: data/refMaize/random_regions/N1000.L100.regions
scripts$ sbatch getRandomRegions.sh
Submitted batch job 22934334
# and calculating SAF off of these regions using an updated calcSAFAngsd.sh script:
scripts$ sbatch --array=18-31,33-35,360-363,365-374 --mem=4G calcSAFAngsd.sh
Submitted batch job 22934857
scripts$ sbatch --array=1000,2000,3000 --mem=12G -t 24:00:00 calcSAFAngsd.sh
Submitted batch job 22934957

# made new R script to visualize population structure and admixture in my sample - PCA, Fst and mapping to maize for allopatric & sympatric teosinte & sympatric maize
# scripts/eval_PCA_Fst.R
# need to add in allopatric maize. One potential option is downloading HapMap3 bams, a subset of which are 282, but I'm having trouble identifying tropical/subtropical TS lines from the large # in hapmap3 https://cbsusrv04.tc.cornell.edu/users/panzea/filegateway.aspx?category=Genotypes
# to access iplant from farm:
module load icommands
iinit (then follow suggestions by: https://pods.iplantcollaborative.org/wiki/display/DS/Setting+Up+iCommands)
other icommands (e.g. ils, icd): https://pods.iplantcollaborative.org/wiki/display/DS/Using+iCommands
# then use crossmap http://crossmap.sourceforge.net/ and a chain to lift over from v3 to APGv4 reference genome:
ftp://ftp.ensemblgenomes.org/pub/plants/release-32/assembly_chain/zea_mays/ (chain)
Another option is to get original reads from NCBI <<but this is a maze of nonexistent plant samples https://www.ncbi.nlm.nih.gov/biosample/7209658 >>
# the 282 vcf v4 is on farm: /group/jrigrp/Share/genotypes/282_7X/c${i}_282_corrected_onHmp321.vcf.gz
# these have GL's for SNPs that vary within this sample, GT:AD:GL e.g.
# 0/1:7,2:28,0,167 means 0/1 is the called genotype; 7 ref allele & 2 alt allele counts; 28,0,167 are the genotype likelihoods
##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">
##FORMAT=<ID=AD,Number=.,Type=Integer,Description="Allelic depths for the reference and alternate alleles in the order listed">
##FORMAT=<ID=GL,Number=.,Type=Integer,Description="Genotype likelihoods for 0/0, 0/1, 1/1, or  0/0, 0/1, 0/2, 1/1, 1/2, 2/2 if 2 alt alleles">

# making new script to calculate coverage per individual for variant sites in HILO pass1:
scripts$ sbatch calcDepthCovSites.sh
Submitted batch job 23079251 - fixed typo & reran:
Submitted batch job 23079342 - CANCELLED - OUT OF MEMORY > 20g/3nodes at chr2
hilo/scripts$ sbatch calcDepthCovSites.sh - new edits for more nodes, and arrayed by chromosome
Submitted batch job 23505206 - COMPLETED; results files split by CHROM. CHR1 did not finish.
hilo/scripts$ sbatch --mem=30G --array=1 calcDepthCovSites.sh
Submitted batch job 23778351
scripts$ sbatch calcDepthCovRegions.sh
Submitted batch job 23079256 - COMPLETED

# 282 panel doesn't have easy aligned bams -- GL for SNPs is preserved in vcf file. The issue is whether there is significant bias between 282 SNPs and SNPs from admix panel, e.g. fixed differences. If there is, I will have to re-align fastq from 282. Checking SNP overlap:
scripts$ sbatch checkSNPOverlap282.sh

# I am adding in bams for allopatric Maize from Anne: 4 landrace populations, 10 ind's each (same pop = sequentially together)
# first I make a list of the bam file locations on farm
ecalfee@farm:/group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK$ for i in $(ls *removedup_realigned.bam); do echo /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/$i >> ~/hilo/data/bam.maize4popLowland.list; done
# and a combined list for calling new SNPs
ecalfee@farm:~/hilo/data$ cat pass1_bam.all.list bam.maize4popLowland.list > merged_bam.pass1_all.maize4popLowland.list
# Then I get GL at variant sites across the combined sample, now including lowland maize:
scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list allVarAngsdGL.sh
Submitted batch job 23526847 -- Did not run: Mt and Pt chromosomes not in bams from Anne (allopatric maize);
# changed allVarAngsdGL.sh to have a new variable to not check bam headers for same chromosomes;
# ok because I'm restricting analysis to the shared autosomes 1-10 -- THIS WAS A BAD IDEA -- IT'S NOT FINDING THE CORRECT CHROMOSOMES (!) -- MANY SNPs @ 50% freq.
scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23551911 -- ugh, typo (now fixed)
scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23552475 - ERROR IN FILE NAMES (1 instead of 01..)
# creating symlinks for all bams from anne lorant 4 pop lowland maize so that they are local and named consistently
# and I can re-index
# creating symbolic link for #1
~/hilo/data/alloMaize4pop_symlink_bam$ ln -s /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/JRIAL11-1_removedup_realigned.bam maizeLow_01.bam
# and for the rest:
for i in {02..40}; do ln -s /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/JRIAL11-"$i"_removedup_realigned.bam maizeLow_"$i".bam; done
# remaking files with list of bams for lowland
data$ for i in {01..07} {09..40}; do echo alloMaize4pop_symlink_bam/maizeLow_$i.bam >> bam.maize4popLowland.list; done
data$ cat pass1_bam.all.list bam.maize4popLowland.list > merged_bam.pass1_all.maize4popLowland.list
ecalfee@bigmem1:~/hilo/scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23553309 - did not run. remade symlinks wiht proper "":
Submitted batch job 23553370 - took out 08 from alloMaize4popLow:
Submitted batch job 23553429 - RUNNING; I cancelled (needs index files to run)
# indexing allopatric maize symlinks
~/hilo/data/alloMaize4pop_symlink_bam$ (!) I put this into a script samtools index maizeLow_"$i".bam
scripts$ sbatch indexAllo4PopMaize.sh
Submitted batch job 23553496 - Fixed typo below:
Submitted batch job 23554317 - COMPLETED
# now running GL again:
~/hilo/scripts$ sbatch --export=DIR_OUT=geno_lik/merged_pass1_maize4popLowland/allVar,BAM_IN=merged_bam.pass1_all.maize4popLowland.list,CHECK_BAM_HEADERS=0 allVarAngsdGL.sh
Submitted batch job 23563805 -- TOO MEMORY INTENSE -- SOME ARE COMPLETE BUT OTHERS NOTE EVEN CLOSE -- WILL NEVER FINISH.
ALTERNATIVES:
# (1) I find variable sites first (and then will calculate GL for a subset)
# (2) I split the genome into a large number of regions across multiple files (approximately retrieving 1/1000th of the maize total genome)
~/hilo/scripts$ sbatch getRandomRegions_andSplit.sh
Submitted batch job 23570489 - FIXED TYPO:
Submitted batch job 23571407
# after I will use these regions files to create GL files for each set of regions.

# I ALSO NEED TO ADDRESS THIS ERROR -- THERE ARE MANY MANY SNPs at > 50% freq; I THINK THE BAM HEADERS ARE INDEED A PROBLEM, OFFSETTING BY 1 CHRM.
# error in file: Trying to access fasta efter end of chromsome+200:9/9 pos=181122463 ref_len=159769782
"geno_lik/merged_pass1_maize4popLowland/allVar/region_42.arg"
# and again in region 38: Trying to access fasta efter end of chromsome+200:8/8 pos=182381402 ref_len=181122637
# I am re-ordering bams using picard tools -- reorderBam4popAlloMaizeLow.sh (starting with just 16/40 individuals, 4 per pop)
~/hilo/scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23571569 - FIXED DIRECTORY TYPO and made scratch directory in my home folder instead of local to each node (lack of space)
Submitted batch job 23576593 -Failed due to lack of 'sequence dictionary'. will make one. and will make a symlink for reference genome too in makeSeqDictRef.sh script:
scripts$ sbatch makeSeqDictRef.sh
Submitted batch job 23576612 - fix typo:
Submitted batch job 23576616 - COMPLETED
# edited reorderBam4popAlloMaizeLow.sh to refer to local reference AGPv4.fa and .dict
~/hilo/scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23576622 -- produced an error and truncated bams

# making a small test file to avoid future errors in the bams/reordering etc.
data/alloMaize4pop_symlink_bam$ samtools view -b maizeLow_01.bam "10:10-20" > maizeLow_99.bam # subset of reads
~/hilo/scripts$ sbatch --array=99 -t 10:00 --mem=1G -p med reorderBam4popAlloMaizeLow.sh
Submitted batch job 23706200 #e.g. run, tried many; produces no error ONLY IF I don't provide a maizeLow_99.bam.bai index file (or maizeLow_99.bai) from samtools
# making slightly larger test region:
data/alloMaize4pop_symlink_bam$ samtools view -b maizeLow_01.bam "10:1000-2000" > maizeLow_98.bam
scripts$ sbatch --array=98 -t 10:00 --mem=1G -p med reorderBam4popAlloMaizeLow.sh
Submitted batch job 23706846


# also making a small test file for a hilo individual
data/TEST$ samtools view -b ../filtered_bam/hilo_23.sort.dedup.baq.bam "10:1000-2000" > hilo_23.subset10.1-2k.bam
data/TEST$ samtools index hilo_23.subset10.1-2k.bam
TEST$ echo hilo_23.subset10.1-2k.bam > test_veryshort_bams.list
TEST$ echo maizeLow_98.reordered.bam >> test_veryshort_bams.list
# ANGSD is able to run with the reordered pair:
~/hilo/data/TEST$ angsd -out test_small_maizeLow_hilo -ref ../refMaize/AGPv4.fa \
-bam test_veryshort_bams.list -remove_bads 1 \
-minMapQ 30 -minQ 20 \
-doMajorMinor 2 \
-doCounts 1 -minMaf 0.05 -doMaf 8 \
-GL 1 -doGlf 2

# REORDERING MAIZE_LOW BAMS:
# moved all .bai files to /outdated/ to temporarily save
scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23707049
-- ALL BUT IND01 CANCELLED BECAUSE IT FOUND THE .BAI FILE FOLLOWNG THE SYMLINK
# 01 may be wrong too because it
# I am trying another solution: just giving a new header to Anne's bams addint mt and pt chromosomes (chr10 will still come out of order, first)
~/hilo/data$ mkdir -p alloMaize/maizeLow/re_head # make new directory
~/hilo/data/alloMaize/maizeLow$ samtools view -H /group/jrigrp6/RILAB_data/LR/JRIAL11/bam/LR_GATK/JRIAL11-02_removedup_realigned.bam > new_header_with_mt.sam
# edit header with nano, adding these 2 lines:
@SQ	SN:Pt	LN:140384
@SQ	SN:Mt	LN:569630

# modified header of the short bam file
~/hilo/data/alloMaize/maizeLow$ samtools reheader new_header_with_mt.sam ../../alloMaize4pop_symlink_bam/maizeLow_98.bam > re_head/maizeLow_TEST.bam
~/hilo/data/alloMaize/maizeLow/re_head$ samtools index maizeLow_TEST.bam
~/hilo/data/alloMaize/maizeLow/re_head$ mv maizeLow_TEST.bam* ../../../TEST/.
# test ANGSD for full mini files
~/hilo/data/TEST$ echo maizeLow_TEST.bam > TEST_ANGSD_GL_rehead_bams.list
~/hilo/data/TEST$ echo hilo_23.subset10.1-2k.bam >> TEST_ANGSD_GL_rehead_bams.list
~/hilo/data/TEST$ angsd -out TEST_ANGSD_GL_rehead -ref ../refMaize/AGPv4.fa -minMapQ 30 -minQ 20 -remove_bads 1 -doMajorMinor 2 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -bam TEST_ANGSD_GL_rehead_bams.list
-- THE ABOVE WILL NOT WORK -- with or without a region -r listed
# test with the test re-ordered bam
ecalfee@bigmem7:~/hilo/data/TEST$ echo maizeLow_98.reordered.bam > TEST_ANGSD_GL_reorder_bams.list
ecalfee@bigmem7:~/hilo/data/TEST$ echo hilo_23.subset10.1-2k.bam >> TEST_ANGSD_GL_reorder_bams.list
~/hilo/data/TEST$ angsd -out TEST_ANGSD_GL_reorder -ref ../refMaize/AGPv4.fa -minMapQ 30 -minQ 20 -remove_bads 1 -doMajorMinor 2 -doCounts 1 -minMaf 0.05 -doMaf 8 -GL 1 -doGlf 2 -bam TEST_ANGSD_GL_reorder_bams.list
-- THE ABOVE WORKED ON TEST FILES --
# So I am trying reordering again, but where I copy the bams first so they have
# no bai files (and I delete intermediate copied pre-reordering bams at the close of the script)
ecalfee@bigmem7:~/hilo/scripts$ sbatch reorderBam4popAlloMaizeLow.sh
Submitted batch job 23752487 -- ONLY individual 01 ran without issue. Changed output names & scratch directory (issue was couldn't write to scratch) & running all but ind 01
~/hilo/scripts$ sbatch --array=2-4,11-14,21-24,31-34 reorderBam4popAlloMaizeLow.sh
Submitted batch job 23776475
# copying over file for individual 1 (with index)
~/hilo/data$ cp alloMaize/maizeLow/reordered_01.bam alloMaize4Low/.
~/hilo/data$ cp alloMaize/maizeLow/reordered_01.bai alloMaize4Low/.
~/hilo/data$ rm -r alloMaize/
# made list of new bams
data$ for i in {01..04} {11..14} {21..24} {31..34}; do echo alloMaize4Low/reordered_$i.bam >> alloMaize4Low_16_bam.list; done
# and combined list
data$ cat pass1_bam.all.list alloMaize4Low_16_bam.list merged_bam.pass1_all.alloMaize4Low_16.list
# re-running; delayed to start when the last reorder job finishes
scripts$ sbatch --dependency=afterok:23776475_12 --export=DIR_OUT=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,BAM_IN=merged_bam.pass1_all.alloMaize4Low_16.list allVarAngsdGL.sh
Submitted batch job 23807271 -- long time to make priority : CANCELLED (see below, running on smaller regions)
# To reduce output time and computations I made a new script to just get variant sites by region:
scripts$ sbatch --export=OUT_DIR=var_sites/pass1_alloMaize4Low,BAM_LIST=merged_bam.pass1_all.alloMaize4Low_16.list callVarAngsd_Regions.sh
Submitted batch job 23905396 -- CANCELLED. WILL REWRITE FOR SMALLER TASKS AND RUN ON LOW PRIORITY.
# Got permission to run as 'high priority' on bigmemh. Made 0-425 new regions (each 5Mb) and running with just 1 core
# (no threading) ... and letting 50 tasks run at one time: (!) mistake -- only use 8 cpu's at a time on bigmemh!
ecalfee@bigmem3:~/hilo/scripts$ sbatch -p bigmemh allVarAngsdGL1.sh --Jeff cancelled & restarted
Submitted batch job 24226853
# attempting to rerun those that cancelled due to memory surplus as short jobs on low priority
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=0,1,25,39,75,81,92,93,97,103 --mem=12G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24263414
ecalfee@bigmem3:~/hilo/scripts$ sbatch -p bigmeml --array=0 --mem=24G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24264289 (WHY DOES REGION 0 NEED SO MUCH MEMORY?) -- DOUBLING MEMORY AGAIN:
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=0 --mem=48G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24268274
# and re-running 39. All other 'low priority' tasks finished:
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=39 --mem=24G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24269347 - EXCEEDED MEMORY
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=39 --mem=48G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24271818 - COMPLETED
ecalfee@bigmem2:~/hilo/scripts$ sbatch -p bigmeml --array=127 --mem=24G -t 2:00:00 allVarAngsdGL1.sh
Submitted batch job 24271599 - COMPLETED
# and cancelled then re-submitted other jobs not yet queued with higher (max-per-node memory = 8G) but lower walltime (3 hrs max)
# will start after this set of 8 completes
ecalfee@bigmem2:~/hilo/scripts$ sbatch --dependency=afterany:24230156 --array=148-425%8 -t 3:00:00 --mem=8G allVarAngsdGL1.sh
Submitted batch job 24263714
# because I accidentally set it to medium priority, I'll get rid of the task limit and put the last 100 on high priority:
scontrol update ArrayTaskThrottle=0 JobId=24263714
scancel 24263714_[325-425] - RAN WITH SOME CANCELLED (Memory/timeout)
ecalfee@bigmem2:~/hilo/scripts$ sbatch --array=325-425%6 -t 3:00:00 --mem=8G -p bigmemh allVarAngsdGL1.sh
Submitted batch job 24272202 - RAN WITH SOME CANCELLED (memory/timeout)
Re-running ones that timed out on bigmemh with longer times and 2x the memory:
~/hilo/scripts$ sbatch --array=324,178,179,181,216,257,262,281%4 -t 6:00:00 --mem=16G -p bigmemh allVarAngsdGL1.sh
Submitted batch job 24378121 - COMPLETED
Re-running ones that were memory short on bigmeml with 2 hours and 3x the memory:
~/hilo/scripts$ sbatch --array=361,358,252,256,298,320 -t 2:00:00 --mem=24G -p bigmeml allVarAngsdGL1.sh
Submitted batch job 24378629 - COMPLETED
# Concatenating results into a file for every 1000th SNP:
scripts$ sbatch -p bigmemh --export=startR=0,endR=425,DIR_GL=data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,N=1000 catBeagleGL_nth.sh
Submitted batch job 24402884 -- ERROR FOR LOOP
Submitted batch job 24402416 -- ERROR BASH VARIABLES
Submitted batch job 24402989 -- ERROR WORKING DIRECTORY
scripts$ sbatch -p bigmemh --export=startR=0,endR=425,DIR_GL=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,N=1000 catBeagleGL_nth.sh
Submitted batch job 24403123
# and finally calculating PCA for this subset of SNPs
scripts$ sbatch -p bigmemh --dependency=afterok:24403123 --export=GL_PREFIX=data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/whole_genome_pruned_every_1000 runPCAngsd.sh
Submitted batch job 24403224 - DONE - RENAMED ALL FILES with .partial. so I can rerun below
# ran but I should re-do PCA etc. because it's missing a 2 segments that didn't complete GL properly:
~/hilo/scripts$ sbatch --array=261,415 -t 6:00:00 --mem=16G -p bigmemh allVarAngsdGL1.sh
Submitted batch job 24414246
scripts$ sbatch -p bigmemh --dependency=afterok:24414246 --export=startR=0,endR=425,DIR_GL=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar,N=1000 catBeagleGL_nth.sh
Submitted batch job 24414522
~/hilo/scripts$ sbatch -p bigmemh --dependency=afterok:24414522 --export=GL_PREFIX=data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/whole_genome_pruned_every_1000 runPCAngsd.sh
Submitted batch job 24414553




# FST

# I save the allopatric maize list of bams also as pop 4000:
ecalfee@farm:~/hilo/data$ cp bam.maize4popLowland.list pass1_bam_pops/pop4000.list
# then I calculate SAF to get Fst between this allopatric maize landraces and the sympatric maize/mex and allopatric mex
# calcualte SAF for regions:
scripts$ sbatch --array=4000 -t 10:00:00 --mem=4G calcSAFAngsd.sh
Submitted batch job 23526611 - on hold: (AssocGrpJobsLimit) - did not finish because of lack of index files (fixed see above)
Submitted batch job 23563858 - NODE FAIL (will rerun excluding bigmem5:)
scripts$ sbatch --array=4000 -t 10:00:00 --mem=4G -x bigmem5 calcSAFAngsd.sh
Submitted batch job 23564165 - COMPLETED.
# and pairwise SFS between 4000 and 1000, 2000, 3000:
scripts$ for i in 1000 2000 3000; do sbatch --export=POP1=$i,POP2=4000,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; done
Submitted batch job 23571618-20 - COMPLETED
# then Fst pairwise with 4000 and other population groups:
scripts$ for i in 1000 2000 3000; do sbatch --export=POP1=$i,POP2=4000,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; done
Submitted batch job 23903001-3 - COMPLETED (and not very different from below..it worked fine despite not re-ordered)

# RE-DOING FST after REORDERING BAMS -- there could be mistakes in original above due to comparing
# SAF across individuals with same chromosome names but diff. order (I'm not sure of effect):
# made new bam list 5000 for 16 bams the that I re-ordered (4 per pop from allopatric lowland Maize):
~/hilo/data$ cp alloMaize4Low_16_bam.list pass1_bam_pops/pop5000.list
# calculate SAF for regions:
scripts$ sbatch --array=5000 -t 12:00:00 --mem=4G calcSAFAngsd.sh
Submitted batch job 23902868 - ERROR FROM IND 01. Recopied over reordered bam for this ind & reran:
Submitted batch job 23903106 - I CANCELLED immediately (arguments after script.sh were being ignored)
Submitted batch job 23903114 - COMPLETED
# calculate pairwise SFS between 5000 and 1000, 2000, 3000
scripts$ for i in 1000 2000 3000; do sbatch --dependency=afterok:23903114 --export=POP1=$i,POP2=5000,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; done
Submitted batch job 23903227-9 -COMPLETED
# then calculate pairwise Fst between all groups
scripts$ for i in 1000 2000 3000; do sbatch --dependency=afterok:23903227,23903228,23903229 --export=POP1=$i,POP2=5000,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; done
Submitted batch job 23903329-31 - COMPLETED
# add results to summary file
~/hilo/data/SAF/pass1/N1000.L100.regions$ for i in 1000 2000 3000; do awk -v i=$i '{print i "\t" 5000 "\t" $1 "\t" $2}' pop$i.pop5000.fst.stats >> ~/hilo/data/SFS/pass1/N1000.L100.regions/pairwise.group.fst.stats; done

# MAPPING AND COVERAGE METRICS -- I moved the first plots on mapping out of eval_PCA_Fst.R to
# their own script: plot_mapping_metrics.R . Then I added other coverage analyses
# for (1) Fst regions
# and (2) a subset of variant sites called in hilo;
# var_sites/pass1/pruned_positions/all.positions (~170k SNPs separated by >= 10kb; used in initial admixture analysis; with over 100 individuals with some data (MORE than cutoff for other SNPs)).

# Adding 282 reference maize data
# Getting 282 SRAs for download from NCBI - ACTUALLY michelle will align 282 'ts' tropical/subtropical bams for me
hilo/data/maize282$ wget -O maize282_SRA_SRP108889 'http://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?save=efetch&db=sra&rettype=runinfo&term="SRP108889"[All Fields]'
# made symlinks for vcf files (variants re-positioned onto genome AGPv4):
~/hilo/data/maize282/vcf_AGPv4$ for i in {1..10}; do ln -s /group/jrigrp/Share/genotypes/282_7X/c${i}_282_corrected_onHmp321.vcf.gz chr$i.vcf.gz; done
# new script to make smaller vcf's with just thinned SNPs for PCA from 282 panel
scripts$ sbatch -p bigmemh --array=1-10%6 subset_282var_nth.sh
Submitted batch job 24423018 -- ERROR (FIXED & RERUNNING BELOW)
Submitted batch job 24484991
# now pulling out a sites file to use in angsd which has chr, pos, major, minor alleles
~/hilo/data/maize282/vcf_AGPv4$ for i in {1..10}; do grep -v "#" subset_chr$i.recode.vcf | awk '{print $1"\t"$2"\t"$4"\t"$5}' > subset_chr$i.sites; done
# and making a matching regions file that just has chr:pos for every site
~/hilo/data/maize282/vcf_AGPv4$ for i in {1..10}; do grep -v "#" subset_chr$i.recode.vcf | awk '{print $1":"$2}' > subset_chr$i.regions; done
# now getting genotype likelihoods for all hilo individuals and lowland maize 16 individuals for these sites
# using samtools GL method:
~/hilo/scripts$ sbatch --array=1-10%4 -p bigmemh calcGL4maize282sites.sh
Submitted batch job 24490030 - FIXED QUOTES AROUND VARIABLES:
Submitted batch job 24490494 - MORE QUOTE FIXES:
Submitted batch job 24490702 - NEEDED DOUBLE BACKSLASH \\
Submitted batch job 24490876 - MORE BACKSLASH
Submitted batch job 24491053 - TO FORWARD SLASH
Submitted batch job 24491358
# Now I'll reformat the GL's from the vcf file into ones matching BEAGLE expectations using a python script


# getting new metadata - cloning from JRI github 'riplasm':
hilo/data$ git clone https://github.com/rossibarra/riplasm.git

# PI / diversity statistics (theta estimates):
# To get within-pop diversity genomewide, I use a small set of regions and calculate a SFS per pop (on bigmeml)
scripts$ sbatch -p bigmeml calc1DSFSAngsd.sh
Submitted batch job 24302961 - ERROR in calling SFS program (removed misc/)
scripts$ sbatch -p bigmeml calc1DSFSAngsd.sh
Submitted batch job 24403505 - RAN & COMPLETED WITHIN SECONDS


# GOT NEW LABELS / POPULATION IDs for HILO INDIVIDUALS HILO161-HILO200 + FIXED LABEL SWAP ERROR BETWEEN HILO61 and HILO66 (swap hand-written in lab notes):

# running NGSadmix with addition of 16 lowland 4 pop allopatric maize individuals and SNPs pruned to 1/1000:
~/hilo/scripts$ sbatch -p bigmemh --array=2-4 --export=GL_FILE=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/whole_genome_pruned_every_1000.beagle.gz,OUT_DIR=NGSadmix/merged_pass1_all_alloMaize4Low_16/ runNGSadmix.sh
Submitted batch job 24980206 - COMPLETED

# made new script that links metadata to hilo ID using updated ID-population link from Anne 8.8.18:
addMetadata2HiloIDs.R creates new files hilo_ids.txt and pass1_ids.txt and pass1_allo4Low_ids.txt
# To avoid confusion I put all txt and csv files with the old labels (pre fixing the label swap)
# in a separate directory e.g. "../data/pre_label_fix/HILO_samples.csv"
# This resolved the label switch error (yay!). The only thing left is to switch HILO61 and HILO66 that have a documented mix up in field notes - DONE 8.15.18
# And to remake files listing bams by population
# Some but not all plots have been re-made with the new labels
# Also still working on how to drop out-of-order SNPs from recombination map

# RE-DOING FST WITH NEW LABELS AND EXCLUDING INDIVIDUALS WITH LESS THAN 0.05x COVERAGE
# calcualte SAF for regions:
# edited SAF script to only use 1 core for computing (should be light computing with just using regions)
hilo/scripts$ sbatch --array=1000,2000,3000,5000 -p bigmemh -t 10:00:00 --mem=4G calcSAFAngsd.sh
Submitted batch job 25459455
# then on med calc 2D SFS
scripts$ for i in 1000 2000 3000 5000; do for j in 1000 2000 3000 5000; do if [ $j -gt $i ]; then sbatch --dependency=afterok:25459455 --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calc2DSFSAngsd.sh; fi; done; done
Submitted batch job 25459940-5
# then calculate pairwise Fst between all groups
scripts$ for i in 1000 2000 3000 5000; do for j in 1000 2000 3000 5000; do if [ $j -gt $i ]; then sbatch --dependency=afterok:25459940,25459941,25459942,25459943,25459944,25459945 --export=POP1=$i,POP2=$j,DIR=SAF/pass1/N1000.L100.regions calcFSTAngsd.sh; fi; done; done
Submitted batch job 25459976-81 -- FIXED TYPO w/ always 2nd pop = 5000 & reran:
Submitted batch job 25461663-8 - COMPLETED
# add results to summary file
~/hilo/data/SAF/pass1/N1000.L100.regions$ for i in 1000 2000 3000 5000; do for j in 1000 2000 3000 5000; do if [ $j -gt $i ]; then awk -v i=$i -v j=$j '{print i "\t" j "\t" $1 "\t" $2}' pop$i.pop$j.fst.stats >> pairwise.group.fst.stats; fi; done; done
RE-CALCULATING PI WITHIN GROUPS
hilo/scripts$ sbatch -p bigmeml --array=1000,2000,3000,5000 -t 30:00 calc1DSFSAngsd.sh
Submitted batch job 25462433 - COMPLETED (THEN CALCULATE PI USING R SCRIPT calcPifromSFS.R)

DOING ABBA-BABA TEST
First allo_mex symp_mex symp_maize allo_maize (underpowered to detect sympatric gene flow) doAbbaBaba4pop_maize.sh
here, allopatric maize is the stand in 'outgroup'
# helper files:
# bam list in population order
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list pass1_bam_pops/maize.symp.list alloMaize4Low_16_bam.list; do cat $i >> 4pop_maize_bam.list; done
# size of populations file
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list pass1_bam_pops/maize.symp.list alloMaize4Low_16_bam.list; do wc -l $i | awk '{print $1}' >> 4pop_maize_bam.sizeFile; done
# try to run:
hilo/scripts$ sbatch -p bigmemh doAbbaBaba4pop_maize.sh
Submitted batch job 25463604
# 3 argument      100 is unknown will exit -- I'll try not specifying a block size
Submitted batch job 25463679 - typo
Submitted batch job 25463718 - still won't recognize arguments (?) may need to download latest ANGSD version from git
# Fixed problem with script and rerunning:
hilo/scripts$ sbatch -p bigmemh --mem=8G doAbbaBaba4pop_maize.sh
Submitted batch job 25496804 - CANCELLED, RENAMED FILES & RESTARTED:
Submitted batch job 25497360
# Summarise D-statistic results: first make a population names
hilo/data$ for i in mexicana.allo mexicana.symp maize.symp maize.allo; do echo $i >> AbbaBaba/4pop_maize_bam.popNames; done
# run Rscript with ANGSD
~/hilo/data/AbbaBaba$ Rscript ~/software/angsd/R/estAvgError.R angsdFile="4pop_maize_bam.regions.abbababa" out="4pop_maize_bam.summary" sizeFile=4pop_maize_bam.sizeFile nameFile=4pop_maize_bam.popNames > 4pop_maize_bam.summary.readable
# also created 2 other summary files (important one is 4pop_maize_bam.summary.Observed.txt)
# basically it's not significant but there are low #'s of independent blocks and total Abba/Baba observations
# and I know I'm underpowered setting it up this way, so yeah.

# NEXT: use Aanc as outgroup (Arbitrary A)
Need to make helper files:
# first test (admixture into symp. maize):
# make pop order: allopatric maize, sympatric maize, allopatric teosinte, out
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.allo.list; do cat $i >> AbbaBaba/3pop_alloMaize_sympMaize_alloMex_bam.list; done
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.allo.list; do wc -l $i | awk '{print $1}' >> AbbaBaba/3pop_alloMaize_sympMaize_alloMex_bam.sizeFile; done
scripts$ sbatch --export=PREFIX=3pop_alloMaize_sympMaize_alloMex_bam -p bigmemh doAbbaBaba3pop_Aanc.sh
Submitted batch job 25497465
# second test (admixture into symp. teosinte):
# and make pop order: allopatric teosinte, sympatric teosinte, allopatric maize, out
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list alloMaize4Low_16_bam.list; do cat $i >> AbbaBaba/3pop_alloMex_sympMex_alloMaize_bam.list; done
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list alloMaize4Low_16_bam.list; do wc -l $i | awk '{print $1}' >> AbbaBaba/3pop_alloMex_sympMex_alloMaize_bam.sizeFile; done
scripts$ sbatch --export=PREFIX=3pop_alloMex_sympMex_alloMaize_bam -p bigmemh doAbbaBaba3pop_Aanc.sh
Submitted batch job 25497534
# third test (admixture into symp. maize using all teosinte's as admixing population):
# lower admixture expected due to admixture into sympatric teosintes but also higher power to
# detect admixture because of a much larger panel of individuals/more coverage
# pop order: allopatric maize, sympatric maize, all teosinte, out
# first make a list of all mexicana individuals, combining sympatric and allopatric bams
hilo/data$ for i in pass1_bam_pops/mexicana.allo.list pass1_bam_pops/mexicana.symp.list; do cat $i >> pass1_bam_pops/mexicana.all.list; done
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.all.list; do cat $i >> AbbaBaba/3pop_alloMaize_sympMaize_allMex_bam.list; done
hilo/data$ for i in alloMaize4Low_16_bam.list pass1_bam_pops/maize.symp.list pass1_bam_pops/mexicana.all.list; do wc -l $i | awk '{print $1}' >> AbbaBaba/3pop_alloMaize_sympMaize_allMex_bam.sizeFile; done
scripts$ sbatch --export=PREFIX=3pop_alloMaize_sympMaize_allMex_bam -p bigmemh doAbbaBaba3pop_Aanc.sh
Submitted batch job 25497879
### NOTE: only the 4population and not the 3population script appear to output anything

# next use tripsacum or sorghum as outgroup and possibly extend to a much larger portion of the genome
# can I actually do an f3 or 3 population test?

# New landraces : 4 lowland mexico, 6 lowland South America & 5 Andes
# ind's listed in ~/hilo/data/landraces_fromLi/alloMaizeInclude.list
# (will check andean ind's to confirm no admixture later)
# Getting new landraces aligned properly:
# downloading bams from li
ssh -p 2022 ecalfee@farm.cse.ucdavis.edu # connect to port 2022 for file downloads on farm
cd hilo/data/landraces_fromLi/original
# use icommands to get files (sign in for iplant use, -K checks checksums)
module load icommands
iinit # login w/ password
# run my script to download data from iplant with nohup in background, hopefully this works (?)
ecalfee@c11-42:~/hilo/data/landraces_fromLi/original$ nohup ../../../scripts/getLiDataiplant.sh &
[1] 18482
# did not work :/ . So first I am using irsync to get files from Li's iplant directory
# onto mine (to free up her space). Then I'll delete files I don't need (DONE) and use irsync
# again to get the files onto farm
my_computer$ irsync -r -V i:/iplant/home/lilepisorus/landraces_AGPv4 i:/iplant/home/ecalfee/landraces_fromLi
# now attempting to sync directly to farm
my_computer$ ssh -p 2022 ecalfee@farm.cse.ucdavis.edu
$ module load icommands
$ iinit
$ irsync -r -V i:/iplant/home/ecalfee/landraces_fromLi ~/hilo/data/landraces_fromLi/original
# first ran above with -s option (faster, only checks size), then ran without -s so it uses checksums
# using script to re-align to my version of v4 maize genome (no extra contigs)
scripts$ sbatch remapAlloMaizeV4.sh
Submitted batch job 25847933 - # needed to index reference, but did get all files to .fq output
scripts$ sbatch indexRefBWA.sh
Submitted batch job 26008240
scripts$ sbatch --dependency=afterok:26008240 remapAlloMaizeV4.sh
Submitted batch job 26008241 -- DONE, BUT BAM/SAM FILES EMPTY (no sites). .fq look ok.
# all slurm-log files have something like ""[mem_sam_pe] paired reads have different names: "FCC1WDLACXX:4:2116:2328:81442#", "FCC1WDLACXX:4:2108:10115:86292#""
# The issue is that I need to sort by name so read pairs are together, not by mapping coordinate,
# before creating the .fq file. Fixed script to do that remapAlloMaizeV4.sh and rerunning 10.25.18:
scripts$ sbatch remapAlloMaizeV4.sh
Submitted batch job 6105434 --> fixed some naming issues
Submitted batch job 6106605 -- made fq files but couldn't find reference & index
# re-indexing reference for bwa-mem (was just a folder issue finding index but I worry that the original index
# could have not completed properly since the algorithm to index shoudl only be for data <2G
scripts$ sbatch indexRefBWA.sh
Submitted batch job 6132410 - COMPLETED
# Then running remap again (will skip over making fq files if already done)
scripts$ sbatch --dependency=afterok:6106605,6132410 remapAlloMaizeV4.sh
Submitted batch job 6132440 - WAITING 4 DEPENDENCY
## Nov 1 2018: One bam file appears to be corrupted. I think it would be easier to just download the raw sequence data and map from that.
# So I made got the study SRA accession numbers data/landraces_fromLi/SraRunTable.txt and I made a new script to download from NCBI
# scripts/download_ncbi_allo_landraces.sh
scripts$ sbatch --array=1 -t 24:00 --mem=24G download_ncbi_allo_landraces.sh
Submitted batch job 6689982 # running -- just testing 1 file first to see how long it takes, that it works & memory used
# I may want to run 'prefetch' first instead if it's very slow and/or use
# --split-files, --skip-technical, --clip and/or --gzip within the fastq-dump command
# -- see https://github.com/rvalieris/parallel-fastq-dump
scripts$ sbatch --array=2 -t 24:00 --mem=24G download_ncbi_allo_landraces.sh
Submitted batch job 6690926 # running with new options (clip, split, omit technical etc.) -- TIMEDOUT
scripts$ sbatch --array=2 -t 24:00:00 --mem=24G download_ncbi_allo_landraces.sh
Submitted batch job 6811429 - RUNNING now with 24 hrs not 24 min time limit
# If this gets bogged down, I can alternatively use priority 4 cores, each 16G (so 8cpu on bigmemh for 1 individual)
# scripts$ sbatch --array=2 -t 96:00:00 -p bigmemh download_ncbi_allo_landraces.sh -- NOT run
### Li's scripts may be useful for starting from raw reads: (trimming, aligning, and marking duplicates .. I'm not sure I should do indel re-alignment)
# https://github.com/HuffordLab/Wang_et_al._Demography/tree/master/trim_mapping_MD
scripts$ sbatch --array=3 -t 120:00:00 -p bigmemm download_ncbi_allo_landraces.sh - Trying another example with extra time.
Submitted batch job 6811947 -- CANCELLED
scripts$ sbatch --mem=32G --array=3 -t 120:00:00 -p bigmemm download_ncbi_allo_landraces.sh -- added memory
Submitted batch job 6811949

##### how to make better slurm dependencies scripts/pipelines:
https://hpc.nih.gov/docs/job_dependencies.html

# added Xochimilco to a new list of allopatric mexicana
# (because appears unadmixed and we need more until allopatric mex get sequenced more):
~/hilo/data/pass1_bam_pops$ cat mexicana.allo.list pop35.list >> mexicana.allo.withXochi35.list
# I get a list of sites first with major/minor alleles using mafsToSitesFile.sh script:
scripts$ sbatch mafsToSitesFile.sh
Submitted batch job 25543209 - FIXED TYPO
Submitted batch job 25543731 - ANOTHER TYPO
Submitted batch job 25544229 - FINISHED, BUT NEW FILE NAMING & ADDED INDEXING:
Submitted batch job 255534

# the script alleleFreqPipeline.sh basically just stores all the population names so it can run
# calcAlleleFreqPop.sh on each one to calculate allele frequencies for every variable site for each pop
scripts$ sbatch alleleFreqPipeline.sh
Submitted batch job 25544710 - ALL errors; fixed typos, indexed sites file etc.
Submitted batch job 25554149 - RUNNING - COMPLETED
# 8.21.18 now to check which subprocesses didn't finish:
hilo$ for i in $(grep 'Submitted' slurm-log/*25554149*.out | awk '{print $4}'); do grep 'CANCELLED' slurm-log/*$i*.out; done
# these are: (identified by hilo$ grep 25554201 -B 2 slurm-log/*25554149*.out)
maize.allo.4Low16 (#1) - 0, 39, 252, 256
~/hilo/scripts$ sbatch --array=1,39,252,256 --mem=16G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 25772687 -- COMPLETED, OOPS redid region 1 instead of zero:
scripts$ sbatch --array=0 --mem=16G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 25773575
pop29 (#13) - 85
~/hilo/scripts$ sbatch --array=85 --mem=16G -t 24:00:00 --export=POP=pop29,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 25772727 - COMPLETED
# the following freq files also don't exist and need to be re-done:
sbatch --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,\
DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26026327 - cancelled reading old sites file
Submitted batch job 26026424 - didn't quite fix. Now fixed to have correct sites directory (NOTE -- THESE NEW FILES DON'T HAVE DUPLICATED SITES):
scripts$ sbatch --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,\
DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 26026522 - CANCELLED - still a problem with sites file looking the same age or older than the index.
# added sleep 2s to script that removes duplicates (can mimic in other scripts) and removing and remaking sites files for problem files
data/geno_lik/merged_pass1_all_alloMaize4Low_16/allVar$ for i in {111..114} 159 257 {289..291} {326..327} {363..365} 395; do echo $i; rm region_$i.var.sites.*; angsd sites index region_$i.var.sites; echo "done indexing "$i; done
data/var_sites/merged_pass1_all_alloMaize4Low_16$ same thing as above -- COMPLETED
# running the above script for calcAlleleFreqPop again:
Submitted batch job 26027501
# and running the same allele freq script above but with POP=mexicana.allo.withXochi35
scripts$ sbatch --dependency=afterok:26027501 --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=mexicana.allo.withXochi35,DIR_POPS=pass1_bam_pops,\
DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 26027624
# re-doing from old variant sites (with doubles, just to avoid inconsistency) -- output in geno_lik dir
scripts$ sbatch --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=mexicana.allo.withXochi35,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26029518 - ERROR TYPO in sites dir
Submitted batch job 26029702
scripts$ sbatch --dependency=afterok:26029702 --array=111-114,159,257,289-291,326-327,363-365,395%7 -p bigmemh --mem=8G -t 24:00:00 \
--export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26029559 - ALSO HAD TYPO in sites dir
Submitted batch job 26029746 - COMPLETE, but looks like region_0 file is truncated; fix below
scripts$ sbatch --array=0 -p bigmemh --mem=8G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,\
DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26030193 - EXCEEDED Memory, trying again with many cores on partition med
scripts$ sbatch --array=0 -p med --mem=24G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_POPS=pass1_bam_pops,DIR_REGIONS=refMaize/divide_5Mb,DIR_SITES=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar calcAlleleFreqPop.sh
Submitted batch job 26031000 -COMPLETED

# now that I have allele frequencies, I can do an F4 test. The only issue is that
# I need to match alleles by position because ANGSD will omit a frequency if no individuals have
# any data. I *may* also want to make more stringent criteria for # ind's with data, e.g. in maize & allo mex, to include a site, but don't need to for a first pass analysis. I think this should only increase variance in f4, but not bias it to include all sites while a strict site filter for depth may actually increase bias due to mapping bias.

# After making allele freq files I found that my position/sites files had every SNP duplicated because I had specified -rf and -f in calling variant sites and calculating MAF scripts
# this affects all files eg. region_9.mafs.gz region_9.var.sites for regions covering the whole chromosome, including within population frequencies.
# BUT the NGSadmix & PCA results should be largely unchanged because there are no duplicated sites in my
# genome-wide variant sites filtering by every 1000th SNP (they just aren't 100% 10kb apart due to the duplicates). I will need to re-call all variant sites and will want to use new thinned
# SNPs anyways, filtered by cM position rather than bp's. Temporarily I will remove duplicates in a post-processing R-script (which won't be necessary once upstream is corrected & files are re-run).
# this doesn't affect FST and SFS stats that used a small set of regions (not duplicated with -r -rf). After re-running I should confirm that the issue is the -r -rf and the duplication problem goes away.
# I can temporarily fix the doubled sites problem by using split
split -l 73165 region_287.var.sites (then be sure to re-index the new sites file _A and the # lines = half of wc -l region_287.var.sites)
# implemented in removeDuplSites.sh making new sites files in var_sites/merged_pass1_all_alloMaize4Low_16/allVar/region_*.var.sites
scripts$ sbatch removeDuplSites.sh
Submitted batch job 25968083 -- I cancelled & reran after removing # jobs at a time limit:
Submitted batch job 25968187 -- mostly finished except some TIMEOUT run on node 7 and 1 (will rerun & exclude these nodes)
scripts$ sbatch -x bigmem1,bigmem7 --array=32-53,84-95,136-199 -t 8:00:00 removeDuplSites.sh
Submitted batch job 25971992 - COMPLETED -- but turns out a few others didn't say CANCELLED but were cancelled for timeout, rerunning below
scripts$ sbatch -p med --array=54-60,62-83 -t 8:00:00 removeDuplSites.sh
Submitted batch job 26030055 -COMPLETED

##### currently working on doGenoAngsd2Plink.sh (may be done once I have new sites files) and doHaploAngsd.sh
# doGeno worked but doHaploCall did not
# new plan than doHaploAngsd is to get read counts for Maj/Min allele for all individuals
# and then I randomly sample 1 allele from each individual with data. Maybe I filter sites first...
# in the end I need sites filtered by XcM
# with counts for each admixed individual

# first I filter sites for MAF difference maize-mex and min # individuals with reads/coverage in both allopatric pass1_bam_pops
# for now I am including pop35 as allopatric mex to have more individuals (Xochimilco)
scripts$ sbatch filterAlloMAFnInd.sh
Submitted batch job 26022854 - TYPO DIDN'T run
Submitted batch job 26023401 - ERRORS maybe in working directory. trying again with just 1 tester region_
scripts$ sbatch --array=9 filterAlloMAFnInd.sh
Submitted batch job 26024275 - fixed typo and reran
Submitted batch job 26024388
scripts$ sbatch -p bigmemh --array=0-425%7 filterAlloMAFnInd.sh
Submitted batch job 26024492 -- all worked except those with missing freq files for allopatric maize. Fixing that (above see job 26026522) & re-running
scripts$ sbatch -p low --array=111-114,159,257,289-291,326-327,363-365,395 --dependency=afterok:26026522 filterAlloMAFnInd.sh
Submitted batch job 26026785 - CANCELLED, fixed indexing issue and re-trying
scripts$ sbatch -p low --array=111-114,159,257,289-291,326-327,363-365,395 --dependency=afterok:26027624 filterAlloMAFnInd.sh
Submitted batch job 26027652 - STILL FILES MISSING. - reran those files then re-running this script below:
scripts$ sbatch -p low --array=111-114,159,257,289-291,326-327,363-365,395 --dependency=afterok:26029746 filterAlloMAFnInd.sh
Submitted batch job 26029827
scripts$ sbatch -p low --array=0,54-60,62-83 --dependency=afterok:26030055,26030193 filterAlloMAFnInd.sh
Submitted batch job 26030212 - COMPLETED except 0 (why did this run when 26030193 was cancelled for memory?)
scripts$ sbatch -p low --array=0 --dependency=afterok:26031000 filterAlloMAFnInd.sh
Submitted batch job 26031038 - COMPLETED, but DIDN't WAIT -- not sure why, will rerun
scripts$ sbatch -p low --array=0 filterAlloMAFnInd.sh
Submitted batch job 26031985 - COMPLETED

# then I use a second script to thin SNPs to minimize within-population LD
# I choose a fixed cM width of .0001 (~.1Kb) rather than trying to calculate LD within low coverage pops
# a reasonable alternative to consider later is to calculate LD within my maize reference sample only
# and use that to filter (or at least to choose a cM cutoff..but I'm not sure that's necessary)
scripts$ sbatch -p low --array=2 -t 30:00 pruneFixedcM.sh # testing - working
scripts$ sbatch -p bigmemh --dependency=afterok:26027652 --array=1-10%7 pruneFixedcM.sh
Submitted batch job 26028060 - FIRST SCRIPT FOR FILTERING WASN'T ACTUALLY DONE, so re-running
scripts$ sbatch -p bigmemh --dependency=afterok:26030212 --array=1-10%7 pruneFixedcM.sh
Submitted batch job 26030285
scripts$ sbatch -p bigmemh --dependency=afterok:26031038 --array=1 pruneFixedcM.sh
Submitted batch job 26031064 - DIDN'T WORK
Submitted batch job 26032015 (ran again now without dependency because I reran that to completion)

# before running ASEReadCounter I need to add ReadGroups so GATK will process my bams (takes a few mins) and re-index:
scripts$ sbatch addReadGroup2Bam.sh
Submitted batch job 26007563 -- many did not run (retry all:)
Submitted batch job 26007874 -- COMPLETED

# update: I can get a VCF output from angsd for allopatric maize for any positions I desire:
scripts$ sbatch -p bigmemh --dependency=afterok:26032015 --array=1-10%4 getVCFforAllo.sh
Submitted batch job 26035120 # by 4 because it's 16G (or 2 cpu's per task) - exited out due to lack of angsd index, rerunning
scripts$ sbatch -p bigmemh --array=1-10%4 getVCFforAllo.sh
Submitted batch job 26118782 - typo fixed & rerunning:
Submitted batch job 26119622 -- BIG PROBLEM WITH PYTHON OUTPUTTING WEIRD END OF LINE CHARACTERS..
e.g. thinnedHMM/old_CRLF_line_terminators$ file chr4.var.sites
chr4.var.sites: ASCII text, with CRLF line terminators # a good file would only return : ASCII text
PROBLEM: MY PYTHON SCRIPT MUST BE ADDING AN \r NEWLINE CHARACTER AT THE END OF LINES IN VAR.SITES FILES
WHICH THEN CAN'T BE READ BY ANGSD. I DO NOT (YET) KNOW HOW TO FIX THE PYTHON SCRIPT, BUT I DID A WORK-AROUND
WHERE I PUT ALL AFFECTED .VAR.SITES AND .DISTM FILES IN A SUBFOLDER CALLED 'OLD' AND THEN RAN:
for i in $(ls *); do tr -d '\r' < $i > ../$i; done # which fixed them
scripts$ sbatch -p bigmemh --array=2-9%4 getVCFforAllo.sh
Submitted batch job 26184652 - I cancelled right away
Submitted batch job 26185000 - COMPLETED 26185000_2 but cancelled others to run on med instead:
scripts$ sbatch -p med -t 5:00:00 --array=3-9 getVCFforAllo.sh
Submitted batch job 26185607 - COMPLETED
scripts$ sbatch -t 8:00:00 --array=1,10 --mem=32G --export=n_threads=4 -p med getVCFforAllo.sh
Submitted batch job 26185365 - COMPLETED

## problem with ANGSD vcf being used by GATK but plink is able to parse the vcf and get allele counts
scripts$ sbatch -p bigmemh --dependency=afterok:26035120 --array=1-10%8 alloVCF2AlleleCounts.sh
Submitted batch job 26035124 - resubmitting
scripts$ sbatch -p bigmemh --dependency=afterok:26119622 --array=1-10%8 alloVCF2AlleleCounts.sh
Submitted batch job 26119734 - cancelled due to upstream error
scripts$ sbatch --dependency=afterok:26185365,26185607,26185000_2 alloVCF2AlleleCounts.sh
Submitted batch job 26185895 - COMPLETED

# I can get allele counts for major and minor allele using GATK ASEReadCounter
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:26035124_$i countReadsRefAlt.sh; done
Submitted batch job 26035127-36 - resubmitting
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:26119734_$i countReadsRefAlt.sh; done
Submitted batch job 26119815-25 # skips 21 - I cancelled due to upstream error
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:26185895_$i countReadsRefAlt.sh; done
Submitted batch job 26186102-11 - didn't run correctly; chr<blank>.vcf not found; fixed typo (doi!)
Submitted batch job 26222924-33 - oops, fixed typo
Submitted batch job 26225969-78 - COMPLETED

# then in R I can randomly sample 1 read from each allopatric mexicana individual to make an 'allopatric allele counts' file
make_allo_counts_ancestry_hmm.R
# and put individuals and chromosome positions into population input files for reading by ancestry_hmm
make_input_ancestry_hmm.R
# bash scripts to run these R scripts:
scripts$ sbatch makeAlloCountsAncHMMInput.sh
Submitted batch job 26288487 - fixed typo & rerunning
Submitted batch job 26288501 - COMPLETED - 34 R warnings & All NAs for allopatric maize

scripts$ sbatch --dependency=afterok:26288501 makeAncHMMInput.sh
Submitted batch job 26288457 --oops needed dependency!
Submitted batch job 26288513 - gargabe because of upstream errors

# note: currently rounds to map position to 10 digits (but scientific notation coulde be E-6) -- had to remove a few SNPs with no called genotypes in allopatric maize despite filtering above for 4+ individuals wiht data (may need to rethink that filtering to get rid of sites with very low maize coverage, possibly poor quality SNPs (?) and sites with very high coverage (see read counts near the beginning of chromosome 1 -- I'm not sure what's going on!)
# problem: how can there be zero counts when a minimum of 4 allopatric individuals have data?
4    133355 A T  2 12  0  0 1.451326149e-05
4    133388 C T  2 12  0  0 1.197344073e-06 (???)
# I'm not sure what's going on since 5 individuals from angsd output with all the same mapping and baseQ filters
# shows 5 individuals with data
zgrep '133388' geno_lik/merged_pass1_all_alloMaize4Low_16/allVar/mexicana.allo.withXochi35/region_159.mafs.gz
4	133388	C	T	T	0.122774	5
# I can see the genotypes for these 5 individuals (each one read mapped) using mpileup:
# 1 C J means 1 read, allele = C, and J = ASCII code for 74 and I subtract 33 to get mapping score of 41
# (if I filter -Q 42 the sites go away). and the genotypes all appear to be Cc or Gg (fwd/rev strand)
# for allopatric mexicana and if I run the same on pass1_bam_pops/maize.allo.4Low16.list
# mpileup with my quality filters implemented ( I see all 5 individuals with data ):
data$ samtools mpileup -r 4:133388-133388 -q 20 -Q 30 --ff UNMAP --ff SECONDARY --ff QCFAIL --ff DUP -b pass1_bam_pops/mexicana.allo.withXochi35.list
somehow at this site ref allele = alt allele = T (so no C's at all) in the output from ASEReadCounter
~/hilo/data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM$ awk '$2==133388 && $1==4 {print $0}' hilo_*_chr4.csv | head
4	133388	.	T	T	0	0	0	0	0	3	3	0
4	133388	.	T	T	0	0	0	0	0	1	1	0
...

# I need to check if the error is from the VCF from ANGSD, VCF from PLINK or produced by ASEReadCounter
# maybe, e.g. ASEReadCounter doesn't need a reference genome or is overwriting the 'ref' allele identity
# probably the problem is in ASEReadCounter -- I'll remove the reference genome from there
# because C is the ref allele and T is the alt allele in .vcf.gz from angsd and .vcf from plink (going into GATK)
thinnedHMM$ zcat maize.allo.*chr4.vcf.gz | awk '$2==133388 && $1==4 {print $0}'
thinnedHMM$ awk '$2==133388 && $1==4 {print $0}' maize.allo.*chr4.vcf

# SEPARATE ISSUE:: Puerta Encantada individuals are missing from the bam list for allopatric mexicana
cat pass1_bam_pops/mexicana.allo.withXochi35.list # to view -- but this if anything would cause the opp. problem where allele freq files based on this bam had fewer read counts than there should be
# for right now I will just filter out these sites with no counts in either allo maize or allo mex,
# but I need to check upstream what is causing the problem

scripts$ sbatch makeAlloCountsAncHMMInput.sh
Submitted batch job 26289298 - COMPLETED
scripts$ sbatch --dependency=afterok:26289298 makeAncHMMInput.sh
Submitted batch job 26289309 - ERRORS
Submitted batch job 26289377 - COMPLETED (need to fix to have proper sample file w/ ploidy)
# making manually sample file w/ ploidy now:
~/hilo/data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/input$ for i in $(ls pop*.anc_hmm.ids); do awk '{print "HILO"$1"\t"2}' $i > $i.ploidy; done
# right now I'll choose one example pop 366 with 9 individuals and about .14 mexicana ancestry
# to run
# but I will summarise avg. population admixture proportions in a file output from plot_global_ancestry_NGSadmix.R
# "../data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/input/globalAdmixtureByPopN.txt"
# and use those directly from runAncHMM.sh later
# running test population runAncHMM.sh
scripts$ sbatch --array=366 runAncHMM.sh
Submitted batch job 26291049 # fixed typo
~/hilo/scripts$ sbatch --array=366 -p bigmemh runAncHMM.sh
Submitted batch job 26291088 - COMPLETED, results in input/ folder. added longer time & bootstrapping
Submitted batch job 26291281 - RUNNING (but typo in --timin so I added a new one with --tmin)
~/hilo/scripts$ sbatch -p med runAncHMM.sh
Submitted batch job 26291536 (--tmin typo fixed)
~/hilo/scripts$ sbatch -p bigmemh runAncHMM_noboot.sh
Submitted batch job 26291537 # and re-running up to 10K generations without bootstrapping


# updates to pipeline:
# increased SNP spacing to .001 cM (~1kb avg) in pruneFixedcM.sh
# deleted reference genome from ASEReadCounter to hopefully avoid problem of zero alleles because it's ref ref
# remake list of allopatric mexicana
# note temporarily saved old files in 'backup folders': thinnedHMM/ancestry_hmm/input[or output]_backup_highLD and thinnedHMM/backup_temp/
~/hilo/data$ awk '$5=="mexicana" && $6=="allopatric" {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/mexicana.allo.list
~/hilo/data$ awk '$5=="mexicana" && ($6=="allopatric" || $9=="Xochimilco") {print "filtered_bam/hilo_"$1".sort.dedup.baq.bam"}' pass1_ids.txt > pass1_bam_pops/mexicana.allo.withXochi35.list
# redo allele freq. calcs for allopatric mexicana + Xochi and filtering for informative sites with min n=6
scripts$ sbatch --array=0-425 -p med --mem=16G -t 24:00:00 --export=POP=mexicana.allo.withXochi35,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 26294842 - I cancelled
Submitted batch job 26295398 - failed. try again
Submitted batch job 26296462 -- ALL GOOD EXCEPT 2,3,61,160 have .idx index files that look newer than sites files -- will fix
# fixing newer index problem:
hilo/data$ for i in 2 3 160 61; do rm var_sites/merged_pass1_all_alloMaize4Low_16/region_$i.var.sites.idx; angsd sites index var_sites/merged_pass1_all_alloMaize4Low_16/region_$i.var.sites; done
scripts$ sbatch --array=2,3,61,160 -p med --mem=16G -t 24:00:00 --export=POP=mexicana.allo.withXochi35,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 27288178 - COMPLETED
# also re-calculating MAF for allopatric maize (creating maf files in var_sites subfolder)
scripts$ sbatch --array=0-425 -p med --mem=16G -t 24:00:00 --export=POP=maize.allo.4Low16,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 27314347 - COMPLETED ALL BUT 0 (out of memory) - RERUNNING AT HIGH MEM:
hilo/scripts$ sbatch --array=0 -p bigmemh --mem=32G -t 4:00:00 --export=POP=maize.allo.4Low16,DIR_SITES=var_sites/merged_pass1_all_alloMaize4Low_16 calcAlleleFreqPop.sh
Submitted batch job 27317118 - COMPLETED
# filter and thin sites
scripts$ sbatch --dependency=afterok:26296462 filterAlloMAFnInd.sh
Submitted batch job 26296656 - NO (reported) ISSUES (EVEN FROM ONES FAULTED FOR .idx FILE ABOVE)
scripts$ sbatch --array=2,3,61,160 -p med filterAlloMAFnInd.sh - FILTERING REMAINING ONES FIXED ABOVE
Submitted batch job 27288191 - COMPLETED, BUT ISSUE IS THAT IT USED OLD FOLDER geno_lik for MAF - re-doing all:
scripts$ sbatch --array=0-425 --dependency=afterok:27317118 -p med filterAlloMAFnInd.sh
Submitted batch job 27318790 - COMPLETED (super fast)
scripts$ sbatch --dependency=afterok:26296656 pruneFixedcM.sh
Submitted batch job 26296662 - RAN BUT NEED TO RERUN DUE TO ERRORS ABOVE (DELETED OUTPUT FILES)
scripts$ sbatch --array=1,4 pruneFixedcM.sh # re-doing affected chromosomes from 4 files above
Submitted batch job 27288200 - CANCELLED, fixed typo with code to remove extra end of line characters
# re-doing all because of error above
hilo/scripts$ sbatch --array=1-10 -p med --dependency=afterok:27318790 pruneFixedcM.sh
Submitted batch job 27319249 - COMPLETED

# get allele and read counts
scripts$ sbatch -t 8:00:00 --dependency=afterok:26296662 --array=1-10 --mem=32G --export=n_threads=4 -p med getVCFforAllo.sh
Submitted batch job 26296682 - ONLY chr9 RAN DUE TO .idx files being too similar in timestamp to sites files - ACTUALLY ALL ARE MESSED UP (no var)
# fixing issue: hilo/data$ for i in {1..8} 10; do rm var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/chr$i.var.sites.bin; rm var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/chr$i.var.sites.idx; done
scripts$ sbatch -t 8:00:00 --dependency=afterok:27288200 --array=1,4 --mem=32G --export=n_threads=4 -p med getVCFforAllo.sh
Submitted batch job 27298295 - DELETED OUTPUT (no var)
scripts$ sbatch -t 8:00:00 --array=2,3,5-8,10 --mem=32G --export=n_threads=4 -p med getVCFforAllo.sh
Submitted batch job 27298285 - DELETED OUTPUT (no var) & running anew:
scripts$ sbatch -t 8:00:00 --dependency=afterok:27319249 --array=1-10 --mem=32G --export=n_threads=4 -p med getVCFforAllo.sh
Submitted batch job 27319266 - COMPLETED

scripts$ sbatch --dependency=afterok:26296682 alloVCF2AlleleCounts.sh
Submitted batch job 26296705 (DELETED OUTPUT no var). running anew:
scripts$ sbatch --dependency=afterok:27319266 alloVCF2AlleleCounts.sh
Submitted batch job 27319352 - DONE (very fast!)

scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:26296705_$i countReadsRefAlt.sh; done
Submitted batch job 26296733-42 - DELETED OUTPUT (no var). running anew:
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i --dependency=afterok:27319352_$i countReadsRefAlt.sh; done
Submitted batch job 27321466-27321475 - ERROR needs reference to generate counts
# I need to check if it will count major/minor alleles instead of ref/alt of I'll have to use a different program to count reads
# e.g. using samtools mpileup I can get all reads matching quality thresholds for sites in the sites file
# then use the major/minor allele from the var.sites file to get counts, e.g. in python
# be careful to check what mpileup does with loci where there are no reads (zero depth) for an individual

# make input files
scripts$ sbatch --dependency=afterok:26296733,26296734,26296735,26296736,26296737,26296738,26296739,26296740,26296741,26296742 makeAlloCountsAncHMMInput.sh
Submitted batch job 26296784 - NO OUTPUT (no var)
scripts$ sbatch --dependency=afterok:$(echo {27321466..27321475} | tr ' ' ',') makeAlloCountsAncHMMInput.sh
Submitted batch job 27321508 - ERROR NO OUTPUT due to no input files

scripts$ sbatch --dependency=afterok:26296784 makeAncHMMInput.sh
Submitted batch job 26296785 - NO OUTPUT (no var)
scripts$ sbatch --dependency=afterok:27321508 makeAncHMMInput.sh
Submitted batch job 27321523 - ERROR on input files

# Modify script runAncHMM.sh to run on all populations, bootstrap and nonboostrap versions
# uses global ancestry per population pulled from ancestry_hmm/input/globalAdmixtureByPopN.txt
# which is created in interactive R script plot_global_ancestry_NGSadmix.R
# the script goes through all sympatric populations (array indeces 0-27) but skips pops with no admixture
scripts$ sbatch --array=19 -p bigmemh --dependency=afterok:27321523 runAncHMM_noboot.sh
Submitted batch job 27325720 - ERROR on input files

# test bootstrapping too - note: these each use 3 cores (24G) and my max of bigmemh is 8 cores at a time
scripts$ sbatch --array=19 -p bigmemh --dependency=afterok:27321523 runAncHMM.sh
Submitted batch job 27325727 - ERROR on input files

# run for all sympatric populations. First check pop366 and files for SNPs look ok! (no ref ref alleles..)
scripts$ sbatch --array=19 -p bigmemm runAncHMM_noboot.sh --NEVER RAN

# instead of GATK or samtools mpileup I'm using angsd to get counts of all ACGT reads that pass filtering
# for the thinned sites -- countReadsACGT.sh
# then I'm using an R script to pull out major and minor allele counts from all ACGT read counts
# countReadsMajorMinor.sh calls count_reads_major_minor.R
# I need to modify downstream scripts to use this new input: makeAlloCountsAncHMMInput.sh
scripts$ for i in {1..10}; do sbatch -p med --export=CHR=$i countReadsACGT.sh; done
Submitted batch job 27723541-50 # re-doing just ones cancelled due to time limit:
scripts$ sbatch -p med --export=CHR=1 --array=8,16-29,50-54,65-69,89-105,107,133,188 -t 4:00:00 countReadsACGT.sh
Submitted batch job 27929508 - COMPLETED
scripts$ sbatch -p med --export=CHR=2 --array=40,41,44-47,168-171,189 -t 4:00:00 countReadsACGT.sh
Submitted batch job 27929560 - COMPLETED (but found additional NODE_FAIL case below later)
scripts$ sbatch -p med --export=CHR=2 --array=28 -t 4:00:00 countReadsACGT.sh
Submitted batch job 27946888 - COMPLETED
scripts$ sbatch -p med --export=CHR=3 --array=85-86,117,122-126,142-146,162 -t 4:00:00 countReadsACGT.sh
Submitted batch job 27929586 - COMPLETED (BUT ISSUE STILL WITH 148 so rerunning:)
scripts$ sbatch -p med --export=CHR=3 --array=148 -t 4:00:00 countReadsACGT.sh
Submitted batch job 27946896 - COMPLETED
# note: hilo 41-43 have zero length files; hilo 80 has no data
I had already redone bam filtering for hilo 41-43:
slurm-log/filterBam_*_25848426_41.out (or _42, _43)
# but I got errors due to a directory typo when I tried to do the baq calculation
see slurm-log/addBAQBam_*_25848608_41.out # re-doing below:
scripts$ sbatch --array=41-43 -p bigmemh addBAQBam.sh
Submitted batch job 27931987 - COMPLETED (manually ran samtools index afterwards due to typo)
scripts$ for i in {1..10}; do sbatch -p med --dependency=afterok:27931987 --export=CHR=$i --array=41-43 countReadsACGT.sh; done
Submitted batch job 27932007-27932016 - index too old, so re-running:
Submitted batch job 27936230-39 - COMPLETED
# getting major/minor read counts for all hilo individuals except 41-43 and 80:
scripts$ sbatch --array=1-40,44-79,81-200 countReadsMajorMinor.sh
Submitted batch job 27932610 - fixing typo:
Submitted batch job 27932923 - COMPLETED (but had 2 I need to fix:)
scripts$ sbatch --array=28,148 countReadsMajorMinor.sh
Submitted batch job 27946940 - COMPLETED
# and I will do the same for 41-43 when they're finished
scripts$ sbatch --array=41-43 --dependency=afterok:$(echo {27932007..27932016} | tr " " ",") countReadsMajorMinor.sh
Submitted batch job 27932914. rerunning with new files (After fixing index):
Submitted batch job 27936307 - COMPLETED
# need to add hilo 41-43 into metadata
data/filtered_bam$ for i in {1..200}; do samtools flagstat hilo_$i.sort.dedup.baq.bam | awk -v i="$i" '{print "hilo"i" "$0}' | grep 'total' >> pass1.all.metrics.Nreads; done
# (!) this needs to be re-run .. data can come from the dedup file maybe and it takes a long time to run and appends to end (oops!)
# making new input files for ancestry hmm (without 41-43 for now):
scripts$ sbatch makeAlloCountsAncHMMInput.sh
Submitted batch job 27936615 -ERROR: rerunning many times with typos, then fixed:
Submitted batch job 27937464 - COMPLETED
scripts$ sbatch makeAncHMMInput.sh
Submitted batch job 27937571 - many errors fixed, final run:
Submitted batch job 27937757 - COMPLETED BUT NEED TO FIX 2 BELOW MISSING INPUT FILES:
# populations 21 and 27 had truncated anc_hmm input files:
scripts$ sbatch --array=21,27 makeAncHMMInput.sh
Submitted batch job 27946967 - COMPLETED -- NEEDED TO ADD PLOIDY FILE TO SCRIPT:
scripts$ sbatch makeAncHMMInput.sh
Submitted batch job 27947901 - COMPLETED all but # 19 (timeout)
scripts$ sbatch -t 1:00:00 --array=19 -p bigmemh makeAncHMMInput.sh
Submitted batch job 27949487
#(also we'll see if anc_hmm runs with some sites having no maize ref. alleles (very few sites))
scripts$ sbatch --array=19 -p bigmemh runAncHMM_noboot.sh
Submitted batch job 27937887 - errors ancestry proportions not summing to one (!) - fixed:
Submitted batch job 27938233 - CANCELLED (TIMEOUT). fixed typo in directories & re-running:
Submitted batch job 27947541 - CANCELLED (forgot to make ploidy file, rerunning:)
scripts$ sbatch --array=19 -p bigmemh runAncHMM_noboot.sh
Submitted batch job 27949496 - COMPLETED
scripts$ sbatch --array=19 -p bigmemh runAncHMM.sh  # also running version with bootstrap
Submitted batch job 27938248 - I CANCELLED (not reaching calculations..). fixed typo & rerunning:
Submitted batch job 27947544 - CANCELLED (forgot to make ploidy file, rerunning:)
scripts$ sbatch --array=19 -p bigmemh runAncHMM.sh
Submitted batch job 27949532 - RUNNING
# and running other populations too:
scripts$ sbatch --array=0-18,20-27 -p bigmemm runAncHMM_noboot.sh
Submitted batch job 27938289 - CANCELLED (TIMED-OUT). fixed typo & truncated input files & rerunning:
scripts$ sbatch --dependency=afterok:27949496 --array=0-18,20-27%1 -p bigmemh runAncHMM_noboot.sh
Submitted batch job 27949611 - COMPLETED - 200-1200ish generations since admixture estimated (could be diff Ne per pop too)
# running all bootstraps too:
scripts$ sbatch --array=0-18,20-27%6 -p bigmemh --mem=8G -t 6:00:00 runAncHMM.sh
Submitted batch job 27950729
## to get in a usable form the bootstrap time of admixture results (from slurm tasks 27950729 & 27949532),
## I run the following code:
hilo$ for i in $(ls slurm-log/*27950729*; ls slurm-log/*27949532*); do popN=$(grep 'running pop' ${i} | awk '{print $2}'); alpha_mex=$(grep 'running pop' ${i} | awk '{print $6}'); echo ${popN}; echo alpha_mex:${alpha_mex}; grep 'running pop' ${i} | awk '{print $2"\t"$3 $4"\t"$5 $6}'> data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/output_bootT/bootstrap_${popN}.txt; awk -v alpha_mex=$alpha_mex '$3 == alpha_mex {print $0}' ${i} | awk 'NR % 2 == 0' >> data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/output_bootT/bootstrap_${popN}.txt; done
## experiment with make Ne larger or smaller.
scripts$ sbatch --export=Ne=50000,SUBDIR_OUT="output_bootT_Ne50k" runAncHMM.sh
Submitted batch job 27953854 - COMPLETED
scripts$ sbatch --array=0-27%4 -p bigmemh --export=Ne=5000,SUBDIR_OUT="output_bootT_Ne5k" runAncHMM.sh
Submitted batch job 27953869 - COMPLETED
# made bootstrap file manually (see above) but fixed in .sh script to make automatically next time
# Ne 5k, 10k, or 50k appears to make little difference to infered timing -- I can check effect on tracts later
# Next I'll make a plot of time estimates & proportion maize/mex for the different populations (w/ size of point = # ind. or total avg coverage)
# to test source of bias upwards of teosinte ancestry compared to original alpha, I could re-do NGSadmix with my chosen SNP set (or a spaced subset) & see what it says
# it could be I'm choosing sites that are more teosinte (e.g. over/under-sampling by recombination rate)
# or that I've picked snp's where it's easier to identify teosinte than avg. genomewide
### and I haven't included hilo 41-43 yet. And I need to switch to the new lowland SA maize for allopatric maize

# analysing first batch of data: made script to take posterior
# calls for all sites and all 3 genotypes
# and summarise it by population mexicana ancestry frequencies
# calc_genomewide_pop_anc_freq.R takes in HILOXX.posterior files from ancestry_hmm
# and returns and anc/ folder with individual alphas, ancestry frequencies,
# and pop ancestry frequencies
# Zanc_statistic.R is still early draft to run calculations from these files
# also made R script to plot time of admixture for different bootstrap values of Ne
# -- Ne choice makes little difference plot_time_of_admixture_estimate.R

# Barbara scripts$ nohup parallel --noswap --joblog ../data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/output_noBoot/calc_pop_freqs.log --jobs 6 'Rscript calc_genomewide_pop_anc_freq.R {1} ../data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/output_noBoot' ::: 18 19 21 23 24 26 {28..30} 34 360 362 363 {365..368} 370 {372..374} &> ../data/var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/output_noBoot/calc_pop_freqs.out &
[1] 2645 - fixed typo & re-ran
[2] 2928


## FILTERING BASED ON DEPTH - For quality control, I want to filter out sites that have too low or too high of
# coverage across the full sample of individuals. I am doing this by setting a minimum and maximum total depth # when I call variant sites for further analysis for each region across the genome using callVarAngsd_Regions.sh
# Later, I also need to make sure I use these same filtering steps for any non-variant based analyses, including
# NGSadmix, PCA in angsd and Fst (with SAF, SFS steps)
# Additionally, I want to filter out individuals with extremely high coverage from the counts provided to ancestry_hmm
# Very high coverage can indicate poor mapping quality (multiple sites mapping to one region) or TEs in an individual (where e.g. other TE instances all map to the one TE location present in the reference). Very low depth could be places where allopatric mex or maize or the admixed individuals do not map well to the reference genome, leading to dropped reads.
# Because this is low coverage data, I will not set a minimum individual coverage (one read is sufficient to include a site)
# I use coverage calculated over a set of random regions for the whole sample to estimate expected range of coverage
# and choose cutoffs based on coverage calculated over a number of small regions and all hilo + alloMaize4Low ind's:
scripts$ sbatch --partition=bigmemh calcDepthCovRegions.sh
Submitted batch job 6115224 -- modified to account for much higher possible counts (up to 5000 reads)
scripts$ sbatch --partition=bigmemh -t 10:00:00 calcDepthCovRegions.sh
Submitted batch job 6120920
# 3 standard deviations for regular coverage per site or some other reasonable cutoff is my goal
# The mean coverage across sites I will calculate directly from the filtered BAM read counts
# To calculate mean total coverage I first calculate number of reads from the hilo BAMS
data/filtered_bam$ for i in $(ls hilo_*.sort.dedup.baq.bam); do samtools idxstats ${i}; done | cut -f3 | awk 'BEGIN {total=0} {total += $1} END {print total}'
1458097096
# and from Anne's 4 lowland maize populations
data/alloMaize4Low$ for i in $(ls reordered_*.bam); do samtools idxstats ${i}; done | cut -f3 | awk 'BEGIN {total=0} {total += $1} END {print total}'
6270955555
# So on average we have about 504x coverage across all individuals based on
# taking # reads X 150 bp/read divided by 2.3x10^9bp for the maize genome size
# I used my R script plot_mapping_metrics.R to process the results of calcDepthCovRegions.sh
# and find that across these sampled regions (100 regions of 1000 bp each)
# from these regions I calculate a mean depth of only 278. I'm not sure why that's so much lower than
# my predicted 504x mean. depth in angsd is skipping sites that have no reference base...so for regions 100*1000bp = 100,000
# there are only 94756 counted sites with depth recorded, or ~95% of sites.
# for the total sample, the mean + 3 sd's is 1020 coverage
# combining across individual maize, the mean + 10 sd's is ~158
# and for individual hilo low-coverage sequences, the mean + 10 sd's is ~12

# I apply this new filter for <1020x total coverage to my variant sites scripts
# creating new output in depthFilt/var_sites/pass1_alloMaize4Low/region_**.txt
scripts$ sbatch callVarAngsd_Regions.sh
Submitted batch job 6131903 -- CANCELLED (wrong script -- want sites w/ GL1)
Running script below instead, with output to DIR_OUT=geno_lik/merged_pass1_all_alloMaize4Low_16/allVar_depthFilt
scripts$ sbatch allVarAngsdGL1.sh
Submitted batch job 6133390 - COMPLETED MOST (n=192). very helpful to ID jobs that completed sorting numerically: sort --version-sort
hilo$ grep 'State               : COMPLETED' slurm-log/*6133390*.out | wc -l # looks like I accidentally repeated 402 when it worked the first time -- oh well!
# some jobs ran out of memory, some out of time, and some failed for unknown reasons on bigmem6
scripts$ sbatch --array=0,2,4-5,8,14-15,18,20-21,23,27,37,39,42-60,62-157,160,163-166,169-170,173-176,186,197-200,204-205,210,214,216-217,219-222,228-232,235-236,238,240,243-275,287-288,290,296,298,301,303,308,314-315,319-321,325,329,336,341,348,362,364,371,380-382,385,390,392,395,400-402,413,415,417,419-420,424,425 -x bigmem6 -t 24:00:00 --mem=16G allVarAngsdGL1.sh
# re-doing one that ran out of MEMORY. n=228 state 'COMPLETED' or 'RUNNING'; 5 stuck on bigmem2 @ 22 hrs (array=0,2,4,5,8)
scripts$ sbatch --array=39 -x bigmem6,bigmem2 -t 24:00:00 --mem=32G allVarAngsdGL1.sh
Submitted batch job 6811951 -- COMPLETED
data$ scancel 6811334_0,6811334_2,6811334_4,6811334_5,6811334_8 # cancelled at 22 hrs to resubmit on different node
scripts$ sbatch --array=0,2,4,5,8 -x bigmem6,bigmem2 -t 24:00:00 --mem=32G allVarAngsdGL1.sh
Submitted batch job 6811958 -- COMPLETED (now all regions done)

# when this finishes I need to turn these into sites
scripts$ sbatch --dependency=afterok:6133390 mafsToSitesFile.sh
Submitted batch job 6133614 - DEPENDENCY never satisfied
scripts$ sbatch mafsToSitesFile.sh
Submitted batch job 6812146 -- CREATED EMPTY FILES FOR var.sites; problem with zlib verison. Added line 'module load bio' to script.
Submitted batch job 6813426 -- STILL DID NOT WORK. Can't figure out zcat or sleep commands. Now using angsd 9.21 w/ module bio instead of 9.20 locally installed
scripts$ sbatch -p bigmemm mafsToSitesFile.sh
Submitted batch job 6813852 -- DID NOT WORK. added ALL to export list; could be couldn't find $PATH for zcat and sleep commands
scripts$ sbatch mafsToSitesFile.sh
Submitted batch job 6893592 -- ALL COMPLETED EXCEPT A FEW SUSPENDED on one node -- I cancelled and reran these:
scripts$ scancel 6893592_189 6893592_190 6893592_191 6893592_192 6893592_194
scripts$ sbatch --array=189-192,194 -p med -x c8-67 mafsToSitesFile.sh
Submitted batch job 6894551 - COMPLETED
scripts$ sbatch -p bigmemm --array=267-269,324-325,341-343,346-354,357,376-378,382-383,408,412-416,420-425 -x bigmem2,bigmem4 mafsToSitesFile.sh
Submitted batch job 6992893 - RUNNING -- CANCELLED DUE TO out-of-memory handler. Adding memory & re-running:
Submitted batch job 7000964 -COMPLETED (finally)


# Now I need to calculate MAF while filtering individuals at sites where they have excessive read coverage
# And for maize, filtering sites with too low of read depth
# --> actually because MAF is based on samtools' GL algorithm and not counts I can't filter individuals out like that for outlier coverage in angsd
scripts$ sbatch --export=POP=mexicana.allo.withXochi35 --dependency=afterok:6133614 calcAlleleFreqPop.sh
Submitted batch job 6133628 -- I CANCELLED BECAUSE OF ISSUES ABOVE WITH DEPENDENCY NOT COMPLETING
scripts$ sbatch --export=POP=mexicana.allo.withXochi35 --dependency=afterok:6812146 calcAlleleFreqPop.sh
Submitted batch job 6812513 -- DEPENDENCY NEVER MET. Fixed sites script, load angsd 9.21 & added ALL to export:
scripts$ sbatch --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop.sh
Submitted batch job 6894780 -- RAN: cancelled a wide set 60-121,180-295 due to 'slurm 'Munge decode error' expired credential'..not sure what this means. I'm rerunning.
scripts$ sbatch --array=60-121,180-295 -x bigmem4 --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop.sh
Submitted batch job 6960524 -- NEED TO RERUN 200-221, 60-68; COMPLETED 69-121 and 180-199, 222-295
# trying just one first with new local save & then transfer over rsync script:
scripts$ sbatch --array=200 --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop_transfer.sh
Submitted batch job 6986915 - DIDN;T FIND SITES FILE -- fixed typo:
Submitted batch job 6986952 - DONE EXCEPT rysnc typo. fixed:
Submitted batch job 6987003 - WORKED!
scripts$ sbatch --array=201-221,60-68 --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop_transfer.sh
Submitted batch job 6987047 - 60-65 ran (grep 'ALL done' slurm-log/*6987047*) but others STALLED ON bigmem4; so I cancelled just those jobs:
scripts$ sbatch --array=201-221,66-68%6 -x bigmem2,bigmem4 --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop_transfer.sh
Submitted batch job 6987706 - ALL COMPLETED very fast (a few minutes!). Ooops forgot to redo # 199
scripts$ sbatch --array=199 -x bigmem2,bigmem4 --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop.sh
Submitted batch job 6990165 - COMPLETED. I'm not sure what happened to the following files, if they got overwritten or what (?):
scripts$ sbatch --array=267-269,324-325,341-343,346-354,357,376-378,382-383,408,412-416,420-425 -x bigmem2,bigmem4 --export="POP=mexicana.allo.withXochi35,ALL" calcAlleleFreqPop.sh
Submitted batch job 6990924-- CREATED EMPTY FILES -- issue is with sites file containing zero chromosomes. Fixed var.sites files & re-running:
Submitted batch job 7001245 - COMPLETED

scripts$ sbatch --export=POP=maize.allo.4Low16 --dependency=afterok:6133614 calcAlleleFreqPop.sh
Submitted batch job 6133644 -- I CANCELLED BECAUSE OF ISSUES ABOVE WITH DEPENDENCY NOT COMPLETING
scripts$ sbatch --export=POP=maize.allo.4Low16 --dependency=afterok:6812146 calcAlleleFreqPop.sh
Submitted batch job 6812998 -- DEPENDENCY NEVER MET. Fixed sites script, load angsd 9.21 & added ALL to export:
scripts$ sbatch --export="POP=maize.allo.4Low16,ALL" calcAlleleFreqPop.sh
Submitted batch job 6894990 -- RAN -- all but 0, 252 and 256 completed (these out of memory; I'll double time and memory:)
scripts$ sbatch -t 20:00:00 --mem=16G --array=0,252,256 --export="POP=maize.allo.4Low16,ALL" calcAlleleFreqPop.sh
Submitted batch job 6960263 -- CANCELLED b/c STALLED, probably I/O issue
scripts$ sbatch -t 20:00:00 --array=0,252,256 --mem=16G --export="POP=maize.allo.4Low16,ALL" calcAlleleFreqPop_transfer.sh
Submitted batch job 6987156 - COMPLETED
#Note: I replaced my original calcAlleleFreqPop.sh script with the new working transfer script.
scripts$ sbatch --array=267-269,324-325,341-343,346-354,357,376-378,382-383,408,412-416,420-425 -x bigmem2,bigmem4 --export="POP=maize.allo.4Low16,ALL" calcAlleleFreqPop.sh
Submitted batch job 6990984 -- CREATED EMPTY FILES -- issue is with sites file containing zero chromosomes. Fixed var.sites files & re-running:
Submitted batch job 7001332 - RAN but some stalled w/out progress on bigmem10 and bigmem9. Cancelled & rerunning htose below:
scripts$ sbatch --array=348-353 -x bigmem2,bigmem4,bigmem10,bigmem9 --export="POP=maize.allo.4Low16,ALL" calcAlleleFreqPop.sh
Submitted batch job 7002167 - COMPLETED

# Then I can filter based on MAF diff between allopatric pops and nInd included at a site
scripts$ sbatch filterAlloMAFnInd.sh
Submitted batch job 6989688 -- MANY finished; but some error on table merging .. need to see issue (empty tables: problem all the way back at var.sites file. fixed now & rerunning)
scripts$ sbatch --dependency=afterok:7002167 filterAlloMAFnInd.sh
Submitted batch job 7002187 - COMPLETED
# And then filter for minimum cM spacing
scripts$ sbatch --dependency=afterok:7002187 pruneFixedcM.sh
Submitted batch job 7002203 - CANCELLED to change output directory (may have started overwriting var_sites files, not sure, but pre depth filter so okay to lose those earlier results)
scripts$ sbatch pruneFixedcM.sh
Submitted batch job 7002793 - COMPLETED
# making VCF for allopatric maize
scripts$ sbatch --dependency=afterok:7002793 getVCFforAllo.sh
Submitted batch job 7003338 - typo needs to be rerun:
Submitted batch job 7004748 -- all but chr 1 & 2 (on bigmem10) will finish.
But these two are making little progress, so I'm going to cancel these 2 & restart them:
scripts$ sbatch -p bigmemh --array=1-2 -x bigmem10,bigmem2,bigmem4 getVCFforAllo.sh
Submitted batch job 7009561 - 2 is still running; 1 is out of memory CANCELLED
SAID OUT OF MEMORY BUT ALSO LOOKED LIKE CHR2 MAY HAVE FINISHED - ok!
Submitted batch job 7010274 - COMPLETED
scripts$ sbatch --dependency=afterok:7003338 alloVCF2AlleleCounts.sh
Submitted batch job 7003341 - DEP NEVER MET. Rerunning:
scripts$ sbatch --dependency=afterok:7004748 alloVCF2AlleleCounts.sh
Submitted batch job 7004779 -- DEP NEVER MET. rerunning in parts:
scripts$ sbatch -p bigmemm --array=3-10 -x bigmem10,bigmem2,bigmem4 alloVCF2AlleleCounts.sh
Submitted batch job 7009588 - COMPLETED
scripts$ sbatch -p bigmemm --dependency=afterok:7009561,7010274 --array=1-2 -x bigmem10,bigmem2,bigmem4 alloVCF2AlleleCounts.sh
Submitted batch job 7009675 - cancelled. added 2nd dependency:
Submitted batch job 7010291 - DEP NEVER MET.
Submitted batch job 7014656 - COMPLETED

# getting counts for hilo ind's
scripts$ for i in {1..10}; do sbatch --dependency=afterok:7002793 \
--export="CHR="$i",ALL" countReadsACGT.sh; done
Submitted batch job 7003349-58 - COMPLETE except stalled on c9-73 jobs 51_88,189,194,195,200:
scripts$ sbatch -p bigmemm -x bigmem10 --array=1,2,88,189,194,195,200 --export="CHR=3,ALL" countReadsACGT.sh
Submitted batch job 7009774-cancelled immediately (didn't set array -- only started files 1 and 2 so I'll add those to array)
Submitted batch job 7009839 -- oops! 188 not 88 needed:
scripts$ sbatch -p bigmemm -x bigmem10 --array=188 --export="CHR=3,ALL" countReadsACGT.sh
Submitted batch job 7009861 - COMPLETED. and looks like on from chr4 too got stalled:
scripts$ sbatch -p bigmemm -x bigmem10 --array=7 --export="CHR=4,ALL" countReadsACGT.sh
Submitted batch job 7009885 - COMPLETED

# converting ACGT counts to major/minor counts
scripts$ sbatch --dependency=afterok:$(echo {7003349..7003358} | tr " " ",") countReadsMajorMinor.sh
Submitted batch job 7004200 - DEPENDENCY NEVER MET. CANCELLED.
scripts$ sbatch -p bigmemm -x bigmem10 countReadsMajorMinor.sh
Submitted batch job 7009949 - COMPLETED

# making allopatric counts input
scripts$ sbatch --dependency=afterok:7003341,7004200 makeAlloCountsAncHMMInput.sh
Submitted batch job 7004208 -- cancelled. New dependency:
scripts$ sbatch --dependency=afterok:7004779,7004200 makeAlloCountsAncHMMInput.sh
Submitted batch job 7004978 -- again CANCELLED. New dependency:
scripts$ sbatch --dependency=afterok:7009949,7009675 makeAlloCountsAncHMMInput.sh
Submitted batch job 7010176 - WAITING . cancelled. dependency changed:
scripts$ sbatch --dependency=afterok:7010291 makeAlloCountsAncHMMInput.sh
Submitted batch job 7010360 - cancelled. dependency never met. rerunning:
Submitted batch job 7014675

# putting it all together
scripts$ sbatch --dependency=afterok:7004208 makeAncHMMInput.sh
Submitted batch job 7004214 -- cancelled--new dependency:
scripts$ sbatch -p bigmemm --dependency=afterok:7004978 makeAncHMMInput.sh
Submitted batch job 7005324 -- cancelled--NEW DEPENDENCY:
scripts$ sbatch -p bigmemm -x bigmem10 --dependency=afterok:7010176 makeAncHMMInput.sh
Submitted batch job 7010188 - cancelled--NEW DEPENDENCY:
scripts$ sbatch -p bigmemm -x bigmem10 --dependency=afterok:7010360 makeAncHMMInput.sh
Submitted batch job 7010417 - DEP NEVER MET:
scripts$ sbatch -p bigmemm -x bigmem10 --dependency=afterok:7014675 makeAncHMMInput.sh
Submitted batch job 7014690 - COMPLETED

# copied over global ancestry proportions (can rerun later with last round of sequencing data, but ok proxy for now)
data$ mkdir -p geno_lik/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/input
data$ cp var_sites/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/input/globalAdmixtureByPopN.txt geno_lik/merged_pass1_all_alloMaize4Low_16/thinnedHMM/ancestry_hmm/input/.
# running ancestry_hmm no_boot and bootstrap
scripts$ sbatch -x bigmem10 --dependency=afterok:7010188 runAncHMM_noboot.sh
Submitted batch job 7010208 - cancelled new dependency:
scripts$ sbatch -x bigmem10 --dependency=afterok:7010417 runAncHMM_noboot.sh
Submitted batch job 7010427 - DEP NEVER MET:
scripts$ sbatch -x bigmem10 --dependency=afterok:7014690 runAncHMM_noboot.sh
Submitted batch job 7014736 -- ERROR: (I'll try to rerun with bio module loaded -- not sure issue (?))
ancestry_hmm: error while loading shared libraries: libarmadillo.so.6: cannot open shared object file: No such file or directory
GOT FARM UPDATED COPY Ancestry_HMM loaded as a module:
scripts$ sbatch -x bigmem10 runAncHMM_noboot.sh
Submitted batch job 7015475 - COMPLETED

scripts$ sbatch -x bigmem10 --dependency=afterok:7010188 runAncHMM.sh
Submitted batch job 7010210 - cancelled new dependency:
scripts$ sbatch -x bigmem10 --dependency=afterok:7010417 runAncHMM.sh
Submitted batch job 7010444 - DEPENDENCY NEVER MET: ( need to update loading module here too)
scripts$ sbatch -x bigmem10 runAncHMM.sh
Submitted batch job 7048315 - completed except some on bigmem2 & 3:
scripts$ sbatch -x bigmem2,bigmem3 runAncHMM.sh
Submitted batch job 7104632 - Several timed out (oops it's rerunning all of them not just 0,1,2!)
# oops #2 timed out. also had slurm error on bigmem5: 1,3,4,19,20
scripts$ sbatch --array=1-4,19-20 -t 48:00:00 -x bigmem2,bigmem3,bigmem5 runAncHMM.sh
Submitted batch job 7136935 - RUNNING

# Formalized R script to summarise ancestry from posteriors into a bash script
scripts$ sbatch calcAncfromPost.sh # runs # calc_genomewide_pop_anc_freq.R for each admixed pop
Submitted batch job 7016848 - ERROR. adding R script to git:
Submitted batch job 7016963 - munge code error on just some of them on bigmem3, rerunning:
scripts$ sbatch --array=0-4,6,8 calcAncfromPost.sh
Submitted batch job 7048166 - ALL CANCELLED DUE TO TIME LIMIT on bigmem2. rerunning:
scripts$ sbatch --array=0-4,6,8 -x bigmem2,bigmem3 -t 8:00:00 calcAncfromPost.sh
Submitted batch job 7104529 - COMPLETED

# Gene density: I want to calculate exonic bp per cM in 10kb windows across the genome (what Li Wang 2017 does)
# (or I could calculate gene density each included site? not sure)
# for example, I could block genome into nonoverlapping 10kb windows, average any variant sites for which I call ancestry within those windows,
# and then get their mean gene density too (Li Wang/JRI approach) or I could calculate for each variant site where I've called local ancestry
# the attached load to that site ... # exonic bp within 1cM of a focal site (for each CD within 1cM window of site, take length of CDS, and set its position as the center of the CD,
# and its potential 'strength of sel' as the # bp in the CDS x some constant probability of a bp being selected.. then sum over all nearby CDS (Ivan Juric/Graham approach; also tried 10cM distance)
# possibly the best approach is to split genome into 1cM bins and calculate # exonic sites per bin (Ivan/Graham do this for one plot) -- but how do you choose 1cM? Could also do 10cM
# I definitely prefer calculating gene density for a fixed cM distance, not fixed bp distance, b/c that's the natural unit for linked selection
# I need to download gff file from ensemble and identify the CoDing Sequence (CDS) - which is the whole exon minus the 3' and 5' UTRs
# I downloaded the gff3 file from ensemble 11.6.18 ftp://ftp.ensemblgenomes.org/pub/plants/release-41/gff3/zea_mays
# then I extracted the locations of all the CDS on the chromosomes, mtDNA, ptDNA (n=918923)
# realized there was lots of overlap between different CDS.
# So I first made an R script to pre-process the gff3 file into the largest contiguous CDS (ignoring strand identity) from any overlapping or adjacent CDS
reduceCDSFromGFF3.R # ran interactively to produce output: CDS_1.txt, CDS_2.txt ... CDS_Mt.txt (1 per chromosome)

# Now using a short python script to output the map position (in cM) for the start, end, and midpoint of each CDS. And its total length too.
scripts$ module load python3
scripts$ python3 getMapPosCDS.py -- NOPE: running interactively = too long. Will make script after fixing issues below
scripts$ sbatch getMapPosCDS.sh
Submitted batch job 6958913 - CANCELLED, fixed typos & made more efficient by passing just chr map to calcMapPos.py: rerunning:
scripts$ sbatch --array=0 getMapPosCDS.sh # chr 0 is a very short test file
Submitted batch job 6961885 # made temporary tweak to .sh script to run this test file using chr1 map & fixed all bugs
scripts$ sbatch getMapPosCDS.sh
Submitted batch job 6962448 -- RUNNING
# note: I was having difficulty using pandas and python3 on my laptop, but it works on farm. I stopped the script writing to output once it gets to positions on the mt and pt chromosomes (because not integers)
# I made some scripts to calculate cM <-> bp and recomb. rates
# from the maize genetic map as well as gene density
# within a window, either from % coding bp or calculating a 'linkage score'
gene_density.R rmap_functions.R
# rmap.R runs these functions to calculate recombination rate and coding density for every site
scripts$ sbatch -x bigmem10 getGeneDensity.sh
Submitted batch job 7010755 -- TIMED OUT at 12 hours

# I can write a separate script to divide the whole genome into 1cM chunks and then calculate what % of bp's are CDS within each of those chunks (save start/end and # CDS bp and percent). Also potentially save mean recomb. rate over region.
# note if I can get a list of start and end bp for the 1cM chunks, I can use IRanges() in R to count overlapping bases with my set of CDS in CDS.txt

# Or maybe it's better to get start and end bp for 1cM around each included variant site. This is an approximation also of local recombination rate, though somewhat smoothed from the point estimate at a locus.

# INVERSIONS: I want to run all analyses with and without major known inversions. So I'm getting from the literature the inversion coordinates.
# I found several identified inversion from prior mapping, LD, and experimental studies:
# Literature citations and descriptions are saved refMaize/inversions/README_knownInv.txt
# I've put the genomic coordinates in a new file refMaize/inversions/knownInv.txt

# MAPPING ISSUES: To find out if there are regions not being called for mexicana
# ancestry due to mapping issues, I need to see if there are places where maize
# has high coverage but allopatric teosinte and hilo admixed ind's have low coverage
# I'm not sure how to do this as these sites will be filtered out of 'ancestry informative'
# SNP set for ancestry_hmm and the may or may not be in the original ANGSD SNP set if my
# minimum # individuals who need some data is set too high

# RE-DOING NGSAdmix and PCAngsd with new variant sites filtered for excess depth
scripts$ sbatch pruneFixedcM_PCA.sh
Submitted batch job 7049445 - COMPLETED
# now making GL file for all individuals at these ~125K sites
scripts$ sbatch thinnedVarAngsdGL1.sh
Submitted batch job 7103658 - fixed typo:
Submitted batch job 7104011 - space typo:
Submitted batch job 7104191 - OUT OF MEMORY & TIMED OUT
# new approach is instead of making a new GL file at these ~125K sites,
# I thin the existing GL file to only include these ~125K sites using R
# I made a script to accomplish this: thinGLFile.R run from thinGLFile4PCA.sh:
scripts$ sbatch thinGLFile4PCA.sh

# Then I need to concatenate all the regions together to 1 large GL file:
